{"pages":[],"posts":[{"title":"CAS","text":"原理CAS通过调用JNI的代码实现的。而compareAndSwapInt就是借助C来调用CPU底层指令实现的。 1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 缺点： ABA问题 解决1：使用版本号或者时间戳； 解决2：Java1.5开始在atomic包中提供一个类AtomicStampedReference来解决ABA问题，这个类的compareAndSet方法中首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 循环时间长开销大 只能保证一个共享变量的原子操作 可以使用Java1.5开始提供的AtomicReference来包装多个变量，或者使用Synchronized。","link":"/2020/06/24/CAS/"},{"title":"ComputerNetwork","text":"HTTPSHTTP + SSL/TLS SSL：Secure Sockets Layer 安全套接层 TLS：Transport Layer Security 传输层安全协议 HTTPS加解密流程 用户在浏览器发起HTTPS请求，默认使用服务端的443端口进行连接； HTTPS需要使用一套CA数字证书，证书内会附带一个公钥Pub，而与之对应的私钥Private保留在服务端不公开； 服务端收到请求，返回配置好的包含公钥Pub的证书给客户端； 客户端收到证书，校验合法性，主要包括是否在有效期内、证书的域名与请求的域名是否匹配，上一级证书是否有效（递归判断，直到判断到系统内置或浏览器配置好的根证书），如果不通过，则显示HTTPS警告信息，如果通过则继续； 客户端生成一个用于对称加密的随机Key，并用证书内的公钥Pub进行加密，发送给服务端； 服务端收到随机Key的密文，使用与公钥Pub配对的私钥Private进行解密，得到客户端真正想发送的随机Key； 服务端使用客户端发送过来的随机Key对要传输的HTTP数据进行对称加密，将密文返回客户端； 客户端使用随机Key对称解密密文，得到HTTP数据明文； 后续HTTPS请求使用之前交换好的随机Key进行对称加解密； 对称加密与非对称加密对称加密：加密与解密用的是同样的密钥 非对称加密：使用一对密钥，公钥和私钥，私钥只能由一方保管，不能外泄，而公钥可以发给任何请求它的人。 对称加密 + 非对称加密的方案： 服务端有非对称加密的公钥A1，私钥A2； 客户端发起请求，服务端将公钥A1返回给客户端； 客户端随机生成一个对称加密的密钥K，用公钥A1加密后发送给服务端； 服务端收到密文后用自己的私钥A2解密，得到对称密钥K，此时完成了对称密钥交换，解决了对称加密时密钥传输被人窃取的问题； 之后双方通信都使用密钥K进行对称加解密； 存在的问题：当服务端向客户端返回公钥A1时，中间人将其替换成自己的公钥B1，传送给浏览器，而浏览器对此并无感知，使用B1加密密钥K发送出去，又被中间人截获，中间人使用自己的私钥B2解密，得到密钥K，再使用服务端的公钥A1加密传送给服务端，完成了通信链路，而服务端和客户端毫无感知。 原因：客户端无法确认收到的公钥是不是真的是服务端发来的。 解决方案：使用公信机构，CA + 数字签名 CA机构拥有自己的一对公钥和私钥； CA机构在颁发证书时对证书明文信息进行哈希； 将哈希值用私钥进行加签，得到数字签名； 明文数据和数字签名组成证书传递给客户端； 客户端得到证书，分解成明文部分Text和数字签名Sig1； 用CA机构的公钥进行解签，得到Sig2（由于CA机构是一种公信身份，因此在系统或浏览器中会内置CA机构的证书和公钥信息）； 用证书里声明的哈希算法对明文Text部分进行哈希得到H； 当计算得到的哈希值T与解签后的Sig2相等，表示证书可信，没有被篡改； 总结HTTPS 使用非对称加密+对称加密方案，兼顾性能和安全性；为了保证公钥传输过程中不被篡改，又使用了非对称加密的数字签名功能，借助CA机构和系统根证书的机制保证了HTTPS证书的公信力。","link":"/2020/05/05/ComputerNetwork/"},{"title":"ConcurrentHashMap","text":"多线程环境下如何解决HashMap线程安全的问题？ 使用Collections.synchronizedMap(Map)创建线程安全的map集合； 123456789101112private final Map&lt;K,V&gt; m; // Backing Mapfinal Object mutex; // Object on which to synchronizeSynchronizedMap(Map&lt;K,V&gt; m) { this.m = Objects.requireNonNull(m); mutex = this;}SynchronizedMap(Map&lt;K,V&gt; m, Object mutex) { this.m = m; this.mutex = mutex;} 在SynchronizedMap内部维护了一个普通对象Map和排斥锁mutex，并且有两个构造器，如果传入了mutex参数，则将对象排斥锁赋值为传入的对象；如果没有，则将对象排斥锁赋值为this。创建出SynchronizedMap后，再操作map时，会对方法上锁。 123456789101112131415public int size() { synchronized (mutex) {return m.size();}}public boolean isEmpty() { synchronized (mutex) {return m.isEmpty();}}public boolean containsKey(Object key) { synchronized (mutex) {return m.containsKey(key);}}public boolean containsValue(Object value) { synchronized (mutex) {return m.containsValue(value);}}public V get(Object key) { synchronized (mutex) {return m.get(key);}} Hashtable 效率低，因为在对数据操作时都会上锁。 与HashMap的区别： 实现方式不同：Hashtable继承了Dictionary类，HashMap继承的是AbstractMap类； 初始化容量不同：HashMap的初始容量为16，Hashtable为11，二者的负载因子默认都是0.75； 扩容机制不同：当现有容量大于总容量*负载因子时，HashMap的扩容规则为当前容量翻倍，Hashtable为当前容量翻倍 + 1； 迭代器不同：HashMap中的Iterator迭代器是fail-fast的，而Hashtable的Enumerator不是fail-fast的； Hashtable不允许键或值为null，HashMap的键值都可以为null； 原因：Hashtable在put空值时会抛出空指针异常，Hashtable使用的是安全失败机制fail-safe，这种机制会使此次读到的数据不一定是最新的数据。如果使用null值，就会使其无法判断对应的key是不存在还是为空，ConcurrentHashMap同理。 fail-fast： 是Java集合中的一种机制，在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。 原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 使用场景：java.util包下的集合类都是fail-fast的，不能在多线程下并发修改。 fail-safe： 遍历基于容器的一个克隆，对容器内容的修改不影响遍历。java.util.concurrent包下的容器都是fail-safe的，在多线程下并发修改，例如，ConcurrentHashMap，CopyOnWrite等。 ConcurrentHashMap ConcurrentHashMap1.7 由Segment数组和HashEntry组成，基于数组+链表，采用了分段锁技术，Segment继承于ReentrantLock。Segment 的个数一但初始化就不能改变。 put()：尝试获取锁，获取失败则利用scanAndLockForPut()自旋获取锁，如果重试次数达到了MAX_SCAN_RETRIES，则改为阻塞锁获取，保证能获取成功；在Segment中通过key的hashcode定位到HashEntry；遍历该HashEntry，如果不为空则判断传入的key和当前遍历的key是否相等，相等则覆盖旧的value；为空则需要新建一个HashEntry并加入到Segment中，同时会先判断是否需要扩容；释放锁。 get()：key通过hash定位到具体的Segment，再通过一次hash定位到具体元素上，因为HashEntry的value属性是用volatile修饰的，保证了内存可见性，所以每次获取时都是最新值。get的整个过程不需要加锁。 缺点：查询是需要遍历链表，效率较低； 1.8 使用CAS + Synchronized保证线程安全。HashEntry改为Node，但作用不变，并且也引入了红黑树。 初始化初始化通过自旋+CAS完成，变量sizeCtl决定当前初始化状态 -1 说明正在初始化； -N 说明有N-1个线程正在进行扩容； 表示 table 初始化大小，如果 table 没有初始化； 表示 table 容量，如果 table 已经初始化； 123456789101112131415161718192021222324252627/** * Initializes table, using the size recorded in sizeCtl. */private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 如果sizeCtl &lt; 0 ,说明另外的线程执行CAS成功，正在进行初始化 if ((sc = sizeCtl) &lt; 0) // 让出CPU使用权 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } break; } } return tab;} put12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public V put(K key, V value) { return putVal(key, value, false);}/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) { // key 和 value 不能为空 if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { // f 代表目标位置元素 Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // 数组桶为空，初始化数组桶（自旋+CAS) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 桶内为空，CAS 放入，不加锁，成功了就直接 break 跳出 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; // 使用 synchronized 加锁加入节点 synchronized (f) { if (tabAt(tab, i) == f) { // 说明是链表 if (fh &gt;= 0) { binCount = 1; // 循环加入新的或者覆盖节点 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } // 红黑树 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null;} put()：根据key计算出hashcode；判断是否需要进行初始化；根据key定位，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功；如果当前位置的hashcode == MOVED == -1，则需要进行扩容；如果都不满足，则利用synchronized锁写入数据；如果数量大于TREEIFY_THRESHOLD 则要转换为红黑树。 123456789101112131415161718192021222324public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // key 所在的 hash 位置 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { // 如果指定位置元素存在，头结点hash值相同 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) // key hash 值相等，key值相同，直接返回元素 value return e.val; } else if (eh &lt; 0) // 头结点hash值小于0，说明正在扩容或者是红黑树，find查找 return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { // 是链表，遍历查找 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null;} get()：根据计算出来的hashcode寻址，如果在桶上（头结点）就直接返回；如果是红黑树就按照树的方式获取值；都不满足就按照链表方式获取值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public V put(K key, V value) { return putVal(key, value, false);} /** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) { //ConcurrentHashMap 不允许插入null键，HashMap允许插入一个null键 if (key == null || value == null) throw new NullPointerException(); //计算key的hash值 int hash = spread(key.hashCode()); int binCount = 0; //for循环的作用：因为更新元素是使用CAS机制更新，需要不断的失败重试，直到成功为止。 for (Node&lt;K,V&gt;[] tab = table;;) { // f：链表或红黑二叉树头结点，向链表中添加元素时，需要synchronized获取f的锁。 Node&lt;K,V&gt; f; int n, i, fh; //判断Node[]数组是否初始化，没有则进行初始化操作 if (tab == null || (n = tab.length) == 0) tab = initTable(); //通过hash定位Node[]数组的索引坐标，是否有Node节点，如果没有则使用CAS进行添加（链表的头结点），添加失败则进入下次循环。 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } //检查到内部正在移动元素（Node[] 数组扩容） else if ((fh = f.hash) == MOVED) //帮助它扩容 tab = helpTransfer(tab, f); else { V oldVal = null; //锁住链表或红黑二叉树的头结点 synchronized (f) { //判断f是否是链表的头结点 if (tabAt(tab, i) == f) { //如果fh&gt;=0 是链表节点 if (fh &gt;= 0) { binCount = 1; //遍历链表所有节点 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; //如果节点存在，则更新value if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } //不存在则在链表尾部添加新节点。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } //TreeBin是红黑二叉树节点 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; //添加树节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { //如果链表长度已经达到临界值8 就需要把链表转换为树结构 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } //将当前ConcurrentHashMap的size数量+1 addCount(1L, binCount); return null;} 判断Node[]数组是否初始化，没有则进行初始化操作 通过hash定位Node[]数组的索引坐标，是否有Node节点，如果没有则使用CAS进行添加（链表的头结点），添加失败则进入下次循环。 检查到内部正在扩容，如果正在扩容，就帮助它一块扩容。 如果f!=null，则使用synchronized锁住f元素（链表/红黑二叉树的头元素）f是目标位置元素 4.1 如果是Node(链表结构)则执行链表的添加操作。 4.2 如果是TreeNode(树型结果)则执行树添加操作。 判断链表长度已经达到临界值8 就需要把链表转换为树结构。","link":"/2020/08/06/ConcurrentHashMap/"},{"title":"CopyOnWrite","text":"CopyOnWrite容器简介可以理解为写时复制的容器，当向容器中添加元素时，不是直接往容器中添加，而是将当前容器进行copy，复制出一个新的容器，然后向新容器中添加元素，最后将原容器的引用指向新容器。 好处：在并发的场景下对容器进行”读操作”而不需要”加锁”，从而达到读写分离的目的。 从JDK 1.5 开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器，分别是CopyOnWriteArrayList和CopyOnWriteArraySet。 CopyOnWriteArrayList优缺点分析优点：适用于读多写少的场景。在Java中遍历线程非安全的List时（如：ArrayList和 LinkedList），若中途有其他线程对List容器进行修改，那么会抛出ConcurrentModificationException异常。CopyOnWriteArrayList由于其“读写分离”，遍历和修改操作分别作用在不同的List容器，所以在使用迭代器遍历的时候，则不会抛出异常。 缺点：1）每次执行写操作都会将原容器拷贝一份，数据量大的时候，内存会存在较大的压力，可能会引起频繁Young GC和Full GC；2）只能保证最终数据一致性，不能保证实时一致性。 源码分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; // 拷贝原容器，长度加一 Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; // 将原容器引用指向新副本 setArray(newElements); return true; } finally { lock.unlock(); }}/** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). Returns the element that was removed from the list. * * @throws IndexOutOfBoundsException {@inheritDoc} */public E remove(int index) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; // 如果要删除的是列表末端数据，拷贝前len-1个数据到新副本上，再切换引用 if (numMoved == 0) setArray(Arrays.copyOf(elements, len - 1)); else { // 否则，将除要删除元素之外的其他元素拷贝到新副本中，并切换引用 Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); } return oldValue; } finally { lock.unlock(); }}public E get(int index) { return get(getArray(), index);}","link":"/2020/07/17/CopyOnWrite/"},{"title":"Dubbo","text":"Dubbo相关的知识及常见问题：） 各种知识点总结常见面试题 什么是Dubbo？ 是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 面向服务的架构（SOA）是一个组件模型，它将应用程序的不同功能单元（称为服务）进行拆分，并通过这些服务之间定义良好的接口和协议联系起来。接口是采用中立的方式进行定义的，它应该独立于实现服务的硬件平台、操作系统和编程语言。这使得构件在各种各样的系统中的服务可以以一种统一和通用的方式进行交互。 什么是RPC？ Remote Procedure Call 是一种远程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。 例如，A，B服务分别部署在两台不同的机器上，如何解决服务A调用服务B中的某个方法的问题，HTTP请求较慢，RPC应运而生。 为什么使用Dubbo？ 负载均衡：同一个服务部署在不同的机器时该调用那一台机器上的服务； 服务调用链路生成：随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，Dubbo可以解决服务之间互相是如何调用的； 服务访问压力以及时长统计、资源调度和治理：基于访问压力实时管理集群容量，提高集群利用率； 服务降级：某个服务挂掉之后调用备用服务； Dubbo的架构 Provider：暴露服务的服务提供方； Consumer：调用远程服务的服务消费方； Registry：服务注册与发现的注册中心； Monitor：统计服务的调用次数和调用时间的监控中心； Container：服务运行容器； 调用关系 服务容器负责启动，加载，运行服务提供者； 服务提供者在启动时，向注册中心注册自己提供的服务； 服务消费者在启动时，向注册中心订阅自己所需的服务； 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者； 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用； 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心； 重要知识点总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小； 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示； 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外； 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者； 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表； 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者； 服务提供者无状态，任意一台宕掉后，不影响使用； 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复； Dubbo工作原理 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 各层说明： 第一层：service层，接口层，给服务提供者和消费者来实现的； 第二层：config层，配置层，主要是对dubbo进行各种配置的； 第三层：proxy层，服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton； 第四层：registry层，服务注册层，负责服务的注册与发现； 第五层：cluster层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务； 第六层：monitor层，监控层，对rpc接口的调用次数和调用时间进行监控； 第七层：protocol层，远程调用层，封装rpc调用； 第八层：exchange层，信息交换层，封装请求响应模式，同步转异步； 第九层：transport层，网络传输层，抽象mina和netty为统一接口； 第十层：serialize层，数据序列化层。网络传输需要； Dubbo中的负载均衡算法及其特点 1）RandomLoadBalance：随机负载均衡，是Dubbo的默认负载均衡策略。 按照权重分配随机概率，在Dubbo中可以对 Provider设置权重。比如机器性能好的，可以设置大一点的权重，性能差的，可以设置小一点的权重。如果没有在Dubbo Admin中对服务 Provider 设置权重，那么所有的 Invoker 的权重就是一样的，默认是100。 2）RoundRobinLoadBalance：轮询负载均衡，依次的调用所有的Provider。 和随机负载均衡策略一样，轮询负载均衡策略也有权重的概念。轮询负载均衡算法可以让RPC调用严格按照我们设置的比例来分配。但是轮询负载均衡算法也有不足的地方，存在慢的Provider累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上，导致整个系统变慢。 3）LeastActiveLoadBalance：最少活跃调用数，相同活跃数的随机。活跃数指调用前后计数差。使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。 比如：每个服务维护一个活跃数计数器。当A机器开始处理请求，该计数器加1，此时A还未处理完成。若处理完毕则计数器减1。而B机器接受到请求后很快处理完毕。那么A,B的活跃数分别是1，0。当又产生了一个新的请求，则选择B机器去执行(B活跃数最小)，这样使慢的机器A收到少的请求。4）ConsistentHashLoadBalance：一致性Hash算法。相同参数的请求总是落在同一Provider。当某一台 Provider 崩溃时，原本发往该 Provider 的请求，基于虚拟节点，平摊到其它 Provider，不会引起剧烈变动。 一致性Hash算法可以和缓存机制配合起来使用。比如有一个服务getUserInfo(String userId)。设置了Hash算法后，相同的userId的调用，都会发送到同一个 Provider。这个 Provider 上可以把用户数据在内存中进行缓存，减少访问数据库或分布式缓存的次数。如果业务上允许这部分数据有一段时间的不一致，可以考虑这种做法。减少对数据库，缓存等中间件的依赖和访问次数，同时减少了网络IO操作，提高系统性能。 和Dubbo其他的配置类似，多个配置是有覆盖关系的： 方法级优先，接口级次之，全局配置再次之； 如果级别一样，则消费方优先，提供方次之； dubbo中的URL protocol://username:password@host:port/path?key=value&amp;key=value 12345678dubbo://192.168.1.6:20880/moe.cnkirito.sample.HelloService?timeout=3000描述一个 dubbo 协议的服务zookeeper://127.0.0.1:2181/org.apache.dubbo.registry.RegistryService?application=demo-consumer&amp;dubbo=2.0.2&amp;interface=org.apache.dubbo.registry.RegistryService&amp;pid=1214&amp;qos.port=33333&amp;timestamp=1545721981946描述一个 zookeeper 注册中心consumer://30.5.120.217/org.apache.dubbo.demo.DemoService?application=demo-consumer&amp;category=consumers&amp;check=false&amp;dubbo=2.0.2&amp;interface=org.apache.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=1209&amp;qos.port=33333&amp;side=consumer&amp;timestamp=1545721827784描述一个消费者 protocol：一般是 dubbo 中的各种协议 如：dubbo、thrift、http、zk username/password：用户名/密码 host/port：主机IP地址/端口号 path：接口名称 parameter：参数键值对 dubbo 服务暴露 Spring IOC容器刷新完毕 ServiceBean#OnApplicationEvent ServiceConfig#export （如果需要延迟就延迟，不需要就直接用） ServiceConfig#doExport （获取注册中心URL，循环调用） ServiceConfig#doExportUrls ServiceConfig#doExportUrlsFor1Protocol 1、dubbo中”读接口”和”写接口”有什么区别? 3、最小活跃数算法中是如何统计这个活跃数的？ 5、服务发布过程中做了哪些事？ 6、dubbo都有哪些协议,他们之间有什么特点,缺省值是什么？ 7、什么是本地暴露和远程暴露,他们的区别？ 8、服务提供者能实现失效踢出是根据什么原理? 9、讲讲dubbo服务暴露中本地暴露,并画图辅助说明？ 10、一般选择什么注册中心,还有别的选择吗? 11、dubbo中zookeeper做注册中心,如果注册中心集群都挂掉,那发布者和订阅者还能通信吗?(面试高频题) 12、项目中有使用过多线程吗?有的话讲讲你在哪里用到了多线程?(面试高频题) 13、zookeeper的java客户端你使用过哪些? 14、服务提供者能实现失效踢出是什么原理？(高频题) 15、zookeeper的有哪些节点,他们有什么区别?讲一下应用场景。 17、在dubbo中,什么时候更新本地的zookeeper信息缓存文件?订阅zookeeper信息的整体过程是怎么样的? 18、谈一下你们项目架构设计(很多人在回答这个的时候都容易回答SSH或者SSM,注意,所谓是SSH这些是技术选型,不是架构的设计) 19、既然你们项目用到了dubbo,那你讲讲你们是怎么通过dubbo实现服务降级的,降级的方式有哪些,又有什么区别? 20、dubbo监控平台能够动态改变接口的一些设置,其原理是怎样的? 21、既然你说你看过dubbo源码,那讲一下有没有遇到过什么坑?(区分度高,也是检验是否看过源码的试金石) 23、有没有考虑过自己实现一个类似dubbo的RPC框架,如果有,请问你会如果着手实现?(面试高频题,区分度高) 24、你说你用过mybatis,那你知道Mapper接口的原理吗?(如果回答得不错,并且提到动态代理这个关键词会继续往下问,那这个动态代理又是如何通过依赖注入到Mapper接口的呢?) 26、既然你提到了dubbo的服务引用中封装通信细节是用到了动态代理,那请问创建动态代理常用的方式有哪些,他们又有什么区别?dubbo中用的是哪一种?(高频题) 27、除了JDK动态代理和CGLIB动态代理外,还知不知道其他实现代理的方式?(区分度高) 28、你是否了解spi,讲一讲什么是spi,为什么要使用spi? 29、对类加载机制了解吗,说一下什么是双亲委托模式,他有什么弊端,这个弊端有没有什么我们熟悉的案例,解决这个弊端的原理又是怎么样的? 30、既然你对spi有一定了解,那么dubbo的spi和jdk的spi有区别吗?有的话,究竟有什么区别? 31、你提到了dubbo中spi也增加了IoC,那你先讲讲Spring的IoC,然后再讲讲dubbo里面又是怎么做的？ 32、你提到了dubbo中spi也增加了AOP,那你讲讲这用到了什么设计模式,dubbo又是如何做的？","link":"/2020/06/21/Dubbo/"},{"title":"Callable","text":"使用直接继承Thread，或者实现Runnable接口创建线程时，都存在执行完任务后无法获取执行结果的缺陷，Callable和Future应运而生。 12345678910@FunctionalInterfacepublic interface Callable&lt;V&gt; { /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;} Future1234567891011121314151617public interface Future&lt;V&gt; { // 取消任务，取消任务成功返回true，失败则返回false // 参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务，如果设置true，则表示可以取消正在执行过程中的任务 // 即如果取消已经完成的任务会返回false // 如果任务还没有执行，返回true boolean cancel(boolean mayInterruptIfRunning); // 任务是否被取消成功，如果在任务正常完成前被取消成功，返回true boolean isCancelled(); // 任务是否已经完成，若完成，返回true boolean isDone(); // 用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回 V get() throws InterruptedException, ExecutionException; // 用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;} 因为Future只是一个接口，无法直接用来创建对象，所以FutureTask应运而生。 FutureTask1234567891011public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; { /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run();}public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; { ...} 所以FutureTask既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。","link":"/2020/06/14/Callable/"},{"title":"HandlerInterceptor","text":"单用户限制接口请求频率HandlerInterceptorSpringMVC的处理器拦截器，类似于Servlet中的过滤器filter。 1234567891011121314151617181920212223242526public interface HandlerInterceptor { /** * 预处理回调方法，实现处理器的预处理（如检查登陆），第三个参数为响应的处理器，自定义Controller * 返回值：true表示继续流程（如调用下一个拦截器或处理器） * false表示流程中断(如登录检查失败)，不会继续调用其他的拦截器或处理器，此时我们需要通过response来产生响应； */ boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; /** * 后处理回调方法，实现处理器的后处理（但在渲染视图之前）， * 此时我们可以通过modelAndView（模型和视图对象）对模型数据进行处理 * 或对视图进行处理，modelAndView也可能为null */ void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception; /** * 整个请求处理完毕回调方法，即在视图渲染完毕时回调， * 如性能监控中我们可以在此记录结束时间并输出消耗时间，还可以进行一些资源清理， * 类似于try-catch-finally中的finally，但仅调用处理器执行链中 */ void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception;} 适用场景 日志记录，记录请求信息的日志，以便进行信息监控，信息统计等； 权限检查，如登陆检测，进入处理器检测是否登陆，如果没有直接返回到登陆页面； 性能监控，如慢日志； HandlerInterceptorAdapter有时只需实现三个回调方法中的某个，如果实现HandlerInterceptor接口，三个方法必须都实现，此时Spring提供了一个HandlerInterceptorAdapter适配器，允许只实现需要的回调方法。 1234567891011121314151617public abstract class HandlerInterceptorAdapter implements AsyncHandlerInterceptor { public HandlerInterceptorAdapter() { } public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return true; } public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { } public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception { } public void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { }} 运行流程总结如下： 1、拦截器执行顺序是按照Spring配置文件中定义的顺序而定的； 2、先按照顺序执行所有拦截器的preHandle方法，一直遇到return false为止，return false之后的所有拦截器都不会执行；若都是return true，则按顺序加载完preHandle方法； 3、执行主方法（自己的controller接口），若中间抛出异常，则跟return false效果一致，不会继续执行postHandle，只会倒序执行afterCompletion方法； 4、在主方法执行完业务逻辑（页面还未渲染数据）时，按倒序执行postHandle方法。若第三个拦截器的preHandle方法return false，则会执行第二个和第一个的postHandle方法和afterCompletion（postHandle都执行完才会执行这个，也就是页面渲染完数据后，执行after进行清理工作）方法。（postHandle和afterCompletion都是倒序执行）； 项目中的实现声明注解1234567@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface AccessLimit { int seconds(); int maxCount(); boolean needLogin() default true;} 重写 HandlerInterceptorAdapter12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Servicepublic class AccessInterceptor extends HandlerInterceptorAdapter { @Autowired private UserService userService; @Autowired private RedisService redisService; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { if (handler instanceof HandlerMethod) { SkUser skUser = getUser(request, response); UserContext.setUser(skUser); HandlerMethod handlerMethod = (HandlerMethod) handler; // 获取注解 AccessLimit accessLimit = handlerMethod.getMethodAnnotation(AccessLimit.class); if (accessLimit == null) { return true; } // 获取限流的单位时间 int seconds = accessLimit.seconds(); // 获取限制的最大请求数 int maxCount = accessLimit.maxCount(); // 是否需要登录 boolean needLogin = accessLimit.needLogin(); String key = request.getRequestURI(); if (needLogin) { if (skUser == null) { render(response, ResultStatus.SESSION_ERROR); return false; } key = key + &quot;_&quot; + skUser.getId(); } else { // do nothing } AccessKey accessKey = AccessKey.withExpire(seconds); // 从缓存中获取当前请求计数 Integer count = redisService.get(accessKey, key, Integer.class); if (count == null) { // 第一次请求 redisService.set(accessKey, key, 1); } else if (count &lt; maxCount) { // 小于限制的最大请求数 redisService.incr(accessKey, key); } else { // 达到最大请求数 render(response, ResultStatus.ACCESS_LIMIT_REACHED); return false; } } return true; } private void render(HttpServletResponse response, ResultStatus resultStatus) throws Exception { response.setContentType(&quot;application/json;charset=UTF-8&quot;); OutputStream outputStream = response.getOutputStream(); String str = JSON.toJSONString(ResultSk.error(resultStatus)); outputStream.write(str.getBytes(StandardCharsets.UTF_8)); outputStream.flush(); outputStream.close(); } ...}","link":"/2020/07/23/HandlerInterceptor/"},{"title":"IMPROMPTU","text":"《即兴演讲》读后总结即兴演讲的兴起 即兴演讲兴起的三个原因 组织扁平化 科技的发展 科技的发展加快了组织从传统的自上而下的领导力模式向分布式领导力模式的转变。 时空观的改变 即兴交流的力量即兴演讲为领导者带来引人注目的力量，主要体现在： 大量的机会 非正式对话可能发生于办公室、电梯间、餐厅等各种公共场合。 组织内的纵向、横向合作 即兴交流推倒了等级的藩篱，并在不同的专业领域间搭建了理解的桥梁。 更快、更好的决策 自然、自发的对话使你更及时地解决问题并及时反馈。 一种拉近关系的新方法 使你真实、可信 最精彩的语言 即兴对话可以产生一些最精彩的语言。当然如果你没有准备或者你对现场缺乏敏感性，结果可能会非常失败。相反，如果你有备而来，优雅而庄重，你的演讲会令人着迷。 使你魅力非凡 拥有当领导者的意愿即想要感染他人的欲望：不管是帮助他人形成自己的观点、影响他人行为，还是在人际层面与他们联结并使其感受到工作场所或生活的美好。 睿智的领导者知道什么时候应当抓住领导力时刻，以下为确定最佳领导力时刻的方法： 选做合适的时间和地点 整理思路 获得观众充分的注意力 谈话内容要有价值 建立关系 面对面沟通 得体沟通 在组织中任何人都需要深刻理解权力的界限，不符合身份、不得体的沟通会阻碍职业生涯发展。 意识到“麦克风始终开着” 成为一名听众斯多葛派哲学家Epictetus观察到：“人有两只耳朵却只有一张嘴巴，因此，我们听的多，它是我们说的两倍。” 真诚聆听他人是即兴思维的重要组成部分。 积极倾听不仅需要“带着耳朵”去听，还需要我们全身心的参与。掌握了积极倾听的艺术，你就有能力充分利用即兴时刻。 用你的身体：身体倾听 用你的大脑：理智倾听 用你的心：有感情地倾听 领导力思维必须包含倾听的欲望和倾听的能力。 保持真实即兴对话冲破了传统组织规则的约束，是判断人是否具有真实性的最佳途径。 什么是真实性领导力 它指分享定义我们自己、属于我们自己的那些真实特征，包括拥有领导意愿，倾听、尊重他人的观点和尊严等等。但还不止于此，它还需要具有代表最佳领导力的价值观和信念。 展示真实性的策略 真实地处于当下 分享你的想法 分享你的信念和价值观 分享你的感受 分享你的脆弱 分享你的故事 保持专注但凡伟大的领导者都目标明确、意志坚定。 保持尊重真诚、敞开心扉的讲话并不意味着随心所欲，随着即兴发言变得越来越普遍，随着组织层级和组织界限的模糊，每个人都需要有尊重的意识。 尊重你的组织 尊重你的管理者 即兴发言时一定要避免对上司或管理层的不敬 尊重你的同事 尊重你自己 做好准备记住，每次说话都是一个潜在的领导力时刻，而花些时间做好准备将使你能够最大限度地利用这个机会发挥影响力。 了解主题 首先是科学知识 其次是一般性知识 最后是经验性知识 准备即兴演讲首先要确保有扎实的知识基础，还需要有不同领域的广泛知识和经验。 牢记领导力信息 无论你所在组织、团队如何，一定要写下关键信息和其他支持性信息，并且将这些信息刻在脑子里，每次讲话时都引用它，这些将构成你即兴演讲草稿的基础。 了解听众“我心里总是有两个声道在同时播放，一个声道播放着我正在想的东西，另一个播放着我认为听众想要或需要的或他们期待我讲的。于是，我的一半思考专注于我自己的声道，另一半思考则停留在和听众共鸣的地方。” –保罗瓦利 只在讲话的时候了解听众是不够的，还应当在讲话前、讲话中和讲话后了解观众。 了解听众：讲话前 事先分析听众会让我们知道如何可以讲得更好。 了解听众：讲话中 在讲话时，一定要记住下面的问题，这些问题会引导你更好地了解听众。 听众在听吗？ 哪些观点有吸引力？ 组织文化是怎样的？ 如今组织的政治现状如何？ 男女员工表现如何？ 了解听众：讲话后 事后评估非常重要，是我们提高演讲技能的好方法。 领导者讲话脚本模板","link":"/2020/09/29/IMPROMPTU/"},{"title":"InnoDB","text":"InnoDB的架构设计https://dev.mysql.com/doc/refman/8.0/en/innodb-architecture.html 由图可知，InnoDB主要分为两部分： InnoDB In-Memory Structures InnoDB On-Disk Structures InnoDB In-Memory StructuresMySQL如果发现需要修改的页不在内存里，就先将对页的修改记到Change Buffer，同时记录redo log，然后慢慢把数据load到内存，load完成后，再将Change Buffer里记录的修改应用到Buffer Pool（merge），之后再把数据刷到磁盘（purge）。 Buffer Poolhttps://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html 数据load的地方就是Buffer Pool，类似缓存。 Buffer Pool是一个以页为元素的链表，基于LRU算法来管理内存。 1show engine innodb status; 在”BUFFER POOL AND MEMORY”部分可以看到Buffer Pool的大小； Change Bufferhttps://dev.mysql.com/doc/refman/8.0/en/innodb-change-buffer.html merge：Change Buffer -&gt; Buffer Pool purge：Buffer Pool -&gt; Disk Change Buffer只在操作Secondary Index（二级索引）时才使用，因为聚簇索引必须是唯一的，即每次INSERT，UPDATE，DELETE时都需要检查是否已经存在相同的字段，所以就没必要使用Change Buffer了；而且聚簇索引操作的随机性比较小，通常在相邻的页操作，而二级索引访问非常随机。 Adaptive Hash Index因为B+ Tree的查找次数取决于树的高度，在很多场景中，比如存在被频繁访问的数据，每次都要走B+ Tree查询，如果此时使用一个指针就可以把数据的位置记下就好了。 这就是Adaptive Hash Index自适应哈希，MySQL会自动评估使用自适应索引是否值得，如果观察到建立哈希索引可以提升速度，则建立。 If a table fits almost entirely in main memory, a hash index can speed up queries by enabling direct lookup of any element, turning the index value into a sort of pointer. InnoDB has a mechanism that monitors index searches. If InnoDB notices that queries could benefit from building a hash index, it does so automatically. Log Buffer从架构图可以看到，Log Buffer里的redo log会被刷到磁盘。 The log buffer is the memory area that holds data to be written to the log files on disk. InnoDB On-Disk Structureshttps://dev.mysql.com/doc/refman/8.0/en/innodb-on-disk-structures.html TableSpaces 表空间https://dev.mysql.com/doc/refman/8.0/en/innodb-tablespace.html 分为五种： The System Tablespace File-Per-Table Tablespaces General Tablespace Undo Tablespaces Temporary Tablespaces 平时创建的表数据，可以存放到The System Tablespace，File-Per-Table Tablespaces，General Tablespace三者中的任意一个地方，具体取决于配置和创建表时的SQL语句。 Doublewrite BufferChange Buffer提升性能，Doublewrite Buffer保证数据页的可靠性。 “双写缓冲”，MySQL在刷数据到磁盘之前，要先把数据写到Doublewrite Buffer，写完后，再开始写磁盘。Doublewrite Buffer可以理解为是一个备份，如果发生 crash，就可以利用Doublewrite Buffer来修复磁盘里的数据。 设置InnoDB_doublewrite=0，即可关闭Doublewrite Buffer。 The doublewrite buffer is a storage area where InnoDB writes pages flushed from the buffer pool before writing the pages to their proper positions in the InnoDB data files. If there is an operating system, storage subsystem, or mysqld process crash in the middle of a page write, InnoDB can find a good copy of the page from the doublewrite buffer during crash recovery. Although data is written twice, the doublewrite buffer does not require twice as much I/O overhead or twice as many I/O operations. Data is written to the doublewrite buffer in a large sequential chunk, with a single fsync() call to the operating system (except in the case that innodb_flush_method is set to O_DIRECT_NO_FSYNC). Prior to MySQL 8.0.20, the doublewrite buffer storage area is located in the InnoDB system tablespace. As of MySQL 8.0.20, the doublewrite buffer storage area is located in doublewrite files.","link":"/2020/07/28/InnoDB/"},{"title":"Lc-1114","text":"Lc 1114.按序打印我们提供了一个类： public class Foo { public void one() { print(“one”); } public void two() { print(“two”); } public void three() { print(“three”); }}三个不同的线程将会共用一个 Foo 实例。 线程 A 将会调用 one() 方法线程 B 将会调用 two() 方法线程 C 将会调用 three() 方法请设计修改程序，以确保 two() 方法在 one() 方法之后被执行，three() 方法在 two() 方法之后被执行。 1234567891011121314151617181920212223242526272829303132333435363738class Foo { boolean firstFinished = false; boolean secondFinished = false; final Object obj = new Object(); public Foo() { } public void first(Runnable printFirst) throws InterruptedException { synchronized (obj) { // printFirst.run() outputs &quot;first&quot;. Do not change or remove this line. printFirst.run(); firstFinished = true; } } public void second(Runnable printSecond) throws InterruptedException { synchronized (obj) { while (firstFinished) { // printSecond.run() outputs &quot;second&quot;. Do not change or remove this line. printSecond.run(); secondFinished = true; } } } public void third(Runnable printThird) throws InterruptedException { synchronized (obj) { while (secondFinished) { // printThird.run() outputs &quot;third&quot;. Do not change or remove this line. printThird.run(); } } }} 提交结果：超时 原因：while循环，不停抢占CPU资源来判断自身是否结束循环（自旋）。 优化使用wait+notify避免了自旋 1234567891011121314151617181920212223242526272829303132333435363738394041class Foo { final Object obj = new Object(); boolean firstFinished = false; boolean secondFinished = false; public Foo() { } public void first(Runnable printFirst) throws InterruptedException { synchronized (obj) { // printFirst.run() outputs &quot;first&quot;. Do not change or remove this line. printFirst.run(); firstFinished = true; obj.notifyAll(); } } public void second(Runnable printSecond) throws InterruptedException { synchronized (obj) { while (!firstFinished) { obj.wait(); } // printSecond.run() outputs &quot;second&quot;. Do not change or remove this line. printSecond.run(); secondFinished = true; obj.notifyAll(); } } public void third(Runnable printThird) throws InterruptedException { synchronized (obj) { while (!secondFinished) { obj.wait(); } // printThird.run() outputs &quot;third&quot;. Do not change or remove this line. printThird.run(); } }} 优化：使用CountDownLatch（Semaphore同理） 1234567891011121314151617181920212223242526272829class Foo { CountDownLatch c2; CountDownLatch c3; public Foo() { c2 = new CountDownLatch(1); c3 = new CountDownLatch(1); } public void first(Runnable printFirst) throws InterruptedException { // printFirst.run() outputs &quot;first&quot;. Do not change or remove this line. printFirst.run(); c2.countDown(); } public void second(Runnable printSecond) throws InterruptedException { c2.await(); // printSecond.run() outputs &quot;second&quot;. Do not change or remove this line. printSecond.run(); c3.countDown(); } public void third(Runnable printThird) throws InterruptedException { c3.await(); // printThird.run() outputs &quot;third&quot;. Do not change or remove this line. printThird.run(); }}","link":"/2020/04/13/Lc-1114/"},{"title":"Lcof03","text":"Lcof 03.数组中重复的数字找出数组中重复的数字。 在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。 示例 1： ​ 输入：[2, 3, 1, 0, 2, 5, 3]​ 输出：2 或 3 123456789101112131415161718192021 class Solution { public int findRepeatNumber(int[] nums) { if (nums == null || nums.length &lt;= 0) { return 0; } int i = 0; while (i &lt; nums.length) { if (nums[i] == i) { i++; continue; } if (nums[nums[i]] == nums[i]) { return nums[i]; } int tmp = nums[i]; nums[i] = nums[nums[i]]; nums[tmp] = tmp; } return 0; }} 复杂度分析： 时间复杂度O(N) 空间复杂度O(1)","link":"/2020/05/20/Lcof03/"},{"title":"Lc-1115","text":"Lc 1115.交替打印FooBar我们提供一个类： 12345678910111213class FooBar { public void foo() { for (int i = 0; i &lt; n; i++) { print(&quot;foo&quot;); } } public void bar() { for (int i = 0; i &lt; n; i++) { print(&quot;bar&quot;); } }} 两个不同的线程将会共用一个 FooBar 实例。其中一个线程将会调用 foo() 方法，另一个线程将会调用 bar() 方法。 请设计修改程序，以确保 “foobar” 被输出 n 次。 12345678910111213141516171819202122232425262728293031323334353637383940class FooBar { private int n; private boolean isFooTurn = true; private Object obj = new Object(); public FooBar(int n) { this.n = n; } public void foo(Runnable printFoo) throws InterruptedException { for (int i = 0; i &lt; n; i++) { synchronized (obj){ if (!isFooTurn){ obj.wait(); } // printFoo.run() outputs &quot;foo&quot;. Do not change or remove this line. printFoo.run(); isFooTurn = false; obj.notifyAll(); } } } public void bar(Runnable printBar) throws InterruptedException { for (int i = 0; i &lt; n; i++) { synchronized (obj){ if (isFooTurn){ obj.wait(); } // printBar.run() outputs &quot;bar&quot;. Do not change or remove this line. printBar.run(); isFooTurn = true; obj.notifyAll(); } } }} 优化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class FooBar { private int n; private final Lock lock = new ReentrantLock(); private boolean allowedAProcess = true; private final Condition conditionA = lock.newCondition(); private final Condition conditionB = lock.newCondition(); public FooBar(int n) { this.n = n; } public void foo(Runnable printFoo) throws InterruptedException { for (int i = 0; i &lt; n; i++) { lock.lock(); try { while (!allowedAProcess) { conditionA.await(); } // printFoo.run() outputs &quot;foo&quot;. Do not change or remove this line. printFoo.run(); allowedAProcess = false; conditionB.signalAll(); } finally { lock.unlock(); } } } public void bar(Runnable printBar) throws InterruptedException { for (int i = 0; i &lt; n; i++) { lock.lock(); try { while (allowedAProcess) { conditionB.await(); } // printBar.run() outputs &quot;bar&quot;. Do not change or remove this line. printBar.run(); allowedAProcess = true; conditionA.signalAll(); } finally { lock.unlock(); } } }}","link":"/2020/04/16/Lc-1115/"},{"title":"Lcof04","text":"Lcof 04.二维数组中的查找在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 示例: 现有矩阵 matrix 如下： [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]给定 target = 5，返回 true。 给定 target = 20，返回 false。 12345678910111213141516171819202122class Solution { public boolean findNumberIn2DArray(int[][] matrix, int target) { if (matrix == null || matrix.length == 0 || matrix[0].length == 0) { return false; } // 行 int i = matrix.length - 1; // 列 int j = 0; // 从左下角开始 while (i &gt;= 0 &amp;&amp; j &lt; matrix[0].length) { if (target &lt; matrix[i][j]) { i--; } else if (target &gt; matrix[i][j]) { j++; } else { return true; } } return false; }} 复杂度分析 时间复杂度：O(n+m)O(n+m)。访问到的下标的行最多增加 n 次，列最多减少 m 次，因此循环体最多执行 n + m 次。空间复杂度：O(1)O(1)。","link":"/2020/05/25/Lcof04/"},{"title":"Lcof05","text":"Lcof 05.替换空格请实现一个函数，把字符串 s 中的每个空格替换成”%20”。 示例 1： ​ 输入：s = “We are happy.”​ 输出：”We%20are%20happy.” 12345678910111213class Solution { public String replaceSpace(String s) { StringBuffer stringbuffer = new StringBuffer(); for (int i = 0; i &lt; s.length(); i++) { if (s.charAt(i) == ' ') { stringbuffer.append(&quot;%20&quot;); } else { stringbuffer.append(s.charAt(i)); } } return stringbuffer.toString(); }} 优化：1）p1指针指向初始字符串末尾位置； 2）遍历字符串，当遍历到空格时，在尾部填充两个任意字符，使得字符串长度等于替换后的长度； 3）p2指针指向现在遍历填充后的字符串末尾位置； 4）p1，p2指针从后向前遍历，当p1指向空格时，p2指向的位置依次填充’0’ ‘2’ ‘%’，否则就直接填充p1指向的字符值； 5）当p2遇到p1或者遍历结束，退出循环； 123456789101112131415161718192021222324class Solution { public String replaceSpace(String s) { StringBuffer stringbuffer = new StringBuffer(s); int p1 = stringbuffer.length() - 1; for (int i = 0; i &lt;= p1; i++) { if (stringbuffer.charAt(i) == ' ') { // 在字符串尾部添加两个空格 stringbuffer.append(&quot; &quot;); } } int p2 = stringbuffer.length() - 1; while (p1 &gt;= 0 &amp;&amp; p1 &lt; p2) { char tmp = stringbuffer.charAt(p1--); if (tmp != ' ') { stringbuffer.setCharAt(p2--, tmp); } else { stringbuffer.setCharAt(p2--, '0'); stringbuffer.setCharAt(p2--, '2'); stringbuffer.setCharAt(p2--, '%'); } } return stringbuffer.toString(); }}","link":"/2020/05/27/Lcof05/"},{"title":"Lcof06","text":"Lcof 06.从尾到头打印链表输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。 示例 1： ​ 输入：head = [1,3,2]​ 输出：[2,3,1] 因为栈具有先进后出的特点，所以在遍历链表时将值push到栈中，pop的顺序即为逆序。 1234567891011121314151617class Solution { public int[] reversePrint(ListNode head) { Stack&lt;Integer&gt; tmp = new Stack&lt;Integer&gt;(); ListNode temp = head; while (temp != null) { tmp.push(temp.val); temp = temp.next; } // 要记录size，因为pop后size()会改变 int size = tmp.size(); int[] ret = new int[size]; for (int i = 0; i &lt; size; i++) { ret[i] = tmp.pop(); } return ret; }}","link":"/2020/05/31/Lcof06/"},{"title":"Lcof09","text":"Lcof 09.用两个栈实现队列用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deleteHead 操作返回 -1 ) 1234567891011121314151617181920212223242526272829303132333435class CQueue { Stack in; Stack out; int size; public CQueue() { in = new Stack&lt;&gt;(); out = new Stack&lt;&gt;(); size=0; } public void appendTail(int value) { in.push(value); size++; } public int deleteHead() { if(size==0){ return -1; } if (out.isEmpty()) { while (!in.isEmpty()) { out.push(in.pop()); } } size--; return (int) out.pop(); }}/** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.deleteHead(); */ 优化因为Stack继承了Vector接口，而Vector底层是一个Object[]数组，所以要考虑空间扩容和移位的问题。 可以使用LinkedList来做Stack的容器，其本身结构是个双向链表，扩容消耗少。 1234567891011121314151617181920212223class CQueue { LinkedList&lt;Integer&gt; stack1; LinkedList&lt;Integer&gt; stack2; public CQueue() { stack1 = new LinkedList&lt;&gt;(); stack2 = new LinkedList&lt;&gt;(); } public void appendTail(int value) { stack1.add(value); } public int deleteHead() { if (stack2.isEmpty()) { if (stack1.isEmpty()) return -1; while (!stack1.isEmpty()) { stack2.add(stack1.pop()); } return stack2.pop(); } else return stack2.pop(); }} 关系图","link":"/2020/06/02/Lcof09/"},{"title":"Lcof10_1","text":"Lcof 10-1.斐波那契数列写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项。斐波那契数列的定义如下： F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1. 斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 1234567891011class Solution { public int fib(int n) { if (n == 0) { return 0; } else if (n == 1) { return 1; } else { return fib(n - 1) + fib(n - 2); } }} 提交结果：超时 原因： 大量重复的递归计算。 优化： 1234567891011121314class Solution { public int fib(int n) { if (n &lt;= 1) { return n; } int a = 0, b = 1, sum = 1; for (int i = 1; i &lt; n; i++) { sum = (a + b) % 1000000007; a = b; b = sum; } return sum; }}","link":"/2020/06/07/Lcof10-1/"},{"title":"Lcof10-2","text":"Lcof 10-2.青蛙跳台阶问题一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 示例 1： ​ 输入：n = 2​ 输出：2示例 2： ​ 输入：n = 7​ 输出：21 1234567891011121314151617class Solution { public int numWays(int n) { if (n == 0) { return 1; } if (n &lt;= 2) { return n; } int a = 1, b = 2, sum = 1; for (int i = 2; i &lt; n; i++) { sum = (a + b) % 1000000007; a = b; b = sum; } return sum; }}","link":"/2020/06/13/Lcof10-2/"},{"title":"Lcof11","text":"Lcof 11.旋转数组的最小数字把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。 示例 1： ​ 输入：[3,4,5,1,2]​ 输出：1示例 2： ​ 输入：[2,2,2,0,1]​ 输出：0 12345678910class Solution { public int minArray(int[] numbers) { for (int i = 0;i &lt; numbers.length - 1; i++) { if (numbers[i] &gt; numbers[i + 1]) { return numbers[i + 1]; } } return numbers[0]; }}","link":"/2020/06/18/Lcof11/"},{"title":"Lcof15","text":"Lcof 15.二进制中1的个数请实现一个函数，输入一个整数，输出该数二进制表示中 1 的个数。例如，把 9 表示成二进制是 1001，有 2 位是 1。因此，如果输入 9，则该函数输出 2。 示例 1： ​ 输入：00000000000000000000000000001011​ 输出：3​ 解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 ‘1’。示例 3： ​ 输入：11111111111111111111111111111101​ 输出：31​ 解释：输入的二进制串 11111111111111111111111111111101 中，共有 31 位为 ‘1’。 若 n &amp; 1 = 0 ，则n二进制最右一位为0；若 n &amp; 1 = 1 ，则n二进制最右一位为1； 12345678910111213public class Solution { public int hammingWeight(int n) { int ret = 0; while (n != 0) { // 需注意位运算符的优先级 ret = ret + (n &amp; 1); // 要使用无符号右移，若使用有符号右移，对于负数 最高位填1， // 无法达到n = 0，无法跳出循环 n = n &gt;&gt;&gt; 1; } return ret; } } 复杂度分析： 时间复杂度O(logn)：因为循环内部只有移位，加，与等基本运算，占用O(1)，逐位判断需循环logn次，其中n代表最高位1的所在位数； 空间复杂度O(1)； 优化(n - 1)：二进制数字n最右边的1变成0，此1右边的0都变成1； n &amp; (n - 1)：二进制最右边的1变成0，其余不变； 1234567891011class Solution { // you need to treat n as an unsigned value public int hammingWeight(int n) { int res = 0; while (n != 0) { res += 1; n = n &amp; (n - 1); } return res; }} 复杂度分析： 时间复杂度O(M)：n &amp; (n - 1)操作仅有减法和与运算，占用O(1)，M为二进制数字n中1的个数； 空间复杂度O(1)；","link":"/2020/06/19/Lcof15/"},{"title":"Lcof18","text":"Lcof 18.删除链表的节点给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。 返回删除后的链表的头节点。 注意：此题对比原题有改动 示例 1: ​ 输入: head = [4,5,1,9], val = 5​ 输出: [4,1,9]​ 解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 1 -&gt; 9.示例 2: ​ 输入: head = [4,5,1,9], val = 1​ 输出: [4,5,9]​ 解释: 给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 5 -&gt; 9. 12345678910111213141516171819class Solution { public ListNode deleteNode(ListNode head, int val) { if (head == null) { return null; } if (head.val == val) { return head.next; } ListNode res = head; // 当下一个节点是删除节点时跳出循环 while(head.next != null &amp;&amp; head.next.val != val) { head = head.next; } if (head.next != null) { head.next = head.next.next; } return res; }} 原题： 123456789101112131415161718192021public ListNode deleteNode(ListNode head, ListNode tobeDelete) { if (head == null || tobeDelete == null) return null; if (tobeDelete.next != null) { // 要删除的节点不是尾节点 ListNode next = tobeDelete.next; tobeDelete.val = next.val; tobeDelete.next = next.next; } else { if (head == tobeDelete) // 只有一个节点 head = null; else { ListNode cur = head; while (cur.next != tobeDelete) cur = cur.next; cur.next = null; } } return head;}","link":"/2020/06/22/Lcof18/"},{"title":"Lcof22","text":"Lcof 22.链表中倒数第k个节点输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。 示例： ​ 给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 k = 2. ​ 返回链表 4-&gt;5. 123456789101112131415161718192021class Solution { public ListNode getKthFromEnd(ListNode head, int k) { if (head == null || k &lt; 0) { return null; } ListNode right = head; while (right != null &amp;&amp; k &gt; 0) { right = right.next; k--; } if (k &gt; 0) { return null; } ListNode left = head; while (right != null) { left = left.next; right = right.next; } return left; }} 优化 1234567891011121314class Solution { public ListNode getKthFromEnd(ListNode head, int k) { ListNode fast = head; while (fast != null) { if (k == 0) { head = head.next; } else { k--; } fast = fast.next; } return head; }}","link":"/2020/06/28/Lcof22/"},{"title":"Lcof21","text":"Lcof 21.调整数组顺序使奇数位于偶数前面输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分。 示例： ​ 输入：nums = [1,2,3,4]​ 输出：[1,3,2,4]​ 注：[3,1,2,4] 也是正确的答案之一。 快慢指针： 1234567891011121314151617181920class Solution { public int[] exchange(int[] nums) { if (nums == null || nums.length == 0) { return nums; } int i = 0, j = 0; while (j != nums.length) { if (nums[j] % 2 == 1) { if (i != j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } i++; } j++; } return nums; }} 两端指针： 123456789101112131415161718class Solution { public int[] exchange(int[] nums) { int left = 0, right = nums.length - 1, tmp; while (left &lt; right) { // x&amp;1位运算 等价于 x%2取余运算 while (left &lt; right &amp;&amp; (nums[left] &amp; 1) != 0) { left++; } while (left &lt; right &amp;&amp; (nums[right] &amp; 1) != 1) { right--; } tmp = nums[left]; nums[left] = nums[right]; nums[right] = tmp; } return nums; }} 复杂度分析：时间复杂度O(N)：N为数组nums长度，双指针i，j共同遍历整个数组；空间复杂度O(1)：双指针i，j使用常数大小的额外空间；","link":"/2020/06/25/Lcof21/"},{"title":"Lcof24","text":"Lcof 24.反转链表定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。 示例: ​ 输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL​ 输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 双指针迭代 12345678910111213class Solution { public ListNode reverseList(ListNode head) { ListNode pre = null; ListNode cur = head; while (cur != null) { ListNode tmp = cur.next; cur.next = pre; pre = cur; cur = tmp; } return pre; }} 递归 12345678910111213141516171819class Solution { public ListNode reverseList(ListNode head) { //递归终止条件是当前为空，或者下一个节点为空 if(head==null || head.next==null) { return head; } //这里的cur就是最后一个节点 ListNode cur = reverseList(head.next); //这里请配合动画演示理解 //如果链表是 1-&gt;2-&gt;3-&gt;4-&gt;5，那么此时的cur就是5 //而head是4，head的下一个是5，下下一个是空 //所以head.next.next 就是5-&gt;4 head.next.next = head; //防止链表循环，需要将head.next设置为空 head.next = null; //每层递归函数都返回cur，也就是最后一个节点 return cur; }}","link":"/2020/07/05/Lcof24/"},{"title":"Lcof25","text":"Lcof 25.合并两个排序的链表输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。 示例1： ​ 输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4​ 输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 迭代 1234567891011121314151617181920212223class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { ListNode head = new ListNode(-1); ListNode res = head; while (l1 != null &amp;&amp; l2 != null) { if (l1.val &lt; l2.val) { res.next = l1; l1 = l1.next; } else { res.next = l2; l2 = l2.next; } res = res.next; } if (l1 == null) { res.next = l2; } if (l2 == null) { res.next = l1; } return head.next; }} 递归 1234567891011121314151617class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null) { return l2; } if (l2 == null) { return l1; } if (l1.val &lt; l2.val) { l1.next = mergeTwoLists(l1.next, l2); return l1; } else { l2.next = mergeTwoLists(l1, l2.next); return l2; } }}","link":"/2020/07/06/Lcof25/"},{"title":"MQ","text":"消息队列顺序消费不同消息中间件的解决方案不同。 RocketMQ提供了MessageQueueSelector选择机制，可以使用SelectMessageQueueByHash，是同一个订单发送到 同一个队列中，再使用同步发送，只有同个订单的创建消息发送成功，再发送支付消息，即保证了发送有序。 RabbitMQ如何保证消息的可靠性传输（如何处理消息的丢失问题） 生产者丢了数据 RabbitMQ的事务功能 生产者发送数据之前开启RabbitMQ事务（channel.txSelect），然后发送消息，如果消息没有成功被RabbitMQ接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。 缺点：吞吐量下降，太耗性能。 confirm机制 在生产者那里设置开启confirm模式之后，每次写的消息都会分配一个唯一的id，然后如果写入了RabbitMQ中，RabbitMQ会给你回传一个ack消息，告诉说这个消息ok了。如果RabbitMQ没能处理这个消息，会回调nack接口，告诉说这个消息接收失败，可以重试。而且可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么可以重发。 不同：事务机制是同步的，提交事务后会阻塞，但confirm机制是异步的，发送消息之后可以发送下一个消息，然后那个消息RabbitMQ接收了之后会异步回调接口通知你这个消息接收到了。 所以一般在生产者端避免数据丢失，都是用confirm机制的。 RabbitMQ丢失了数据 这个必须开启RabbitMQ的持久化，就是消息写入后会持久化到磁盘，RabbitMQ挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ还没持久化就挂了，可能导致少量数据会丢失的，但概率较小。 设置持久化有两个步骤： 创建queue的时候将其设置为持久化的，这样就可以保证RabbitMQ持久化queue的元数据，但是不会持久化queue里的数据； 发送消息时，将消息的deliveryMode设置为2，即将消息设置为持久化的，此时RabbitMQ就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，也是可以自己重发的。 消费者丢了数据 刚消费到，还没处理，此时进程挂了，比如重启，RabbitMQ认为你都消费了，这数据就丢了。 这时用RabbitMQ提供的ack机制，简单来说，就是关闭RabbitMQ自动ack，可以通过一个api来调用就行，然后每次代码里确保处理完的时候，再程序里ack。这样的话，如果还没处理完，就没有ack，那RabbitMQ就认为还没处理完，这个时候RabbitMQ会把这个消费分配给别的consumer去处理，消息是不会丢的。 如何保证消息队列的高可用RabbitMQ有三种模式：单机模式，普通集群模式，镜像集群模式。 单机模式 生产环境不会用到。 普通集群模式 多台机器上启动多个RabbitMQ实例，每个机器启动一个。但是创建的queue，只会放在一个RabbitMQ实例上，但是每个实例都同步queue的元数据。消费时，如果连接到了另一个实例，那么那个实例会从queue所在实例上拉取数据过来。 这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。 镜像集群模式 这种模式，才是所谓的RabbitMQ的高可用模式，跟普通集群模式不一样的是，创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。 缺点：性能开销较大，消息同步所有机器，导致网络带宽压力和消耗很重；扩展性较差。 RabbitMQ有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求就同步到指定数量的节点，然后你再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 如何保证消息不被重复消费（幂等性） 在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列； 在消息消费时，要求消息体中必须要有一个bizId（对于同一业务全局唯一，如支付ID、订单ID、帖子ID等）作为去重的依据，避免同一条消息被重复消费； 当下游业务异常时，触发消息队列的重试机制，但存在多个服务在监听该消息队列，可能导致消息重复消费，需保证接口幂等性。 幂等：同样的参数调用接口，结果都是一样的。 强校验 每次消息过来都要拿着订单号+业务场景这样的唯一标识去流水表查，看看有没有这条流水，有就直接return，没有就执行后面的逻辑。 123456789101112131415public void process (String orderId) { try{ // 查询这个订单是否存在这个活动加GMV的流水 Object gmvFlow = getFlowByOrderId(&quot;addGmv&quot; + orderId); if(Object.isNull(gmvFlow)) { // 不存在流水，去加GMV和加流水 需放在一个事务中 addGmvAndFlow(orderId); } else { // 存在流水证明加过了 直接返回 return; } } catch (Exception e) { // 发送异常 触发消息队列的重试机制 }} 弱校验 把这个id + 场景唯一标识，作为Redis的key，放到缓存里面（失效时间看场景），一定时间内的这个消息就去Redis判断。 如何保证消息的顺序性消息积压 大量消息在MQ里积压了几小时还没解决 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 1）先修复consumer的问题，确保其恢复消费速度，然后将现有consumer都停掉； 2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量； 3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue； 4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据； 5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据； 6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息； 设置了过期时间 这个情况下，实际上没啥积压，而是丢了大量的消息。可以采取批量重导，大量积压时，直接丢弃数据，然后等过了高峰期以后，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入MQ里面去，把白天丢的数据给他补回来。 假设1万个订单积压在MQ里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到MQ里去再补一次。 死信队列DLX，全称为 Dead-Letter-Exchange，当消息在一个队列中变成死信 (dead message) 之后，它能被重新被发送到另一个交换器中，这个交换器就是 DLX，绑定 DLX 的队列就称之为死信队列。DLX 也是一个正常的交换器，和一般的交换器没有区别，它能在任何的队列上被指定， 实际上就是设置某个队列的属性。当这个队列中存在死信时，RabbitMQ 就会自动地将这个消息重新发布到设置的 DLX 上去，进而被路由到另一个队列，即死信队列。 延迟队列延迟队列存储的对象是对应的延迟消息，所谓“延迟消息”是指当消息被发送后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费 应用场景：订单系统，用延迟队列处理超时订单；用户希望通过手机远程遥控家里的智能设备在指定的时间进行工作。这时候就可以将用户指令发送到延迟队列，当指令设定的时间到了再将指令推送到智能设备。持久化？ 使用RabbitMQ的场景服务间异步通信，顺序消费，定时任务，请求削峰 消息如何路由消息提供方 -&gt; 路由 -&gt; 一至多个队列消息发布到交换器时，消息将拥有一个路由键（routing key），在消息创建时设定。 通过binding key将Exchange和Queue链接在一起；消息到达交换器后，RabbitMQ会将消息的路由键与队列的路由键进行匹配（针对不同的交换器有不同的路由规则），常用的Exchange主要分为一下三种：fanout：它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中；direct：它会把消息路由到那些binding key与routing key完全匹配的Queue中；topic：可以通过通配符满足一部分规则就可以传送。它的约定是： routing key为一个句点号“. ”分隔的字符串（我们将被句点号“. ”分隔开的每一段独立的字符串称为一个单词），“stock.usd.nyse”、“nyse.vmw”、“quick.orange.rabbit”； binding key与routing key一样也是句点号“. ”分隔的字符串； binding key中可以存在两种特殊字符“ * ”与“#”，用于做模糊匹配，其中“ * ”用于匹配一个单词，“#”用于匹配多个单词（可以是零个）；","link":"/2020/07/07/MQ/"},{"title":"Lcof29","text":"Lcof 29. 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字。 示例 1： ​ 输入：matrix = [[1,2,3],[4,5,6],[7,8,9]]​ 输出：[1,2,3,6,9,8,7,4,5]示例 2： ​ 输入：matrix = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]​ 输出：[1,2,3,4,8,12,11,10,9,5,6,7] 1234567891011121314151617181920212223242526272829303132333435363738class Solution { public int[] spiralOrder(int[][] matrix) { if (matrix.length == 0) { return new int[0]; } int rows = matrix.length - 1; int columns = matrix[0].length - 1; int r = 0, c = 0, t = 0; int[] res = new int[(rows + 1) * (columns + 1)]; while (true) { for (int i = c; i &lt;= columns; i++) { res[t++] = matrix[r][i]; } if (++r &gt; rows) { break; } for (int i = r; i &lt;= rows; i++) { res[t++] = matrix[i][columns]; } if (--columns &lt; c) { break; } for (int i = columns; i &gt;= c; i--) { res[t++] = matrix[rows][i]; } if (--rows &lt; r) { break; } for (int i = rows; i &gt;= r; i--) { res[t++] = matrix[i][c]; } if (++c &gt; columns) { break; } } return res; }} 复杂度分析：时间复杂度O(MN)：M,N分别为矩阵行数和列数。空间复杂度O(1)：四个边界使用常数大小的额外空间（res为必须使用的空间）。","link":"/2020/07/09/Lcof29/"},{"title":"Lcof39","text":"Lcof 39.数组中出现次数超过一半的数字数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。你可以假设数组是非空的，并且给定的数组总是存在多数元素。 示例 1: ​ 输入: [1, 2, 3, 2, 2, 2, 5, 4, 2]​ 输出: 2 解法1：哈希表统计法，时间和空间复杂度均为O(N) 解法2：数组排序法，时间复杂度为O(NlongN) 解法3：摩尔投票法，时间复杂度为O(N)，空间复杂度为O(1) 算法原理： 使用 votes 来统计一个元素出现的次数，当遍历到的元素和统计元素相等时，令 votes++，否则令 votes–; 如果前面查找了 i 个元素，且 votes == 0，说明前 i 个元素没有 majority，或者有 majority，但是出现的次数少于 i / 2 ，因为如果多于 i / 2 的话 votes 就一定不会为 0； 此时剩下的 n - i 个元素中，majority 的数目依然多于 (n - i) / 2，因此继续查找就能找出 majority。 12345678910111213141516class Solution { public int majorityElement(int[] nums) { int votes = 0, majority = 0; for (int num : nums) { if (votes == 0) { majority = num; } if (num == majority) { votes++; } else { votes--; } } return majority; }}","link":"/2020/07/10/Lcof39/"},{"title":"RedisDistributedLock","text":"分布式锁CAP理论：任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。 特点 互斥性： 同一时刻只能有一个线程持有锁； 可重入性： 同一节点上的同一个线程如果获取了锁之后能够再次获取锁； 锁超时：和J.U.C中的锁一样支持锁超时，防止死锁； 高性能和高可用： 加锁和解锁需要高效，同时也需要保证高可用，防止分布式锁失效； 具备阻塞和非阻塞性：能够及时从阻塞状态中被唤醒； 实现方式 基于数据库 基于Redis 基于zookeeper 基于Redis实现的分布式锁利用 setnx + expire 命令（错误的做法）SETNX key value SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 只在键 key 不存在的情况下， 将键 key 的值设置为 value；若键 key 已经存在， 则 SETNX 命令不做任何动作。 返回值：命令在设置成功时返回 1 ， 设置失败时返回 0 。 因为分布式锁还需要超时机制，所以我们利用expire命令来设置，所以利用 SETNX + expire 命令的核心代码如下： 123456789public boolean tryLock(String key,String requset,int timeout) { Long result = jedis.setnx(key, requset); // result = 1时，设置成功，否则设置失败 if (result == 1L) { return jedis.expire(key, timeout) == 1L; } else { return false; }} 存在问题：SETNX 和 expire 是分开的两步操作，不具有原子性，如果执行完第一条指令应用异常或者重启了，锁将无法过期。 使用 SET key value [EX seconds] [PX milliseconds] [NX | XX] 命令（正确做法）如果 key 已经持有其他值，SET 就覆写旧值， 无视类型；当 SET 命令对一个带有生存时间（TTL）的键进行设置之后，该键原有的 TTL 将被清除。 从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX seconds ：将键的过期时间设置为 seconds 秒。执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value ； PX milliseconds ：将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value； NX ：只在键不存在时， 才对键进行设置操作。 执行 SET key value NX 的效果等同于执行 SETNX key value； XX ：只在键已经存在时， 才对键进行设置操作； 123public boolean tryLock_with_set(String key, String UniqueId, int seconds) { return &quot;OK&quot;.equals(jedis.set(key, UniqueId, &quot;NX&quot;, &quot;EX&quot;, seconds));} value必须要具有唯一性，我们可以用UUID来做，设置随机字符串保证唯一性，至于为什么要保证唯一性？假如value不是随机字符串，而是一个固定值，那么就可能存在下面的问题： 客户端1获取锁成功； 客户端1在某个操作上阻塞了太长时间； 设置的key过期了，锁自动释放了； 客户端2获取到了对应同一个资源的锁； 客户端1从阻塞中恢复过来，因为value值一样，所以执行释放锁操作时就会释放掉客户端2持有的锁，这样就会造成问题； 所以通常来说，在释放锁时，我们需要对value进行验证。 释放锁的实现释放锁时需要验证value值，也就是说我们在获取锁的时候需要设置一个value，不能直接用del key这种粗暴的方式，因为直接del key任何客户端都可以进行解锁了，所以解锁时，我们需要判断锁是否是自己的，基于value值来判断，代码如下： 12345public boolean releaseLock_with_lua(String key,String value) { String luaScript = &quot;if redis.call('get',KEYS[1]) == ARGV[1] then &quot; + &quot;return redis.call('del',KEYS[1]) else return 0 end&quot;; return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L);} 这里使用Lua脚本的方式，尽量保证原子性。使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 看上去很OK，实际上在Redis集群的时候也会出现问题，比如说A客户端在Redis的master节点上拿到了锁，但是这个加锁的key还没有同步到slave节点，master故障，发生故障转移，一个slave节点升级为master节点，B客户端也可以获取同个key的锁，但客户端A也已经拿到锁了，这就导致多个客户端都拿到锁。所以针对Redis集群这种情况，还有其他方案。 Redlock算法 与 Redisson 实现 Redlock 算法 Redisson 实现 Jedis 是阻塞式I/O，而 Redisson 底层使用Netty可以实现非阻塞I/O，该客户端封装了锁的，继承了 J.U.C 的Lock接口，所以我们可以像使用 ReentrantLock 一样使用 Redisson ，具体使用过程如下。 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.10.6&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920// 1. 配置文件Config config = new Config();config.useSingleServer() .setAddress(&quot;redis://127.0.0.1:6379&quot;) .setPassword(RedisConfig.PASSWORD) .setDatabase(0);//2. 构造RedissonClientRedissonClient redissonClient = Redisson.create(config);//3. 设置锁定资源名称RLock lock = redissonClient.getLock(&quot;redlock&quot;);lock.lock();try { System.out.println(&quot;获取锁成功，实现业务逻辑&quot;); Thread.sleep(10000);} catch (InterruptedException e) { e.printStackTrace();} finally { lock.unlock();} Redis 实现的分布式锁轮子自定义注解被注解的方法会执行获取分布式锁的逻辑 12345678910111213141516171819202122232425262728293031@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documented@Inheritedpublic @interface RedisLock { /** * 业务键 * * @return */ String key(); /** * 锁的过期秒数,默认是5秒 * * @return */ int expire() default 5; /** * 尝试加锁，最多等待时间 * * @return */ long waitTime() default Long.MIN_VALUE; /** * 锁的超时时间单位 * * @return */ TimeUnit timeUnit() default TimeUnit.SECONDS;} AOP拦截器实现在AOP中我们去执行获取分布式锁和释放分布式锁的逻辑 12345678910111213141516171819202122232425262728293031323334353637@Aspect@Componentpublic class LockMethodAspect { @Autowired private RedisLockHelper redisLockHelper; @Autowired private JedisUtil jedisUtil; private Logger logger = LoggerFactory.getLogger(LockMethodAspect.class); @Around(&quot;@annotation(com.redis.lock.annotation.RedisLock)&quot;) public Object around(ProceedingJoinPoint joinPoint) { Jedis jedis = jedisUtil.getJedis(); MethodSignature signature = (MethodSignature) joinPoint.getSignature(); Method method = signature.getMethod(); RedisLock redisLock = method.getAnnotation(RedisLock.class); String value = UUID.randomUUID().toString(); String key = redisLock.key(); try { final boolean islock = redisLockHelper.lock(jedis,key, value, redisLock.expire(), redisLock.timeUnit()); logger.info(&quot;isLock : {}&quot;,islock); if (!islock) { logger.error(&quot;获取锁失败&quot;); throw new RuntimeException(&quot;获取锁失败&quot;); } try { return joinPoint.proceed(); } catch (Throwable throwable) { throw new RuntimeException(&quot;系统异常&quot;); } } finally { logger.info(&quot;释放锁&quot;); redisLockHelper.unlock(jedis,key, value); jedis.close(); } }} Redis实现分布式锁核心类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107@Componentpublic class RedisLockHelper { private long sleepTime = 100; /** * 直接使用setnx + expire方式获取分布式锁 * 非原子性 * * @param key * @param value * @param timeout * @return */ public boolean lock_setnx(Jedis jedis,String key, String value, int timeout) { Long result = jedis.setnx(key, value); // result = 1时，设置成功，否则设置失败 if (result == 1L) { return jedis.expire(key, timeout) == 1L; } else { return false; } } /** * 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作 * * @param jedis * @param key * @param UniqueId * @param seconds * @return */ public boolean Lock_with_lua(Jedis jedis,String key, String UniqueId, int seconds) { String lua_scripts = &quot;if redis.call('setnx',KEYS[1],ARGV[1]) == 1 then&quot; + &quot;redis.call('expire',KEYS[1],ARGV[2]) return 1 else return 0 end&quot;; List&lt;String&gt; keys = new ArrayList&lt;&gt;(); List&lt;String&gt; values = new ArrayList&lt;&gt;(); keys.add(key); values.add(UniqueId); values.add(String.valueOf(seconds)); Object result = jedis.eval(lua_scripts, keys, values); //判断是否成功 return result.equals(1L); } /** * 在Redis的2.6.12及以后中,使用 set key value [NX] [EX] 命令 * * @param key * @param value * @param timeout * @return */ public boolean lock(Jedis jedis,String key, String value, int timeout, TimeUnit timeUnit) { long seconds = timeUnit.toSeconds(timeout); return &quot;OK&quot;.equals(jedis.set(key, value, &quot;NX&quot;, &quot;EX&quot;, seconds)); } /** * 自定义获取锁的超时时间 * * @param jedis * @param key * @param value * @param timeout * @param waitTime * @param timeUnit * @return * @throws InterruptedException */ public boolean lock_with_waitTime(Jedis jedis,String key, String value, int timeout, long waitTime,TimeUnit timeUnit) throws InterruptedException { long seconds = timeUnit.toSeconds(timeout); while (waitTime &gt;= 0) { String result = jedis.set(key, value, &quot;nx&quot;, &quot;ex&quot;, seconds); if (&quot;OK&quot;.equals(result)) { return true; } waitTime -= sleepTime; Thread.sleep(sleepTime); } return false; } /** * 错误的解锁方法—直接删除key * * @param key */ public void unlock_with_del(Jedis jedis,String key) { jedis.del(key); } /** * 使用Lua脚本进行解锁操纵，解锁的时候验证value值 * * @param jedis * @param key * @param value * @return */ public boolean unlock(Jedis jedis,String key,String value) { String luaScript = &quot;if redis.call('get',KEYS[1]) == ARGV[1] then &quot; + &quot;return redis.call('del',KEYS[1]) else return 0 end&quot;; return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L); }} Controller层控制定义一个TestController来测试我们实现的分布式锁 12345678@RestControllerpublic class TestController { @RedisLock(key = &quot;redis_lock&quot;) @GetMapping(&quot;/index&quot;) public String index() { return &quot;index&quot;; }} 锁续命","link":"/2020/06/25/RedisDistributedLock/"},{"title":"SeckillThinking","text":"秒杀系统设计及问题解决项目地址：https://github.com/JiayiY/SeckillSystem 流程图 链接暴露解决方案：URL动态化，MD5加密 + 盐值 前端先请求path接口，拿到动态生成加密后的秒杀接口，再请求接口{path}/do_miaosha 12345678910111213141516171819202122232425262728293031323334353637383940414243@RequestMapping(value = &quot;{path}/do_miaosha&quot;)@ResponseBodypublic ResultSk&lt;Integer&gt; miaosha(Model model, SkUser skUser, @RequestParam(&quot;goodsId&quot;) long goodsId, @PathVariable(&quot;path&quot;) String path) { ... //验证path boolean check = skService.checkPath(skUser, goodsId, path); if (!check) { result.withError(REQUEST_ILLEGAL.getCode(), REQUEST_ILLEGAL.getMessage()); return result; } ...}@AccessLimit(seconds = 5, maxCount = 5, needLogin = true)@RequestMapping(value = &quot;path&quot;)@ResponseBodypublic ResultSk&lt;String&gt; skPath(Model model, SkUser skUser, @RequestParam(&quot;goodsId&quot;) long goodsId, @RequestParam(value = &quot;verifyCode&quot;, defaultValue = &quot;0&quot;) int verifyCode) { ResultSk&lt;String&gt; result = ResultSk.build(); model.addAttribute(&quot;user&quot;, skUser); if (skUser == null) { result.withError(ResultStatus.SESSION_ERROR.getCode(), ResultStatus.SESSION_ERROR.getMessage()); return result; } boolean check = skService.checkVerifyCode(skUser, goodsId, verifyCode); if (!check) { result.withError(REQUEST_ILLEGAL.getCode(), REQUEST_ILLEGAL.getMessage()); return result; } String path = skService.createSkPath(skUser, goodsId); result.setData(path); return result;}@Overridepublic String createSkPath(SkUser skUser, long goodsId) { // 生成url String str = MD5Util.md5(UUIDUtil.generateUuid() + &quot;123456&quot;); // 放入缓存 redisService.set(SkKey.getSkPath, &quot;&quot; + skUser.getId() + &quot;_&quot; + goodsId, str); return str;} 资源静态化在前后端分离的项目中，页面一般都是不会经过后端的，前端也有自己的服务器，所以提前把能放入CDN服务器的东西都放进去，减少真正秒杀时候服务器的压力。 验证码加入验证码或者滑块，防止秒杀的时候请求一起打过来。 库存预热开始秒杀前通过定时任务提前把商品的库存加载到Redis中，让整个流程都在Redis里面去做。 超卖原因：将商品的库存加载到Redis中，如果秒杀成功，再异步的修改库存。假如此时库存仅剩一个，四台服务器一起查询都发现还有一个，都去扣库存，此时库存变为-3，造成超卖。 解决方案：MySQL使用版本（CAS原理） 12select version from goods WHERE id= 1001update goods set num = num - 1, version = version + 1 WHERE id = 1001 AND num &gt; 0 AND version = @version(上面查到的version) 优化：使用Lua脚本，将判断库存和扣减库存都写在一个脚本中由Redis执行，如果return false则证明库存已减为0。 限流漏桶算法水（请求）先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。 令牌桶算法能够限制数据的平均传输速率，还允许某种程度的突发传输。 在令牌桶算法中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的门限，因此它适合于具有突发特性的流量。 分布式限流解决方案：Redis + Lua 12345678910111213141516171819202122232425public static boolean acquire() throws IOException, URISyntaxException{ Jedis jedis = new Jedis(&quot;127.0.0.1&quot;); String lua = &quot;local key = KEYS[1] &quot; + &quot; local limit = tonumber(ARGV[1]) &quot; + &quot; local current = tonumber(redis.call('get', key) or '0')&quot; + &quot; if current + 1 &gt; limit &quot; + &quot; then return 0 &quot; + &quot; else &quot; + &quot; redis.call('INCRBY', key,'1')&quot; + &quot; redis.call('expire', key,'10') &quot; + &quot; end return 1 &quot;; // 当前秒 String key = &quot;ip:&quot; + System.currentTimeMillis() / 1000; // 最大限制 String limit = &quot;5&quot;; List&lt;String&gt; keys = new ArrayList&lt;&gt;(); keys.add(key); List&lt;String&gt; args = new ArrayList&lt;&gt;(); args.add(limit); jedis.auth(&quot;1234&quot;); String luaScript = jedis.scriptLoad(lua); Long result = (Long) jedis.evalsha(luaScript, keys, args); return result == 1;} 分布式限流的关键是将限流做成具有原子性的功能，使用Redis+lua实现时间窗口内的流量控制。因为操作在lua脚本中并且redis是单线程的，所以线程安全。 单用户限制接口请求频率采用拦截器实现 缓存与数据库的双写一致性 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么不是更新缓存如果是写多读少的场景，频繁更新缓存浪费性能，有很多冷数据； 可能会存在脏数据（线程A更新了数据库；线程B更新了数据库；线程B更新了缓存；线程A更新了缓存）； 为什么不是先删缓存再更新数据库（1）请求A进行写操作，删除缓存； （2）请求B查询发现缓存不存在； （3）请求B去数据库查询得到旧值； （4）请求B将旧值写入缓存； （5）请求A将新值写入数据库； 会导致数据不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 先更新数据库，再删除缓存可能存在的问题假设有两个请求，请求A做查询操作，请求B做更新操作，可能发生： （1）缓存刚好失效； （2）请求A查询数据库，得一个旧值； （3）请求B将新值写入数据库； （4）请求B删除缓存； （5）请求A将查到的旧值写入缓存； 发生这种情况的概率又有多少呢？ 发生上述情况的前提：步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。 但是，数据库的读操作的速度远快于写操作的，因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。 数据库和缓存一致性延时双删（1）先淘汰缓存； （2）再写数据库（这两步和原来一样）； （3）休眠1秒，再次淘汰缓存（休眠时间根据项目自定义）； 写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 缓存key的封装采用什么设计模式模板模式 优点 具体细节步骤实现定义在子类中，子类定义详细处理算法是不会改变算法整体结构； 代码复用的基本技术，在数据库设计中尤为重要； 存在一种反向的控制结构，通过一个父类调用其子类的操作，通过子类对父类进行扩展增加新的行为，符合“开闭原则”； 缺点 每个不同的实现都需要定义一个子类，会导致类的个数增加，系统更加庞大；","link":"/2020/08/09/SeckillThinking/"},{"title":"Shiro","text":"ShiroAuthentication：身份认证/登录，验证用户是不是拥有相应的身份； Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限； Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通JavaSE环境的，也可以是如Web环境的； Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储； Web Support：Web支持，可以非常容易的集成到Web环境； Caching：缓存，比如用户登录后，其用户信息、拥有的角色/权限不必每次去查，这样可以提高效率； Concurrency：shiro支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing：提供测试支持； Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 Shiro四大核心功能：Authentication, Authorization, Cryptography, Session Management Shiro三个核心组件：Subject, SecurityManager 和 Realms. Subject：主体，代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是Subject，如网络爬虫，机器人等；即一个抽象概念；所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；可以把Subject认为是一个门面；SecurityManager才是实际的执行者； SecurityManager：安全管理器；即所有与安全有关的操作都会与SecurityManager交互；且它管理着所有Subject；可以看出它是Shiro的核心，它负责与后边介绍的其他组件进行交互，如果学习过SpringMVC，你可以把它看成DispatcherServlet前端控制器； Realm：域，Shiro从从Realm获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm获取相应的用户进行比较以确定用户身份是否合法；也需要从Realm得到用户相应的角色/权限进行验证用户是否能进行操作；可以把Realm看成DataSource，即安全数据源。 认证原理：1、通过ini配置文件创建securityManager 2、调用subject.login方法主体提交认证，提交的token 3、securityManager进行认证，securityManager最终由ModularRealmAuthenticator进行认证。 4、ModularRealmAuthenticator调用IniRealm(给realm传入token) 去ini配置文件中查询用户信息 5、IniRealm根据输入的token（UsernamePasswordToken，即这里的token是用户从页面输入的信息）从 shiro-first.ini查询用户信息（这里是测试阶段，后面都是查询的数据库，注入service，调用dao），根据账号查询用户信息（账号和密码） ​ 如果查询到用户信息，就给ModularRealmAuthenticator返回用户信息（账号和密码） ​ 如果查询不到，就给ModularRealmAuthenticator返回null 6、ModularRealmAuthenticator接收IniRealm返回Authentication认证信息 ​ 如果返回的认证信息是null，ModularRealmAuthenticator抛出异常（org.apache.shiro.authc.UnknownAccountException） ​ 如果返回的认证信息不是null（说明inirealm找到了用户），对IniRealm返回用户密码 （在ini文件中存在）和 token中的密码 进行对比，如果不一致抛出异常（org.apache.shiro.authc.IncorrectCredentialsException） Spring SecurityWeb/Http 安全：这是最复杂的部分。通过建立 filter 和相关的 service bean 来实现框架的认证机制。当访问受保护的 URL 时会将用户引入登录界面或者是错误提示界面。 业务对象或者方法的安全：控制方法访问权限的。 AuthenticationManager：处理来自于框架其他部分的认证请求。 AccessDecisionManager：为 Web 或方法的安全提供访问决策。会注册一个默认的，但是我们也可以通过普通 bean 注册的方式使用自定义的 AccessDecisionManager。 AuthenticationProvider：AuthenticationManager 是通过它来认证用户的。 UserDetailsService：跟 AuthenticationProvider 关系密切，用来获取用户信息的。 Shiro和Spring Security比较 Shiro比Spring更容易使用，实现和最重要的理解 Spring Security更加知名的唯一原因是因为品牌名称 “Spring”以简单而闻名，但讽刺的是很多人发现安装Spring Security很难 然而，Spring Security却有更好的社区支持 Apache Shiro在Spring Security处理密码学方面有一个额外的模块 Spring-security 对spring 结合较好，如果项目用的springmvc ，使用起来很方便。但是如果项目中没有用到spring，那就不要考虑它了。 Shiro 功能强大、且 简单、灵活。是Apache下的项目比较可靠，且不跟任何的框架或者容器绑定，可以独立运行","link":"/2020/08/25/Shiro/"},{"title":"SpringTransaction","text":"Spring事务Spring的两种事务管理方式编程式事务管理通过TransactionTemplate或者TransactionManager手动管理事务，实际应用中很少使用。 使用 TransactionTemplate 进行编程式事务管理： 12345678910111213141516@Autowiredprivate TransactionTemplate transactionTemplate;public void testTransaction() { transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { try { // .... 业务代码 } catch (Exception e){ //回滚 transactionStatus.setRollbackOnly(); } } });} 使用 TransactionManager 进行编程式事务管理： 123456789101112@Autowiredprivate PlatformTransactionManager transactionManager;public void testTransaction() { TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition()); try { // .... 业务代码 transactionManager.commit(status); } catch (Exception e) { transactionManager.rollback(status); }} 声明式事务管理推荐使用（代码侵入性最小），实际是通过 AOP 实现（基于@Transactional 的全注解方式使用最多）。 Spring事务管理接口 PlatformTransactionManager：（平台）事务管理器，Spring事务策略的核心； 1234567public interface PlatformTransactionManager extends TransactionManager { TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException; void commit(TransactionStatus var1) throws TransactionException; void rollback(TransactionStatus var1) throws TransactionException;} 通过这个接口，Spring 为各个平台如 JDBC(DataSourceTransactionManager)，Hibernate(HibernateTransactionManager)，JPA(JpaTransactionManager)等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 TransactionDefinition：事务定义信息（事务隔离级别，传播行为，超时，只读，回滚规则）； 1234567891011121314151617181920212223242526272829303132333435363738394041public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; int TIMEOUT_DEFAULT = -1; // 返回事务的传播特性，默认值为REQUIRED default int getPropagationBehavior() { return 0; } // 返回事务的隔离级别，默认值是DEFAULT default int getIsolationLevel() { return -1; } // 返回事务的超时时间，默认值为-1。 // 如果超过该时间限制但事务还没有完成，则自动回滚事务。 default int getTimeout() { return -1; } // 返回是否为只读事务，默认值为 false default boolean isReadOnly() { return false; } @Nullable default String getName() { return null; } static TransactionDefinition withDefaults() { return StaticTransactionDefinition.INSTANCE; }} TransactionStatus：事务运行状态； @Transactional 属性详解事务传播行为事务传播行为是为了解决业务层方法之间互相调用的事务问题。 示例：在A类的aMethod()方法中调用了B类的bMethod()方法。这时就涉及到业务层方法之间互相调用的事务问题。如果bMethod()发生异常需要回滚，如何配置事务传播行为才能让aMethod()也跟着回滚呢？ 123456789101112131415Class A { @Transactional(propagation=propagation.xxx) public void aMethod { //do something B b = new B(); b.bMethod(); }}Class B { @Transactional(propagation=propagation.xxx) public void bMethod { //do something }} TransactionDefinition.PROPAGATION_REQUIRED 使用的最多的一个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说： 如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务并且被Propagation.REQUIRED修饰，则所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务，只要一个方法回滚，整个事务均回滚。 TransactionDefinition.PROPAGATION_REQUIRES_NEW 创建一个新的事务，如果当前存在事务，则把当前事务挂起。即不论外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会开启自己的事务，且开启的事务相互独立，互不干扰。 示例：上面的bMethod()使用PROPAGATION_REQUIRES_NEW事务传播行为修饰，aMethod还是用PROPAGATION_REQUIRED修饰。如果aMethod()发生异常回滚，bMethod()不会跟着回滚，因为 bMethod()开启了独立的事务。但是，如果bMethod()抛出了未被捕获的异常并且这个异常满足事务回滚规则，aMethod()同样也会回滚，因为这个异常被aMethod()的事务管理机制检测到了。 123456789101112131415Class A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something B b = new B(); b.bMethod(); }}Class B { @Transactional(propagation=propagation.REQUIRES_NEW) public void bMethod { //do something }} TransactionDefinition.PROPAGATION_NESTED 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则等价于TransactionDefinition.PROPAGATION_REQUIRED。也就是说： 在外部方法未开启事务的情况下Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰； 如果外部方法开启事务，Propagation.NESTED修饰的内部方法属于外部事务的子事务，外部主事务回滚的话，子事务也会回滚，而内部子事务可以单独回滚而不影响外部主事务和其他子事务； 12345678910111213141516171819202122Class A { // 如果aMethod回滚，bMethod和bMethod2都要回滚; // 如果bMethod回滚，不会造成aMethod和bMethod2回滚; @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something B b = new B(); b.bMethod(); b.bMethod2(); }}Class B { @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod { //do something } @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod2 { //do something }} TransactionDefinition.PROPAGATION_MANDATORY 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_SUPPORTS 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER 以非事务方式运行，如果当前存在事务，则抛出异常。 事务隔离级别 TransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别，Oracle 默认采用的 READ_COMMITTED 隔离级别； TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读； TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生； TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生； TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别； MySQL InnoDB存储引擎默认支持的隔离级别是REPEATABLE-READ，在该隔离级别下InnoDB引擎使用的是Next-Key Lock锁算法，因为可以避免幻读的产生。 事务超时属性指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以int的值来表示超时时间，单位是秒，默认值为-1。 事务的只读属性对于只有读取数据查询的事务，可以指定事务类型为 readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。 为什么数据查询操作需要启用事务支持？ 如果不加Transactional，每条SQL会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。 如果一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持 SQL执行期间的读一致性； 如果一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持； 事务的回滚规则默认情况下，事务只有遇到运行期异常（RuntimeException 的子类）时才会回滚，Error也会导致事务回滚，但是在遇到检查型（Checked）异常时不会回滚。 12// 回滚自定义的异常类型@Transactional(rollbackFor= MyException.class) @Transactional的使用 @Transactional的作用范围 方法：推荐将注解使用于方法上，注意，该注解只能应用到public方法上，否则不生效； 类：如果这个注解使用在类上，表明该注解对该类中所有的public方法都生效； 接口：不推荐在接口上使用； 参数配置 属性名 说明 propagation 事务的传播行为，默认值为 REQUIRED isolation 事务的隔离级别，默认值采用 DEFAULT timeout 事务的超时时间，默认值为-1（不会超时）。如果超过该时间限制但事务还没有完成，则自动回滚事务。 readOnly 指定事务是否为只读事务，默认值为 false。 rollbackFor 用于指定能够触发事务回滚的异常类型，并且可以指定多个异常类型。 @Transactional事务注解原理 @Transactional的工作机制是基于AOP实现的，AOP是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用JDK的动态代理，如果目标对象没有实现了接口，会使用CGLIB动态代理。 12345678910111213141516171819public class DefaultAopProxyFactory implements AopProxyFactory, Serializable { @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } .......} 如果一个类或者类中的public方法上被标注注解@Transactional，Spring 容器就会在启动的时候为其创建一个代理类，在调用被@Transactional注解的public方法时，实际调用的是TransactionInterceptor类中的invoke()方法。这个方法的作用就是在目标方法之前开启事务，方法执行过程中如果遇到异常的时候回滚事务，方法调用完成之后提交事务。 Spring AOP自调用的问题 若同一类中的其他没有@Transactional注解的方法内部调用有 @Transactional注解的方法，有@Transactional注解的方法的事务会失效。 这是由于Spring AOP代理造成的，因为只有当@Transactional注解的方法在类以外被调用的时候，Spring事务管理才生效。 示例：MyService类中的method1()调用method2()就会导致method2()的事务失效。 123456789101112@Servicepublic class MyService {private void method1() { method2(); //......}@Transactional public void method2() { //...... }} 解决办法：避免同一类中自调用，或者使用AspectJ取代Spring AOP代理。","link":"/2020/06/11/SpringTransaction/"},{"title":"ThreadLocal","text":"简介利用synchronzed或者lock解决线程安全的问题时，会让未获取到锁的线程进行阻塞等待。线程安全问题的核心在于多个线程会对同一个临界区共享资源进行操作，那么，如果每个线程都使用自己的“共享资源”，各自用各自的，又互相不影响，让多个线程间达到隔离的状态，这样就不会出现线程安全的问题。 于是ThreadLocal应运而生，使每个线程都拥有某个变量副本，达到人手一份的效果，从而避免共享资源的竞争。 ThreadLocal的两个作用 让某个需要用到的对象在线程间隔离（每个线程都有自己的独立的对象）； 在任何方法中都可以轻松获取到该对象； 根据共享对象的生成时机不同，选择 initialValue 或 set 来保存对象 场景一：initialValue ​ 在ThreadLocal第一次get的时候把对象给初始化出来，对象的初始化时机可以由我们控制； 场景二：set ​ 如果需要保存到ThreadLocal里的对象的生成时机不由我们随意控制，例如拦截器生成的用户信息，则使用 ThreadLocal.set，以便后续使用； 源码分析 Thread ThreadLocal ThreadLocalMap 之间的关系 Thread类有一个类型为ThreadLocal.ThreadLocalMap的实例变量threadLocals，即每个线程有一个自己的ThreadLocalMap。 ThreadLocalMap有自己的独立实现，可以简单地将它的key视作ThreadLocal，value为代码中放入的值（实际上key并不是ThreadLocal本身，而是它的一个弱引用）。 每个线程在往ThreadLocal里放值的时候，都会往自己的ThreadLocalMap里存，读也是以ThreadLocal作为引用，在自己的map里找对应的key，从而实现了线程隔离。 ThreadLocalMap有点类似HashMap的结构，只是HashMap是由数组+链表实现的，而ThreadLocalMap中并没有链表结构。 我们还要注意Entry， 它的key是ThreadLocal&lt;?&gt; k，继承自WeakReference， 即弱引用类型。 set()：设置在当前线程中 threadLocal 变量的值 123456789101112public void set(T value) { //1. 获取当前线程实例对象 Thread t = Thread.currentThread(); //2. 通过当前线程实例获取到ThreadLocalMap对象 ThreadLocalMap map = getMap(t); if (map != null) //3. 如果Map不为null,则以当前threadLocl实例为key,值为value进行存入 map.set(this, value); else //4.map为null,则新建ThreadLocalMap并存入value createMap(t, value);} 总结：通过当前线程对象thread获取该thread所维护的threadLocalMap,若threadLocalMap不为null,则以threadLocal实例为key,值为value的键值对存入threadLocalMap,若threadLocalMap为null的话，就新建threadLocalMap然后在以threadLocal为键，值为value的键值对存入即可。 get()：获取当前线程中threadLocal变量的值 1234567891011121314151617181920212223242526272829public T get() { //1. 获取当前线程的实例对象 Thread t = Thread.currentThread(); //2. 获取当前线程的threadLocalMap ThreadLocalMap map = getMap(t); if (map != null) { //3. 获取map中当前threadLocal实例为key的值的entry ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) //4. 当前entitiy不为null的话，就返回相应的值value T result = (T)e.value; return result; } } //5. 若map为null或者entry为null的话通过该方法初始化，并返回该方法返回的value return setInitialValue();}private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;} 总结：通过当前线程thread实例获取到它所维护的threadLocalMap，然后以当前threadLocal实例为key获取该map中的键值对（Entry），若Entry不为null则返回Entry的value。如果获取threadLocalMap为null或者Entry为null的话，就以当前threadLocal为Key，value为null存入map后，并返回null。 initialValue() 123protected T initialValue() { return null;} 这个方法是protected修饰的也就是说继承ThreadLocal的子类可重写该方法，实现赋值为其他的初始值。 remove() 12345public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);} ThreadLocalMap的hash冲突 虽然ThreadLocalMap中使用了黄金分隔数来作为hash计算因子，大大减少了hash冲突的概率，但是仍然会存在冲突。 此时就会线性向后查找，一直找到Entry为null的槽位才会停止查找，将当前元素放入此槽位中。(线性探测) 适用场景 每个线程需要一个独享的对象（通常是工具类，典型需要使用的类有 SimpleDateFormat 和 Random）； 重写了 initialValue() 方法； 每个线程内需要保存全局变量（例如在拦截器中获取用户信息），可以让不同方法直接使用，避免参数传递的麻烦； 用 ThreadLocal 保存一些业务内容（用户权限信息，从用户系统获取到的用户名，userID 等） 这些信息在用一个线程内相同，但是不同的线程使用的业务内容是不同的； 在线程生命周期内，都通过这个静态 ThreadLocal 实例的 get() 方法取得自己 set() 过的那个对象，避免了这个对象（例如user对象）作为参数传递的麻烦； 强调的是同一个请求内（同一个线程内）不同方法间的共享； 不需要重写 initialValue() 方法，但是必须手动调用 set() 方法 ThreadLocal带来的好处 线程安全； 不需要加锁，提高执行效率； 更高效利用内存，节省开销； ThreadLocal使得代码耦合度更低，更优雅：免去传参的繁琐，无论是场景一还是场景二，都可以在任何地方直接通过ThreadLocal拿到，再也不需要每次都传同样的参数。 可能会带来的问题内存泄漏的原因ThreadLocal在ThreadLocalMap中是以一个弱引用身份被Entry中的Key引用的，因此如果ThreadLocal没有外部强引用来引用它，那么ThreadLocal会在下次JVM垃圾收集时被回收。这个时候就会出现Entry中Key已经被回收，出现一个null Key的情况，外部读取ThreadLocalMap中的元素是无法通过null Key来找到Value的。因此如果当前线程的生命周期很长，一直存在，那么其内部的ThreadLocalMap对象也一直生存下来，这些null key就存在一条强引用链的关系一直存在：Thread –&gt; ThreadLocalMap–&gt;Entry–&gt;Value，这条强引用链会导致Entry不会回收，Value也不会回收，但Entry中的Key却已经被回收的情况，造成内存泄漏。 但是JVM团队已经考虑到这样的情况，并做了一些措施来保证ThreadLocal尽量不会内存泄漏：在ThreadLocal的get()、set()、remove()方法调用的时候会清除掉线程ThreadLocalMap中所有Entry中Key为null的Value，并将整个Entry设置为null，利于下次内存回收。 1234567891011121314151617181920212223242526272829303132333435private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } 为什么不使用强引用？ 如果key使用强引用，当引用的ThreadLocal对象被回收，但ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。 InheritableThreadLocal使用ThreadLocal的时候，在异步场景下，无法给子线程共享父线程中创建的线程副本数据。此时可以使用InheritableThreadLocal。 https://blog.csdn.net/ThinkWon/article/details/102508381","link":"/2020/08/01/ThreadLocal/"},{"title":"VariousLock","text":"公平锁和非公平锁公平锁1234567891011121314151617181920212223242526272829303132333435/** * Sync object for fair locks */static final class FairSync extends Sync { private static final long serialVersionUID = -3000897897090466540L; final void lock() { acquire(1); } /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 是否有线程在等待 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; }} 非公平锁1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Sync object for non-fair locks */static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); }}/** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} 区别： 非公平锁在调用lock后，首先就会调用CAS进行一次抢锁，如果这个时候恰巧锁没有被占用，则直接获取到锁返回； 非公平锁CAS抢锁失败后，会和公平锁一样进入到tryAcquire()，在tryAcquire()中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接CAS抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，排到后面； 如果非公平锁两次CAS都不成功，后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁性能更好，吞吐量更大，因为非公平锁减少了线程挂起的几率，后来的线程有一定几率逃离被挂起的开销，但会使阻塞队列中的线程长期处于饥饿状态。 乐观锁和悲观锁乐观锁拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间数据有没有被修改过，可以使用版本号等机制。乐观锁适用于多读的应用类型，提高吞吐量。数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 悲观锁每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java里面的Synchronized关键字的实现也是悲观锁。 存在的问题： 在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题； 一个线程持有锁会导致其它所有需要此锁的线程挂起； 如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险；","link":"/2020/07/15/VariousLock/"},{"title":"jvm","text":"Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的高墙，墙外面的人想进去，墙里面的人却想出来。 JVM运行时数据区 触发MinorGC(Young GC) 虚拟机在进行minorGC之前会判断老年代最大的可用连续空间是否大于新生代的所有对象总空间 1、如果大于的话，直接执行minorGC 2、如果小于，判断是否开启HandlerPromotionFailure，没有开启直接FullGC 3、如果开启了HanlerPromotionFailure, JVM会判断老年代的最大连续内存空间是否大于历次晋升的大小，如果小于直接执行FullGC 4、如果大于的话，执行minorGC 触发FullGC老年代空间不足 如果创建一个大对象，Eden区域当中放不下这个大对象，会直接保存在老年代当中，如果老年代空间也不足，就会触发Full GC。为了避免这种情况，最好就是不要创建太大的对象。 持久代空间不足 如果有持久代空间的话，系统当中需要加载的类，调用的方法很多，同时持久代当中没有足够的空间，就出触发一次Full GC YGC出现promotion failure promotion failure发生在Young GC, 如果Survivor区当中存活对象的年龄达到了设定值，会就将Survivor区当中的对象拷贝到老年代，如果老年代的空间不足，就会发生promotion failure， 接下去就会发生Full GC. 统计YGC发生时晋升到老年代的平均总大小大于老年代的空闲空间 在发生YGC是会判断，是否安全，这里的安全指的是，当前老年代空间可以容纳YGC晋升的对象的平均大小，如果不安全，就不会执行YGC,转而执行Full GC。 显示调用System.gc 类的加载过程加载 通过一个类的全限定名获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各个数据的访问入口 加载 .class 文件的方式：从本地系统中直接加载；通过网络获取，Web Applet；从zip压缩包中读取，jar，war；运行时计算生成，动态代理技术；从其他文件生成，jsp应用等。 链接验证确保Class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全。 主要包括四种验证：文件格式验证，元数据验证，字节码验证，符号引用验证。 准备为类变量分配内存并且设置该类变量的默认初始值，即零值。 特例：1）final修饰的static：因为final在编译时就会分配了，准备阶段会显式初始化；2）不会为实例变量初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 解析将常量池内的符号引用转换为直接引用的过程。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT_Class、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等。 堆堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。 在方法结束后，堆中的对象不会马上被移除，仅仅在垃圾收集的时候才会被移除。 Minor GC、Major GC、Full GC的区别： Minor GC：只是新生代的垃圾收集 Young GC Major GC：只是老年代的垃圾收集 Old GC，目前只有CMS会有单独收集老年代的行为。 Mixed GC：混合收集，收集整个新生代以及部分老年代的垃圾收集，目前只有G1会有这种行为。 Full GC：整堆收集，收集整个Java堆和方法区的垃圾收集。 内存区域堆是进程中最大的一块内存，主要用于存放新创建的对象； 方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据； 虚拟机栈：每个Java方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在JVM栈中入栈到出栈的过程","link":"/2020/05/26/jvm/"},{"title":"logs","text":"Log4j.properties配置详解 Loggers组件这五个级别是有顺序的，DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，分别用来指定这条日志信息的重要程度。 Log4j有一个规则：只输出级别不低于设定级别的日志信息，假设Loggers级别设定为INFO，则INFO、WARN、ERROR和FATAL级别的日志信息都会输出，而级别比INFO低的DEBUG则不会输出。 配置日志信息输出目的地（appender）1log4j.appender.appenderName = className appenderName：自定义appderName，在log4j.rootLogger设置中使用；className：可设值如下： (1) org.apache.log4j.ConsoleAppender（控制台）选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Target=System.err：默认值是System.out。 (2) org.apache.log4j.FileAppender（文件）选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 (3) org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件）选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定当前消息输出到logging.log4j文件中。 DatePattern=’.’yyyy-MM：每月滚动一次日志文件，即每月产生一个新的日志文件。当前月的日志文件名为logging.log4j，前一个月的日志文件名为logging.log4j.yyyy-MM。 另外，也可以指定按周、天、时、分等来滚动日志文件，对应的格式如下： ‘.’yyyy-MM：每月 ‘.’yyyy-ww：每周 ‘.’yyyy-MM-dd：每天 ‘.’yyyy-MM-dd-a：每天两次 ‘.’yyyy-MM-dd-HH：每小时 ‘.’yyyy-MM-dd-HH-mm：每分钟 (4) org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件）选项 Threshold=WARN：指定日志信息的最低输出级别，默认为DEBUG。 ImmediateFlush=true：表示所有消息都会被立即输出，设为false则不输出，默认值是true。 Append=false：true表示消息增加到指定文件中，false则将消息覆盖指定的文件内容，默认值是true。 File=D:/logs/logging.log4j：指定消息输出到logging.log4j文件中。 MaxFileSize=100KB：后缀可以是KB, MB 或者GB。在日志文件到达该大小时，将会自动滚动，即将原来的内容移到logging.log4j.1文件中。 MaxBackupIndex=2：指定可以产生的滚动文件的最大数，例如，设为2则可以产生logging.log4j.1，logging.log4j.2两个滚动文件和一个logging.log4j文件。 （5）org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） 配置日志信息的输出格式（Layout）1log4j.appender.appenderName.layout=className className：可设值如下： (1) org.apache.log4j.HTMLLayout（以HTML表格形式布局） LocationInfo=true：输出java文件名称和行号，默认值是false。 Title=My Logging： 默认值是Log4J Log Messages。 (2) org.apache.log4j.PatternLayout（可以灵活地指定布局模式） ConversionPattern=%m%n：设定以怎样的格式显示消息。 格式化符号说明： %p：输出日志信息的优先级，即DEBUG，INFO，WARN，ERROR，FATAL。 %d：输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，如：%d{yyyy/MM/dd HH:mm:ss,SSS}。 %r：输出自应用程序启动到输出该log信息耗费的毫秒数。 %t：输出产生该日志事件的线程名。 %l：输出日志事件的发生位置，相当于%c.%M(%F:%L)的组合，包括类全名、方法、文件名以及在代码中的行数。例如：test.TestLog4j.main(TestLog4j.java:10)。 %c：输出日志信息所属的类目，通常就是所在类的全名。 %M：输出产生日志信息的方法名。 %F：输出日志消息产生时所在的文件名称。 %L:：输出代码中的行号。 %m:：输出代码中指定的具体日志信息。 %n：输出一个回车换行符，Windows平台为”\\r\\n”，Unix平台为”\\n”。 %x：输出和当前线程相关联的NDC(嵌套诊断环境)，尤其用到像java servlets这样的多客户多线程的应用中。 %%：输出一个”%”字符。 另外，还可以在%与格式字符之间加上修饰符来控制其最小长度、最大长度、和文本的对齐方式。如： c：指定输出category的名称，最小的长度是20，如果category的名称长度小于20的话，默认的情况下右对齐。 %-20c：”-“号表示左对齐。 %.30c：指定输出category的名称，最大的长度是30，如果category的名称长度大于30的话，就会将左边多出的字符截掉，但小于30的话也不会补空格。 （3）org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串） （4）org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息）","link":"/2020/05/12/logs/"},{"title":"springboot","text":"自动配置原理12345678910111213141516@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class}), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class})})public @interface SpringBootApplication { springboot启动时加载主配置类，开启了自动配置功能 SpringBoot启动的时候加载主配置类，开启了自动配置功能@EnableAutoConfiguration @EnableAutoConfiguration Import的AutoConfigurationImportSelector中代码最终调用SpringFactoriesLoader.loadSpringFactories扫描了Jar包的META-INF/spring.factories文件加载了大量的XXAutoConfiguration类 AutoConfiguration类配合Conditonal注解与ConfigurationProperties配置类在特定条件下自动装配我们需要的Bean到IOC容器中。","link":"/2020/10/12/springboot/"},{"title":"synchronized","text":"synchronizedJava多线程的锁都是基于对象的，Java中的每一个对象都可以作为一个锁。 还有一点需要注意的是，我们常听到的类锁其实也是对象锁。 Java类只有一个Class对象（可以有多个实例对象，多个实例共享这个Class对象），而Class对象也是特殊的Java对象。所以我们常说的类锁，其实就是Class对象的锁。 用法 1234567891011121314151617// 关键字在实例方法上，锁为当前实例public synchronized void instanceLock() { // code}// 关键字在静态方法上，锁为当前Class对象public static synchronized void classLock() { // code}// 关键字在代码块上，锁为括号里面的对象public void blockLock() { Object o = new Object(); synchronized (o) { // code }} 所谓“临界区”，指的是某一块代码区域，它同一时刻只能由一个线程执行。在上面的例子中，如果synchronized关键字在方法上，那临界区就是整个方法内部。而如果是使用synchronized代码块，那临界区就指的是代码块内部的区域。 12345678910111213141516171819202122232425// 等价情况1// 关键字在实例方法上，锁为当前实例public synchronized void instanceLock() { // code}// 关键字在代码块上，锁为括号里面的对象public void blockLock() { synchronized (this) { // code }}// 等价情况2// 关键字在静态方法上，锁为当前Class对象public static synchronized void classLock() { // code}// 关键字在代码块上，锁为括号里面的对象public void blockLock() { synchronized (this.getClass()) { // code }} 锁升级在Java 6 及其以后，一个对象其实有四种锁状态，它们级别由低到高依次是：无锁状态，偏向锁状态，轻量级锁状态，重量级锁状态。 一个对象的“锁”的信息存放在什么地方？ Java对象头 Mark Word的格式： 锁状态 29bit/61bit 1bit是否为偏向锁 2bit锁标志位 无锁 0 01 偏向锁 线程ID 1 01 轻量级锁 指向栈中锁记录的指针 此时这一位不用于标识偏向锁 00 重量级锁 指向互斥量（重量级锁）的指针 此时这一位不用于标识偏向锁 10 GC标记 此时这一位不用于标识偏向锁 11 当对象状态为偏向锁时，Mark Word存储的是偏向的线程ID；当状态为轻量级锁时，Mark Word存储的是指向线程栈中Lock Record的指针；当状态为重量级锁时，Mark Word为指向堆中的monitor对象的指针。 偏向锁Hotspot的作者经研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得，于是引入了偏向锁。 偏向锁会偏向于第一个访问锁的线程，如果在接下来的运行过程中，该锁没有被其他的线程访问，则持有偏向锁的线程将永远不需要触发同步。也就是说，偏向锁在资源无竞争情况下消除了同步语句，连CAS操作都不做了，提高了程序的运行性能。 实现原理 一个线程在第一次进入同步块时，会在对象头和栈帧中的锁记录里存储锁的偏向的线程ID。当下次该线程进入这个同步块时，会去检查锁的Mark Word里面是不是放的自己的线程ID。 如果是，表明该线程已经获得了锁，以后该线程在进入和退出同步块时不需要花费CAS操作来加锁和解锁 ；如果不是，就代表有另一个线程来竞争这个偏向锁。这个时候会尝试使用CAS来替换Mark Word里面的线程ID为新线程的ID，这个时候要分两种情况： 成功，表示之前的线程不存在了， Mark Word里面的线程ID为新线程的ID，锁不会升级，仍然为偏向锁； 失败，表示之前的线程仍然存在，那么暂停之前的线程，设置偏向锁标识为0，并设置锁标志位为00，升级为轻量级锁，会按照轻量级锁的方式进行竞争锁。 撤销偏向锁 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时， 持有偏向锁的线程才会释放锁。 偏向锁升级成轻量级锁时，会暂停拥有偏向锁的线程，重置偏向锁标识，大概过程如下： 在一个安全点（在这个时间点上没有字节码正在执行）停止拥有锁的线程； 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Mark Word，使其变成无锁状态； 唤醒被停止的线程，将当前锁升级成轻量级锁； 所以，如果应用程序里所有的锁通常处于竞争状态，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭： 1-XX:UseBiasedLocking=false 轻量级锁多个线程在不同时段获取同一把锁，即不存在锁竞争的情况，也就没有线程阻塞。针对这种情况，JVM采用轻量级锁来避免线程的阻塞与唤醒。 轻量级锁的加锁 JVM会为每个线程在当前线程的栈帧中创建用于存储锁记录的空间，Displaced Mark Word。如果一个线程获得锁时发现是轻量级锁，会把锁的Mark Word复制到自己的Displaced Mark Word里面。 然后线程尝试用CAS将锁的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示Mark Word已经被替换成了其他线程的锁记录，说明在与其它线程竞争锁，当前线程就尝试使用自旋来获取锁。 JDK采用适应性自旋，简单来说就是线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。 自旋也不是一直进行下去的，如果自旋到一定程度（和JVM、操作系统相关），依然没有获取到锁，称为自旋失败，那么这个线程会阻塞。同时这个锁就会升级成重量级锁。 轻量级锁的释放 在释放锁时，当前线程会使用CAS操作将Displaced Mark Word的内容复制回锁的Mark Word里面。如果没有发生竞争，那么这个复制的操作会成功。如果有其他线程因为自旋多次导致轻量级锁升级成了重量级锁，那么CAS操作会失败，此时会释放锁并唤醒被阻塞的线程。 重量级锁重量级锁依赖于操作系统的互斥量（mutex） 实现的，而操作系统中线程间状态的转换需要相对比较长的时间，挂起，唤醒这两个操作进行了两次上下文切换，所以重量级锁效率很低，但被阻塞的线程不会消耗CPU。 锁的升级流程每一个线程在准备获取共享资源时： 第一步，检查MarkWord里面是不是放的自己的ThreadId ,如果是，表示当前线程是处于 “偏向锁”； 第二步，如果MarkWord不是自己的ThreadId，锁升级，这时候，用CAS来执行切换，新的线程根据MarkWord里面现有的ThreadId，通知之前线程暂停，之前线程将Markword的内容置为空； 第三步，两个线程都把锁对象的HashCode复制到自己新建的用于存储锁的记录空间，接着开始通过CAS操作， 把锁对象的MarKword的内容修改为自己新建的记录空间的地址的方式竞争MarkWord； 第四步，第三步中成功执行CAS的获得资源，失败的则进入自旋； 第五步，自旋的线程在自旋过程中，成功获得资源(即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于 轻量级锁的状态，如果自旋失败； 第六步，进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己； 各种锁的优缺点对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块场景。 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 如果始终得不到锁竞争的线程使用自旋会消耗CPU。 追求响应时间。同步块执行速度非常快。 重量级锁 线程竞争不使用自旋，不会消耗CPU。 线程阻塞，响应时间缓慢。 追求吞吐量。同步块执行时间较长。 底层原理同步方法方法级的同步是隐式的。 同步方法的常量池中会有一个ACC_SYNCHRONIZED标志。当某个线程要访问某个方法的时候，会检查是否有ACC_SYNCHRONIZED，如果有设置，则需要先获得监视器锁，然后开始执行方法，方法执行之后再释放监视器锁。 这时如果其他线程来请求执行方法，会因为无法获得监视器锁而被阻断住。 如果在方法执行过程中发生异常，并且方法内部并没有处理该异常，那么在异常被抛到方法外面之前监视器锁会被自动释放。 同步代码块使用monitorenter和monitorexit两个指令实现。可以把执行monitorenter指令理解为加锁，执行monitorexit理解为释放锁。 每个对象维护着一个记录被锁次数的计数器。未被锁定的对象的该计数器为0，当一个线程获得锁（执行monitorenter）后，该计数器自增变为1，当同一个线程再次获得该对象的锁时，计数器再次自增。当同一个线程释放锁（执行monitorexit指令）的时候，计数器再自减。当计数器为0的时候。锁将被释放，其他线程便可以获得锁。 思考 什么是上下文操作 通常指CPU上下文，是CPU运行任何任务前，必须依赖的环境，包括CPU 寄存器和程序计数器。 上下文切换：就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 synchronized和reentrantLock区别 两者都是可重入锁 synchronized依赖于JVM ReentrantLock是JDK层面实现的； ReentrantLock比synchronized增加了一些高级功能：等待可中断；可实现公平锁；可实现选择性通知（锁可以绑定多个条件）","link":"/2020/07/14/synchronized/"},{"title":"volatile","text":"volatileJMMJMM的抽象：主内存和本地内存 JMM有以下规定： ​ 所有变量都存储在主内存中，同时每个线程也有自己独立的工作内存，工作内存中的变量内容是主内存中的拷贝； ​ 线程不能直接读写主内存中的变量，而是只操作自己工作内存中的变量，然后再同步到主内存中； ​ 主内存是多个线程共享的，但线程间不共享工作内存，如果线程间需要通信，必须借助主内存中转来完成； 为什么会有可见性问题 CPU有多级缓存，导致读的数据过期； 高速缓存的容量比主内存小，但是速度仅次于寄存器，所以在CPU和主内存之间就多了Cache层； 线程间的对于共享变量的可见性问题不是直接由多核引起的，而是由多缓存引起的； 如果所有核心都只用一个缓存，那么就不存在内存可见性问题了； 每个核心都会将自己需要的数据读到独占缓存中，数据修改后也是写入到缓存中，然后等待刷入主存中。所以会导致有些核心取到的值是一个过期的值； 重排序与happens-before 为什么存在重排序？为什么可以提高性能？ 12a = b + cd = e - f 先加载b、c（注意，即有可能先加载b，也有可能先加载c），但是在执行add(b,c)的时候，需要等待b、c装载结束才能继续执行，也就是增加了停顿，那么后面的指令也会依次有停顿,这降低了计算机的执行效率。 为了减少这个停顿，我们可以先加载e和f,然后再去加载add(b,c),这样做对程序（串行）是没有影响的,但却减少了停顿。既然add(b,c)需要停顿，那还不如去做一些有意义的事情。 指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。 指令重排的类型 编译器优化重排：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序； 指令并行重排：现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序； 内存系统重排：由于处理器使用缓存和读写缓冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差； happens-before happens-before原则 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前； 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序； 天然的happens-before关系 程序顺序规则：一个线程中的每一个操作，happens-before于该线程中的任意后续操作； 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁； volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读； 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C； start规则：如果线程A执行操作ThreadB.start()启动线程B，那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作； join规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回； 作用 保证变量的内存可见性； 禁止volatile变量与普通变量重排序（JSR133提出，Java 5 开始才有这个“增强的volatile内存语义”）； 原理可见性实现： 修改volatile变量时会强制将修改后的值刷新的主内存中； 修改volatile变量后会导致其他线程工作内存中对应的变量值失效。因此，再读取该变量值的时候就需要重新从读取主内存中的值； 禁止重排序实现JVM限制处理器的重排序 —— 内存屏障 硬件层面，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。 内存屏障有两个作用： 阻止屏障两侧的指令重排序；强制把写缓冲区/高速缓存中的脏数据等写回主内存，或者让缓存中相应的数据失效；（注意这里的缓存主要指的是CPU缓存，如L1，L2等） 编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。编译器选择了一个比较保守的JMM内存屏障插入策略，这样可以保证在任何处理器平台，任何程序中都能得到正确的volatile内存语义。这个策略是： 在每个volatile写操作前插入一个StoreStore屏障； 在每个volatile写操作后插入一个StoreLoad屏障； 在每个volatile读操作后插入一个LoadLoad屏障； 在每个volatile读操作后再插入一个LoadStore屏障； 再介绍一下volatile与普通变量的重排序规则: 如果第一个操作是volatile读，那无论第二个操作是什么，都不能重排序； 如果第二个操作是volatile写，那无论第一个操作是什么，都不能重排序； 如果第一个操作是volatile写，第二个操作是volatile读，那不能重排序； 应用1）单例模式 —— 双重锁检查 12345678910111213141516171819public class Singleton { private volatile static Singleton instance; private Singleton() { } public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; }} 如果这里的变量声明不使用volatile关键字，则可能会发生错误的。它可能会被重排序： instance = new Singleton(); 123456789// 可以分解为以下三个步骤1 memory=allocate();// 分配内存 相当于c的malloc2 ctorInstanc(memory) //初始化对象3 s=memory //设置s指向刚分配的地址// 上述三个步骤可能会被重排序为 1-3-2，也就是：1 memory=allocate();// 分配内存 相当于c的malloc3 s=memory //设置s指向刚分配的地址2 ctorInstanc(memory) //初始化对象 而一旦假设发生了这样的重排序，比如线程A执行了步骤1和步骤3，但是步骤2还没有执行完。这个时候另一个线程B执行到了if (instance == null) ，它会判定instance不为空，然后直接返回了一个未初始化完成的instance！ 常见问题 除了在volatile中使用了内存屏障，Java还有哪里使用了内存屏障？ Oracle的JDK中提供了Unsafe. putOrderedObject，Unsafe. putOrderedInt，Unsafe. putOrderedLong这三个方法，JDK会在执行这三个方法时插入StoreStore内存屏障，避免发生写操作重排序； 初次读包含final域的对象引用和读取这个final域，这两个操作不能重排序，使用了LoadLoad屏障； 原理 将当前内核高速缓存行的数据立刻回写到内存； 使在其他内核里缓存了该内存地址的数据无效； MESI协议：该缓存一致性思路：当CPU写数据时，如果发现操作的变量时共享变量，即其他线程的工作内存也存在该变量，会通过CPU 总线嗅探机制告知其他线程该变量副本已经失效，需要重新从主内存中读取。当其他线程需要使用这个变量时，如内存地址失效，那么它们会在主存中重新读取该值。","link":"/2020/07/13/volatile/"},{"title":"AQS","text":"简介是AbstractQueuedSynchronizer的简称，是一个用来构建锁和同步器的框架，比如ReentrantLock，Semaphore，ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。 AQS源码分析内部数据结构它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待双端队列（多线程争用资源被阻塞时会进入此队列），并使用了两个指针head和tail用于标识队列的头部和尾部。 123456789101112131415161718192021222324252627282930313233343536373839/** * The synchronization state. */private volatile int state;/** * Returns the current value of synchronization state. * This operation has memory semantics of a {@code volatile} read. * @return current state value */protected final int getState() { return state;}/** * Sets the value of synchronization state. * This operation has memory semantics of a {@code volatile} write. * @param newState the new state value */protected final void setState(int newState) { state = newState;}/** * Atomically sets synchronization state to the given updated * value if the current state value equals the expected value. * This operation has memory semantics of a {@code volatile} read * and write. * * @param expect the expected value * @param update the new value * @return {@code true} if successful. False return indicates that the actual * value was not equal to the expected value. */protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} CLH队列并不是直接存储线程，而是存储拥有线程的Node节点 Node的结构： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061static final class Node { // 标记一个结点（对应的线程）在共享模式下等待 static final Node SHARED = new Node(); // 标记一个结点（对应的线程）在独占模式下等待 static final Node EXCLUSIVE = null; // waitStatus的值，表示该结点（对应的线程）已被取消 static final int CANCELLED = 1; // waitStatus的值，表示后继结点（对应的线程）需要被唤醒 static final int SIGNAL = -1; // waitStatus的值，表示该结点（对应的线程）在等待某一条件 static final int CONDITION = -2; /*waitStatus的值，表示有资源可用，新head结点需要继续唤醒后继结点（共享模式下，前继结点不仅会唤醒其后继结点，同时也可能会唤醒后继的后继结点。）*/ static final int PROPAGATE = -3; // 0:新结点入队时的默认状态 // 等待状态，取值范围，-3，-2，-1，0，1 volatile int waitStatus; volatile Node prev; // 前驱结点 volatile Node next; // 后继结点 volatile Thread thread; // 结点对应的线程 Node nextWaiter; // 等待队列里下一个等待条件的结点 // 判断共享模式的方法 final boolean isShared() { return nextWaiter == SHARED; } /** * Returns previous node, or throws NullPointerException if null. * Use when predecessor cannot be null. The null check could * be elided, but is present to help the VM. * * @return the predecessor of this node */ final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) throw new NullPointerException(); else return p; } Node() { // Used to establish initial head or SHARED marker } Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread; } Node(Thread thread, int waitStatus) { // Used by Condition this.waitStatus = waitStatus; this.thread = thread; }} 资源共享模式资源有两种共享模式，或者说两种同步方式： 独占模式（Exclusive）：资源是独占的，一次只能一个线程获取。如ReentrantLock。 共享模式（Share）：同时可以被多个线程获取，具体的资源个数可以通过参数指定。如Semaphore/CountDownLatch。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)：独占模式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占模式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享模式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享模式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 这些方法虽然都是protected方法，但是它们并没有在AQS具体实现，而是直接抛出异常，因为AQS只是一个框架，具体资源的获取/释放方式交由自定义同步器去实现。 123protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();} 这里没有定义成abstract的原因： 独占模式下只需要实现tryAcquire-tryRelease，而共享模式下只需要实现tryAcquireShared-tryReleaseShared。如果都定义成abstract，那么每个模式也要去实现另一模式下的接口。 （一般情况下，子类只需要根据需求实现其中一种模式，当然也有同时实现两种模式的同步类，如ReadWriteLock。） 核心部分的源码分析 获取资源（独占） 123456789101112131415161718/** * Acquires in exclusive mode, ignoring interrupts. Implemented * by invoking at least once {@link #tryAcquire}, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking {@link * #tryAcquire} until success. This method can be used * to implement method {@link Lock#lock}. * * @param arg the acquire argument. This value is conveyed to * {@link #tryAcquire} but is otherwise uninterpreted and * can represent anything you like. */// arg是要获取的资源的个数，在独占模式下始终为1。public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 函数流程如下： 1）tryAcquire()尝试直接去获取资源，如果成功则直接返回（这里体现了非公平锁，每个线程获取锁时会尝试直接抢占加塞一次，而CLH队列中可能还有别的线程在等待）； 2）addWaiter()将该线程加入等待队列的尾部，并标记为独占模式； 3）acquireQueued()使线程阻塞在等待队列中获取资源，一直获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false； 4）如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166/** * Attempts to acquire in exclusive mode. This method should query * if the state of the object permits it to be acquired in the * exclusive mode, and if so to acquire it. * * &lt;p&gt;This method is always invoked by the thread performing * acquire. If this method reports failure, the acquire method * may queue the thread, if it is not already queued, until it is * signalled by a release from some other thread. This can be used * to implement method {@link Lock#tryLock()}. */// 尝试去获取独占资源。如果获取成功，则直接返回true，否则直接返回false。// 需自定义同步器去实现 tryLock()protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();}/** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */// 将当前线程加入到等待队列的队尾，并返回当前线程所在的结点。private Node addWaiter(Node mode) { // 生成该线程对应的Node节点 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 尝试快速方式直接放到队尾 Node pred = tail; if (pred != null) { node.prev = pred; // 使用CAS尝试，成功则返回 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 如果等待队列为空或者上述CAS失败，通过自旋CAS插入（enq方法） enq(node); return node;}/** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node's predecessor */// 将node加入队尾private Node enq(final Node node) { // CAS自旋，直到成功加入队尾 for (;;) { Node t = tail; if (t == null) { // Must initialize // 队列为空，创建一个空的标志结点作为head结点，并将tail也指向它。 if (compareAndSetHead(new Node())) tail = head; } else { //正常流程，放入队尾 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }}/** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return {@code true} if interrupted while waiting *//** * 1. 结点进入队尾后，检查状态，找到安全休息点； * 2. 调用park()进入waiting状态，等待unpark()或interrupt()唤醒自己； * 3. 被唤醒后，看自己是否可以获取到资源。如果拿到，head指向当前结点，并返回从入队到获取到的整个过程中是否被中断过；如果没获取到，继续流程1。 */final boolean acquireQueued(final Node node, int arg) { // 标记是否成功拿到资源 boolean failed = true; try { // 标记等待过程中是否被中断过 boolean interrupted = false; // 自旋 for (;;) { // 获取前驱节点 final Node p = node.predecessor(); // 若前驱结点p是head，说明node是第二个结点，可以尝试去获取资源 if (p == head &amp;&amp; tryAcquire(arg)) { // 拿到资源后，将head指向该结点。 // 所以head所指的结点，就是当前获取到资源的那个结点或null。 setHead(node); // setHead中node.prev已置为null，此处再将head.next置为null，是为了方便GC回收以前的head结点。(意味着将之前拿完资源的结点出队) p.next = null; // help GC // 成功获取资源 failed = false; //返回等待过程中是否被中断过 return interrupted; } // 如果自己可以休息了，就进入waiting状态，直到被unpark() if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { // 如果等待过程中没有成功获取资源（如timeout，或者可中断的情况下被中断了），那么取消结点在队列中的等待。 if (failed) cancelAcquire(node); }}/** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node's predecessor holding status * @param node the node * @return {@code true} if thread should block */private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { // 获取前驱节点状态 int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ // 如果前驱放弃了，就一直往前找，直到找到一个正常等待状态的node，排在它的后面。 do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ // 如果前驱正常，就把前驱的状态设置成SIGNAL。 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false;}/** * Convenience method to park and then check if interrupted * * @return {@code true} if interrupted */private final boolean parkAndCheckInterrupt() { // 调用park()使线程进入waiting状态 LockSupport.park(this); // 如果被唤醒，查看自己是不是被中断的。 return Thread.interrupted();} 注意： 队列的尾部插入新的Node节点：由于AQS中会存在多个线程同时争夺资源的情况，因此肯定会出现多个线程同时插入节点的操作，所以在这里是通过CAS自旋的方式保证了操作的线程安全性。（addWaiter()和enq()） park()会让当前线程进入waiting状态。在此状态下，有两种情况可以唤醒该线程：1）被unpark()；2）被interrupt()。 LockSupport类是Java 6 引入的，提供了基本的线程同步原语。LockSupport实际上是调用了Unsafe类里的函数，归结到Unsafe里，只有两个函数： park(boolean isAbsolute, long time)：阻塞当前线程 unpark(Thread jthread)：使给定的线程停止阻塞 所以结点进入等待队列后，是调用park使它进入阻塞状态的。只有头结点的线程是处于活跃状态的。 释放资源（独占） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * Releases in exclusive mode. Implemented by unblocking one or * more threads if {@link #tryRelease} returns true. * This method can be used to implement method {@link Lock#unlock}. * * @param arg the release argument. This value is conveyed to * {@link #tryRelease} but is otherwise uninterpreted and * can represent anything you like. * @return the value returned from {@link #tryRelease} */// 释放指定量的资源，如果彻底释放了（即state=0）,它会唤醒等待队列里的其他线程来获取资源。public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 唤醒等待队列里的下一个线程 unparkSuccessor(h); return true; } return false;}/** * Attempts to set the state to reflect a release in exclusive * mode. * * &lt;p&gt;This method is always invoked by the thread performing release. * */// 需要独占模式的自定义同步器去实现protected boolean tryRelease(int arg) { throw new UnsupportedOperationException();}/** * Wakes up node's successor, if one exists. * * @param node the node */private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // 置零当前线程所在的节点状态，允许失败 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 找到下一个需要唤醒的节点 Node s = node.next; // 如果这个后继结点为空或者状态大于0，即这个结点已被取消 if (s == null || s.waitStatus &gt; 0) { s = null; // 将等待队列中所有还有用的结点向前移动（从后向前找） for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) // 唤醒 LockSupport.unpark(s.thread);} 获取资源（共享） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * Acquires in shared mode, ignoring interrupts. Implemented by * first invoking at least once {@link #tryAcquireShared}, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking {@link * #tryAcquireShared} until success. * * @param arg the acquire argument. This value is conveyed to * {@link #tryAcquireShared} but is otherwise uninterpreted * and can represent anything you like. */// 获取指定量的资源，获取成功则直接返回；获取失败则进入等待队列，直到获取到资源为止，整个过程忽略中断。public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);}/** * Attempts to acquire in shared mode. This method should query if * the state of the object permits it to be acquired in the shared * mode, and if so to acquire it. * * &lt;p&gt;This method is always invoked by the thread performing * acquire. If this method reports failure, the acquire method * may queue the thread, if it is not already queued, until it is * signalled by a release from some other thread. * * &lt;p&gt;The default implementation throws {@link * UnsupportedOperationException}. */// 返回值：负值代表获取失败；0代表获取成功，但没有剩余资源；正数表示获取成功，还有剩余资源，其他线程还可以去获取。protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException();}/** * Acquires in shared uninterruptible mode. * @param arg the acquire argument */// 将当前线程加入等待队列尾部休息，直到其他线程释放资源唤醒自己，成功拿到相应量的资源后才返回。private void doAcquireShared(int arg) { // 加入队列尾部 final Node node = addWaiter(Node.SHARED); // 标记是否成功 boolean failed = true; try { // 标记等待过程中是否被中断过 boolean interrupted = false; for (;;) { // 获取前驱节点 final Node p = node.predecessor(); // 如果到head的下一个，因为head是拿到资源的线程，此时node被唤醒，很可能是head用完资源来唤醒自己 if (p == head) { // 尝试获取资源 int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC // 如果等待过程中被打断过，此时将中断补上。 if (interrupted) selfInterrupt(); failed = false; return; } } // 判断状态，寻找安全点，进入waiting状态，等着被unpark()或interrupt() if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }}/** * Sets head of queue, and checks if successor may be waiting * in shared mode, if so propagating if either propagate &gt; 0 or * PROPAGATE status was set. * * @param node the node * @param propagate the return value from a tryAcquireShared */private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don't know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ // 如果还有剩余量，继续唤醒下一个邻居线程 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); }} 流程： 1）tryAcquireShared()尝试获取资源，成功则直接返回； 2）失败则通过doAcquireShared()进入等待队列park()，直到被unpark()/interrupt()并成功获取到资源才返回。整个等待过程也是忽略中断的。 释放资源（共享） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Releases in shared mode. Implemented by unblocking one or more * threads if {@link #tryReleaseShared} returns true. * * @param arg the release argument. This value is conveyed to * {@link #tryReleaseShared} but is otherwise uninterpreted * and can represent anything you like. * @return the value returned from {@link #tryReleaseShared} */// 释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。public final boolean releaseShared(int arg) { // 尝试释放资源 if (tryReleaseShared(arg)) { // 唤醒后继节点 doReleaseShared(); return true; } return false;}/** * Release action for shared mode -- signals successor and ensures * propagation. (Note: For exclusive mode, release just amounts * to calling unparkSuccessor of head if it needs signal.) */private void doReleaseShared() { /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 唤醒后继节点 unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; }} 其他 获取资源的方法除了acquire外，还有以下三个： acquireInterruptibly：申请可中断的资源（独占模式） acquireShared：申请共享模式的资源 acquireSharedInterruptibly：申请可中断的资源（共享模式）","link":"/2020/07/16/AQS/"},{"title":"DistributedTransaction","text":"分布式事务事务简介 事务是用来保证一组数据操作的完整性和一致性 事务必须满足ACID的四大特性（待补全） 事务具有四种隔离级别（待补全） 事务具有七种传播行为（待补全） 什么是分布式事务分布式事务就是将多个节点的事务看成一个整体处理。 分布式事务由事务参与者、资源服务器、事务管理器等组成，常见例子有，支付、下订单等。 实现思路两段式事务 请求阶段：协调者向参与者询问是否可以进行事务提交操作，然后开始等待参与者的响应。 提交阶段：在该阶段，协调者将基于第一个阶段的投票结果进行决策：提交或取消。当且仅当所有的参与者同意提交，事务协调者才通知所有的参与者提交事务，否则协调者将通知所有的参与者回滚事务。 缺点：1）当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态；2）当协调者出错，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作；3）假如在第二阶段中，假如协调者发出commit消息后宕机，接收到这条消息的参与者宕机，此时则无法判断事务状态，无法确定是否已被提交； 三段式事务事务询问 -&gt; 执行事务预提交 -&gt; 进行事务提交或者事务回滚 降低了参与者的阻塞范围，但引入了新问题：在参与者接收到precommit后，网络出现问题，参与者和协调者无法通行，在这种情况下，参与者依然会执行事务的提交。 基于XA的分布式事务 缺点：1）性能较差；2）很多nosql不支持XA协议； 基于消息的最终一致性方案 缺点：属于强一致性事务，会存在资源浪费 TCC编程式补偿性事务 TCC事务是柔性事务，在try阶段要对资源做预留，在confirm或cancel阶段释放资源，与基于消息事务对比，TCC的时效性更好。 TCC模型是把锁的粒度完全交给业务处理，它分为三个阶段： Try阶段主要是对业务系统做检测及资源预留； 如果try阶段所有业务资源都预留成功，则执行confirm，否则执行cancel； confirm：不做任务业务检查，仅使用预留的资源执行业务操作，失败会重试； cancel：取消执行业务操作，释放预留的资源，失败会重试； 举例以简单的电商系统为例，小明在淘宝上花100元买了一本书，获赠10个积分，产生如下操作： 订单系统创建商品订单； 支付系统接受小明的支付； 库存系统扣减产品库存； 会员系统给小明账户增加会员积分； 这几个动作需要作为一个事务执行，要同时成功或者同时撤销。如果采用TCC事务模式，那么各个系统需要改造为如下状态： 1）订单系统 try：创建一个订单，状态显示为“待支付”； confirm：更新订单的状态为“已完成”； cancel：更新订单的状态为“已取消”；2）支付系统 try：假设小明账户中有1000元，冻结小明账户中的100元，此时小明看到的余额依然是1000元； confirm：将账户余额变为900元，并清除冻结记录； concel：清除冻结记录；3）库存系统 try：假设库存中还生10本书，冻结其中的一本书，现实库存依然有10本书； confirm：将剩余库存更新为9本书，并清除冻结记录； cancel：清除冻结记录；4）会员系统 try：假设小明原因积分为3000，给小明账户预增加10积分，账户显示的积分依然是3000分； confirm：将账户积分更新为3010，并清除预增加记录； cancel：清除预增加记录； 缺点：TCC 事务模型对业务方侵入较大，需要业务方把功能的实现上由一个接口拆分为三个，开发成本较高。 同时 TCC 事务为了解决异步网络中的通信失败或超时带来的异常情况，要求业务方在设计实现上要遵循三个策略： 允许空回滚：原因是异常发生在阶段 1 时，部分参与方没有收到 try 请求从而触发整个事务的 cancel 操作，try 失败或者没有执行 try 操作的参与方收到 cancel 请求时，要进行空回滚操作； 保持幂等性：原因是异常发生在阶段 2 时，比如网络超时，则会重复调用参与方的 confirm/cancel 方法，因此需要这两个方法实现上保证幂等性； 防止资源悬挂：原因网络异常导致两个阶段无法保证严格的顺序执行，出现参与方侧 try 请求比 cancel 请求更晚到达的情况，cancel 会执行空回滚而确保事务的正确性，但是此时 try 方法也不可以再被执行； 分布式事务框架 全局事务框架GTS 蚂蚁金服分布式事务DTX 开源TCC框架TCC-Transaction（https://github.com/changmingxie/tcc-transaction） 开源TCC框架Byte-（https://github.com/liuyangming/ByteTCC） TCC-Transaction分析 仓库：https://github.com/changmingxie/tcc-transaction 使用方法 在需要提供分布式事务支持的接口方法上添加 @Compensable； 在对应的接口实现方法上也添加 @Compensable，并添加注解参数 confirmMethod, cancelMethod 和 transactionContextEditor； 实现对应的 confirmMethod 和 cancelMethod（必须和 try 方法在同一个类中）； 注意： 在分布式事务框架中，不要轻易在业务层捕获所有异常，只有在抛出异常的情况下，分布式事务框架才知道该业务是执行失败的，继而执行cancelMethod； 使用 TCC-Transaction 时，confirm 和 cancel 的幂等性问题需要人为代码保证； TCC 的数据库应该和业务数据库分开，以保证分布式事务的正常进行； 源码分析 tcc的事务并不是数据库的事务，而是应用层的事务，Transaction如下： 12345678910111213141516171819202122public class Transaction implements Serializable { private static final long serialVersionUID = 7291423944314337931L; // 全局事务id，用来保证事务唯一性 private TransactionXid xid; // 事务的状态 private TransactionStatus status; // 事务类型，ROOT是主事务，BRANCH是分支事务 private TransactionType transactionType; // 事务重试次数 private volatile int retriedCount = 0; // 事务创建时间 private Date createTime = new Date(); // 事务最后一次更新的时间 private Date lastUpdateTime = new Date(); // 事务的版本号 private long version = 1; // 事务的参与者 private List&lt;Participant&gt; participants = new ArrayList&lt;Participant&gt;(); // 附加参数 private Map&lt;String, Object&gt; attachments = new ConcurrentHashMap&lt;String, Object&gt;(); ...} CompensableTransactionAspect是一个AOP切面类，@Pointcut 将 @Compensable 注解标记为切入点，其签名为compensableService()。@Around 表示在compensableService()之前和之后调用 interceptCompensableMethod()。 123456789101112131415@Aspectpublic abstract class CompensableTransactionAspect { private CompensableTransactionInterceptor compensableTransactionInterceptor; public void setCompensableTransactionInterceptor(CompensableTransactionInterceptor compensableTransactionInterceptor) { this.compensableTransactionInterceptor = compensableTransactionInterceptor; } @Pointcut(&quot;@annotation(org.mengyun.tcctransaction.api.Compensable)&quot;) public void compensableService() { } @Around(&quot;compensableService()&quot;) public Object interceptCompensableMethod(ProceedingJoinPoint pjp) throws Throwable { return compensableTransactionInterceptor.interceptCompensableMethod(pjp); } public abstract int getOrder();} CompensableTransactionInterceptor是事务拦截器，具有以下作用： 将事务区分为ROOT事务和PROVIDER分支事务； 不断地修改数据库内的状态（初始化事务、修改事务状态）； 修改和清除事务管理区中的事务队列； 并没有执行目标对象方法，pjp.proceed() 其实是交给了下一个拦截器 ResourceCoordinatorInterceptor； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class CompensableMethodContext { ProceedingJoinPoint pjp = null; Method method = null; // 解析得到的Compensable注解 Compensable compensable = null; Propagation propagation = null; // 保存了全局事务id和事务状态，在调用事务参与者Participant的confirm或cancel方法时会传递过去。 TransactionContext transactionContext = null; public CompensableMethodContext(ProceedingJoinPoint pjp) { this.pjp = pjp; this.method = getCompensableMethod(); this.compensable = method.getAnnotation(Compensable.class); this.propagation = compensable.propagation(); this.transactionContext = FactoryBuilder.factoryOf(compensable.transactionContextEditor()).getInstance().get(pjp.getTarget(), method, pjp.getArgs()); } ...}public class CompensableTransactionInterceptor { public Object interceptCompensableMethod(ProceedingJoinPoint pjp) throws Throwable { // 通过pjp解析各种属性，组成CompansableMethodContext对象 CompensableMethodContext compensableMethodContext = new CompensableMethodContext(pjp); // 是否有存在的事务队列（从ThreadLocal获取transactions来判断当前线程是否已经有事务） boolean isTransactionActive = transactionManager.isTransactionActive(); if (!TransactionUtils.isLegalTransactionContext(isTransactionActive, compensableMethodContext)) { throw new SystemException(&quot;no active compensable transaction while propagation is mandatory for method &quot; + compensableMethodContext.getMethod().getName()); } // 获取并判断当前事务的角色（ROOT表示主事务，PROVIDER表示分支事务或事务参与者），并根据其角色调用不同的方法来处理。 switch (compensableMethodContext.getMethodRole(isTransactionActive)) { // 处理主事务切面 case ROOT: return rootMethodProceed(compensableMethodContext); // 处理PROVIDER事务切面 case PROVIDER: return providerMethodProceed(compensableMethodContext); default: return pjp.proceed(); } } /** * 1. 开启全局事务 * 2. 持久化全局事务 * 3. 注册全局事务 * 4. 判断应该是confirm还是cancel * 5. 清除事务 * * @param compensableMethodContext * @return * @throws Throwable */ private Object rootMethodProceed(CompensableMethodContext compensableMethodContext) throws Throwable { Object returnValue = null; Transaction transaction = null; boolean asyncConfirm = compensableMethodContext.getAnnotation().asyncConfirm(); boolean asyncCancel = compensableMethodContext.getAnnotation().asyncCancel(); Set&lt;Class&lt;? extends Exception&gt;&gt; allDelayCancelExceptions = new HashSet&lt;Class&lt;? extends Exception&gt;&gt;(); allDelayCancelExceptions.addAll(this.delayCancelExceptions); allDelayCancelExceptions.addAll(Arrays.asList(compensableMethodContext.getAnnotation().delayCancelExceptions())); try { /** * 开启一个全新的事务 * 1. 持久化事务形态 -&gt; 全局事务编号 * 2. 注册一个事务【ThreadLocal】 */ transaction = transactionManager.begin(compensableMethodContext.getUniqueIdentity()); try { // 执行后续方法 returnValue = compensableMethodContext.proceed(); } catch (Throwable tryingException) { if (!isDelayCancelException(tryingException, allDelayCancelExceptions)) { logger.warn(String.format(&quot;compensable transaction trying failed. transaction content:%s&quot;, JSON.toJSONString(transaction)), tryingException); // 回滚事务 transactionManager.rollback(asyncCancel); } throw tryingException; } // 提交事务 transactionManager.commit(asyncConfirm); } finally { // 清除队列中的事务 transactionManager.cleanAfterCompletion(transaction); } return returnValue; } private Object providerMethodProceed(CompensableMethodContext compensableMethodContext) throws Throwable { Transaction transaction = null; boolean asyncConfirm = compensableMethodContext.getAnnotation().asyncConfirm(); boolean asyncCancel = compensableMethodContext.getAnnotation().asyncCancel(); try { switch (TransactionStatus.valueOf(compensableMethodContext.getTransactionContext().getStatus())) { case TRYING: // 初始化一份事务参与者的数据进入到当前服务中，并向事务管理器注册事务 transaction = transactionManager.propagationNewBegin(compensableMethodContext.getTransactionContext()); return compensableMethodContext.proceed(); case CONFIRMING: // 修改状态 try { transaction = transactionManager.propagationExistBegin(compensableMethodContext.getTransactionContext()); // 修改事务状态为CONFIRMING，并持久化更新，提交事务 transactionManager.commit(asyncConfirm); } catch (NoExistedTransactionException excepton) { //the transaction has been commit,ignore it. } break; case CANCELLING: try { transaction = transactionManager.propagationExistBegin(compensableMethodContext.getTransactionContext()); // 修改事务状态为 CANCELLING，并持久化更新，回滚事务 transactionManager.rollback(asyncCancel); } catch (NoExistedTransactionException exception) { //the transaction has been rollback,ignore it. } break; } } finally { // 清除事务 transactionManager.cleanAfterCompletion(transaction); } Method method = compensableMethodContext.getMethod(); return ReflectionUtils.getNullValue(method.getReturnType()); } private boolean isDelayCancelException(Throwable throwable, Set&lt;Class&lt;? extends Exception&gt;&gt; delayCancelExceptions) { if (delayCancelExceptions != null) { for (Class delayCancelException : delayCancelExceptions) { Throwable rootCause = ExceptionUtils.getRootCause(throwable); if (delayCancelException.isAssignableFrom(throwable.getClass()) || (rootCause != null &amp;&amp; delayCancelException.isAssignableFrom(rootCause.getClass()))) { return true; } } } return false; }} TransactionManager 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class TransactionManager { private TransactionRepository transactionRepository; private static final ThreadLocal&lt;Deque&lt;Transaction&gt;&gt; CURRENT = new ThreadLocal&lt;Deque&lt;Transaction&gt;&gt;(); private ExecutorService executorService; ... // 开启事务，持久化到repository，注册到ThreadLocal public Transaction begin(Object uniqueIdentify) { Transaction transaction = new Transaction(uniqueIdentify,TransactionType.ROOT); transactionRepository.create(transaction); registerTransaction(transaction); return transaction; } // 从主事务的上下文创建分支事务，xid不变，事务类型变化 public Transaction propagationNewBegin(TransactionContext transactionContext) { Transaction transaction = new Transaction(transactionContext); transactionRepository.create(transaction); registerTransaction(transaction); return transaction; } // 从事务上下文同步事务状态到ThreadLocal public Transaction propagationExistBegin(TransactionContext transactionContext) throws NoExistedTransactionException { Transaction transaction = transactionRepository.findByXid(transactionContext.getXid()); if (transaction != null) { transaction.changeStatus(TransactionStatus.valueOf(transactionContext.getStatus())); registerTransaction(transaction); return transaction; } else { throw new NoExistedTransactionException(); } } public void commit(boolean asyncCommit) { // 从ThreadLocal获取当前事务 final Transaction transaction = getCurrentTransaction(); transaction.changeStatus(TransactionStatus.CONFIRMING); // 数据库更新transaction transactionRepository.update(transaction); if (asyncCommit) { try { Long statTime = System.currentTimeMillis(); // 通过线程池异步执行事务提交 executorService.submit(new Runnable() { @Override public void run() { commitTransaction(transaction); } }); logger.debug(&quot;async submit cost time:&quot; + (System.currentTimeMillis() - statTime)); } catch (Throwable commitException) { logger.warn(&quot;compensable transaction async submit confirm failed, recovery job will try to confirm later.&quot;, commitException); throw new ConfirmingException(commitException); } } else { // 同步执行事务提交 commitTransaction(transaction); } } private void commitTransaction(Transaction transaction) { try { // 调用事务参与者的commit方法 transaction.commit(); // 事务结束，在数据库删除当前事务，如果commit异常，不会删除数据库内事务记录，为了重试补偿 transactionRepository.delete(transaction); } catch (Throwable commitException) { logger.warn(&quot;compensable transaction confirm failed, recovery job will try to confirm later.&quot;, commitException); throw new ConfirmingException(commitException); } } public void rollback(boolean asyncRollback) { final Transaction transaction = getCurrentTransaction(); transaction.changeStatus(TransactionStatus.CANCELLING); transactionRepository.update(transaction); if (asyncRollback) { try { executorService.submit(new Runnable() { @Override public void run() { rollbackTransaction(transaction); } }); } catch (Throwable rollbackException) { logger.warn(&quot;compensable transaction async rollback failed, recovery job will try to rollback later.&quot;, rollbackException); throw new CancellingException(rollbackException); } } else { rollbackTransaction(transaction); } } // 把transaction注册到ThreadLocal对象中 private void registerTransaction(Transaction transaction) { if (CURRENT.get() == null) { CURRENT.set(new LinkedList&lt;Transaction&gt;()); } CURRENT.get().push(transaction); } // 事务结束，从栈中弹出结束的事务 public void cleanAfterCompletion(Transaction transaction) { if (isTransactionActive() &amp;&amp; transaction != null) { Transaction currentTransaction = getCurrentTransaction(); if (currentTransaction == transaction) { CURRENT.get().pop(); if (CURRENT.get().size() == 0) { CURRENT.remove(); } } else { throw new SystemException(&quot;Illegal transaction when clean after completion&quot;); } } } ...} ResourceCoordinatorAspect：主要是为了设置事务的参与者 12345678910111213141516@Aspectpublic abstract class ResourceCoordinatorAspect { private ResourceCoordinatorInterceptor resourceCoordinatorInterceptor; @Pointcut(&quot;@annotation(org.mengyun.tcctransaction.api.Compensable)&quot;) public void transactionContextCall() { } @Around(&quot;transactionContextCall()&quot;) public Object interceptTransactionContextMethod(ProceedingJoinPoint pjp) throws Throwable { return resourceCoordinatorInterceptor.interceptTransactionContextMethod(pjp); } public void setResourceCoordinatorInterceptor(ResourceCoordinatorInterceptor resourceCoordinatorInterceptor) { this.resourceCoordinatorInterceptor = resourceCoordinatorInterceptor; } public abstract int getOrder();} ResourceCoordinatorInterceptor：主要处理 try 阶段的事情，在 try 阶段，就将所有的“资源”封装完成并交给事务管理器。然后事务管理器修改数据库状态。 “资源”指“事务资源”，即事务的参与者：confirm上下文，cancel上下文，分支事务信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class ResourceCoordinatorInterceptor { private TransactionManager transactionManager; public void setTransactionManager(TransactionManager transactionManager) { this.transactionManager = transactionManager; } public Object interceptTransactionContextMethod(ProceedingJoinPoint pjp) throws Throwable { // 获取当前事务 Transaction transaction = transactionManager.getCurrentTransaction(); if (transaction != null) { switch (transaction.getStatus()) { // 只需要在trying时，获取到参与者信息，并设置到transaction中 case TRYING: enlistParticipant(pjp); break; case CONFIRMING: break; case CANCELLING: break; } } // 执行目标方法 return pjp.proceed(pjp.getArgs()); } private void enlistParticipant(ProceedingJoinPoint pjp) throws IllegalAccessException, InstantiationException { // 获取@Compensable注解信息 Method method = CompensableMethodUtils.getCompensableMethod(pjp); if (method == null) { throw new RuntimeException(String.format(&quot;join point not found method, point is : %s&quot;, pjp.getSignature().getName())); } Compensable compensable = method.getAnnotation(Compensable.class); String confirmMethodName = compensable.confirmMethod(); String cancelMethodName = compensable.cancelMethod(); Transaction transaction = transactionManager.getCurrentTransaction(); TransactionXid xid = new TransactionXid(transaction.getXid().getGlobalTransactionId()); if (FactoryBuilder.factoryOf(compensable.transactionContextEditor()).getInstance().get(pjp.getTarget(), method, pjp.getArgs()) == null) { FactoryBuilder.factoryOf(compensable.transactionContextEditor()).getInstance().set(new TransactionContext(xid, TransactionStatus.TRYING.getId()), pjp.getTarget(), ((MethodSignature) pjp.getSignature()).getMethod(), pjp.getArgs()); } Class targetClass = ReflectionUtils.getDeclaringType(pjp.getTarget().getClass(), method.getName(), method.getParameterTypes()); InvocationContext confirmInvocation = new InvocationContext(targetClass, confirmMethodName, method.getParameterTypes(), pjp.getArgs()); InvocationContext cancelInvocation = new InvocationContext(targetClass, cancelMethodName, method.getParameterTypes(), pjp.getArgs()); Participant participant = new Participant( xid, confirmInvocation, cancelInvocation, compensable.transactionContextEditor()); transactionManager.enlistParticipant(participant); }} 此时经过两个拦截器后，才调用到目标对象方法，即对应try逻辑的被切方法。","link":"/2020/06/30/DistributedTransaction/"},{"title":"HashMap","text":"常见相关实现类总结1）HashMap： 遍历顺序不确定； 最多只允许一条记录的键为null，允许多条记录的值为null； 非线程安全，若需保证线程安全，可以用Collections的synchronizedMap()，或者ConcurrentHashMap； 2）Hashtable： 线程安全，因为方法都用synchronized修饰，并发性不如ConcurrentHashMap； 3）LinkedHashMap 是HashMap的一个子类，保存了记录的插入顺序； 4）TreeMap 实现SortedMap接口，默认是按照键值的升序排序，也可以指定排序的比较器； 在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出异常java.lang.ClassCastException； 源码分析基本属性1234567891011121314151617// 默认容量16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认负载因子0.75static final float DEFAULT_LOAD_FACTOR = 0.75f; // 链表节点转换红黑树节点的阈值, 9个节点转static final int TREEIFY_THRESHOLD = 8; // 红黑树节点转换链表节点的阈值, 6个节点转static final int UNTREEIFY_THRESHOLD = 6; // 转红黑树时, table的最小长度static final int MIN_TREEIFY_CAPACITY = 64; 定位哈希桶数组索引位置12345678910// 代码1static final int hash(Object key) { // 计算key的hash值 int h; // 1.先拿到key的hashCode值; 2.将hashCode的高16位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);}// 代码2int n = tab.length;// 将(tab.length - 1) 与 hash值进行&amp;运算int index = (n - 1) &amp; hash; 步骤： 拿到 key 的 hashCode 值； 将 hashCode 的高位参与运算，重新计算 hash 值； 将计算出来的 hash 值与 (table.length - 1) 进行&amp;运算； 总结：当 table.length = 16 时，table.length - 1 = 15，此时低 4 位全为 1，高 28 位全为 0，与 0 进行 &amp; 运算必然为 0，因此此时(n - 1) &amp; hash 的运算结果只取决于hash的低四位，此时hash冲突概率增加。因此，在 JDK 1.8 中，将高位也参与计算 (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)，目的是降低 hash 冲突的概率。 put()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 1.校验table是否为空或者length等于0，如果是则调用resize方法进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 2.通过hash值计算索引位置，将该索引位置的头节点赋值给p，如果p为空则直接在该索引位置新增一个节点即可 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { // table表该索引位置不为空，则进行查找 Node&lt;K,V&gt; e; K k; // 3.判断p节点的key和hash值是否跟传入的相等，如果相等, 则p节点即为要查找的目标节点，将p节点赋值给e节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 4.判断p节点是否为TreeNode, 如果是则调用红黑树的putTreeVal方法查找目标节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { // 5.走到这代表p节点为普通链表节点，则调用普通的链表方法进行查找，使用binCount统计链表的节点数 for (int binCount = 0; ; ++binCount) { // 6.如果p的next节点为空时，则代表找不到目标节点，则新增一个节点并插入链表尾部 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 7.校验节点数是否超过8个，如果超过则调用treeifyBin方法将链表节点转为红黑树节点， // 减一是因为循环是从p节点的下一个节点开始的 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; } // 8.如果e节点存在hash值和key值都与传入的相同，则e节点即为目标节点，跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; // 将p指向下一个节点 } } // 9.如果e节点不为空，则代表目标节点存在，使用传入的value覆盖该节点的value，并返回oldValue if (e != null) { V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 用于LinkedHashMap return oldValue; } } ++modCount; // 10.如果插入节点后节点数超过阈值，则调用resize方法进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); // 用于LinkedHashMap return null;} 总结： 插入操作的入口方法是 put(K,V)，但核心逻辑在V putVal(int, K, V, boolean, boolean) 方法中。putVal()主要流程如下： ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 resize()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 1.老表的容量不为0，即老表不为空 if (oldCap &gt; 0) { // 1.1 判断老表的容量是否超过最大容量值：如果超过则将阈值设置为Integer.MAX_VALUE，并直接返回老表, // 此时oldCap * 2比Integer.MAX_VALUE大，因此无法进行重新分布，只是单纯的将阈值扩容到最大 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 1.2 将newCap赋值为oldCap的2倍，如果newCap&lt;最大容量并且oldCap&gt;=16, 则将新阈值设置为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } // 2.如果老表的容量为0, 老表的阈值大于0, 是因为初始容量被放入阈值，则将新表的容量设置为老表的阈值 else if (oldThr &gt; 0) newCap = oldThr; else { // 3.老表的容量为0, 老表的阈值为0，这种情况是没有传初始容量的new方法创建的空表，将阈值和容量设置为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 4.如果新表的阈值为空, 则通过新的容量*负载因子获得阈值 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } // 5.将当前阈值设置为刚计算出来的新的阈值，定义新表，容量为刚计算出来的新容量，将table设置为新定义的表。 threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 6.如果老表不为空，则需遍历所有节点，将节点赋值给新表 if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { // 将索引值为j的老表头节点赋值给e oldTab[j] = null; // 将老表的节点设置为空, 以便垃圾收集器回收空间 // 7.如果e.next为空, 则代表老表的该位置只有1个节点，计算新表的索引位置, 直接将该节点放在该位置 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 8.如果是红黑树节点，则进行红黑树的重hash分布(跟链表的hash分布基本相同) else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // preserve order // 9.如果是普通的链表节点，则进行普通的重hash分布 Node&lt;K,V&gt; loHead = null, loTail = null; // 存储索引位置为:“原索引位置”的节点 Node&lt;K,V&gt; hiHead = null, hiTail = null; // 存储索引位置为:“原索引位置+oldCap”的节点 Node&lt;K,V&gt; next; do { next = e.next; // 9.1 如果e的hash值与老表的容量进行与运算为0,则扩容后的索引位置跟老表的索引位置一样 if ((e.hash &amp; oldCap) == 0) { // 如果loTail为空, 代表该节点为第一个节点 if (loTail == null) // 则将loHead赋值为第一个节点 loHead = e; else // 否则将节点添加在loTail后面 loTail.next = e; // 并将loTail赋值为新增的节点 loTail = e; } // 9.2 如果e的hash值与老表的容量进行与运算为1,则扩容后的索引位置为:老表的索引位置＋oldCap else { // 如果hiTail为空, 代表该节点为第一个节点 if (hiTail == null) // 则将hiHead赋值为第一个节点 hiHead = e; else // 否则将节点添加在hiTail后面 hiTail.next = e; // 并将hiTail赋值为新增的节点 hiTail = e; } } while ((e = next) != null); // 10.如果loTail不为空（说明老表的数据有分布到新表上“原索引位置”的节点），则将最后一个节点的next设为空，并将新表上索引位置为“原索引位置”的节点设置为对应的头节点 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 11.如果hiTail不为空（说明老表的数据有分布到新表上“原索引+oldCap位置”的节点），则将最后一个节点的next设为空，并将新表上索引位置为“原索引+oldCap”的节点设置为对应的头节点 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } // 12.返回新表 return newTab;} 分为两步： 扩容：创建一个新的空数组，长度是原数组的两倍； rehash：遍历原数组，将所有Node重新Hash到新数组； 因为扩容的代码比较长, 我用文字来叙述下HashMap扩容的过程:.1 如果table == null, 则为HashMap的初始化, 生成空table返回即可;.2 如果table不为空, 需要重新计算table的长度, newLength = oldLength &lt;&lt; 1(注, 如果原oldLength已经到了上限, 则newLength = oldLength);.3 遍历oldTable:.3.2 首节点为空, 本次循环结束;.3.1 无后续节点, 重新计算hash位, 本次循环结束;.3.2 当前是红黑树, 走红黑树的重定位;.3.3 当前是链表, JAVA7时还需要重新计算hash位, 但是JAVA8做了优化, 通过(e.hash &amp; oldCap) == 0来判断是否需要移位; 如果为真则在原位不动, 否则则需要移动到当前hash槽位 + oldCap的位置; treeifyBin()123456789101112131415161718192021222324252627282930/** * 将链表节点转为红黑树节点 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; // 1.如果table为空或者table的长度小于64, 调用resize方法进行扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); // 2.根据hash值计算索引值，将该索引位置的节点赋值给e，从e开始遍历该索引位置的链表 else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { // 3.将链表节点转红黑树节点 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); // 4.如果是第一次遍历，将头节点赋值给hd if (tl == null) // tl为空代表为第一次循环 hd = p; else { // 5.如果不是第一次遍历，则处理当前节点的prev属性和上一个节点的next属性 p.prev = tl; // 当前节点的prev属性设为上一个节点 tl.next = p; // 上一个节点的next属性设置为当前节点 } // 6.将p节点赋值给tl，用于在下一次循环中作为上一个节点进行一些链表的关联操作（p.prev = tl 和 tl.next = p） tl = p; } while ((e = e.next) != null); // 7.将table该索引位置赋值为新转的TreeNode的头节点，如果该节点不为空，则以以头节点(hd)为根节点, 构建红黑树 if ((tab[index] = hd) != null) hd.treeify(tab); }} tableSizeFor(int cap)12345678910111213/** * Returns a power of two size for the given target capacity. */// 找到大于等于cap的2的幂的最小值static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;} Node1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) { ... } public final K getKey(){ ... } public final V getValue() { ... } public final String toString() { ... } public final int hashCode() { ... } public final V setValue(V newValue) { ... } public final boolean equals(Object o) { ... }} 思考 为什么哈希桶数组table的长度length大小必须为2的n次方？ 相对来说素数导致冲突的概率要小于合数，例如，Hashtable初始化桶大小为11（Hashtable扩容后不能保证还是素数） HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 resize() 1.8相对于1.7做了什么优化？ 在jdk1.8中不需要像jdk1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。 为什么jdk8开始使用尾插法？ jdk7使用头插法会颠倒原来一个桶里面链表的顺序，在并发时原来的顺序被另一个线程a颠倒了，而被挂起线程b恢复后拿到扩容前的节点和顺序继续完成第一次循环后，又遵循a线程扩容后的链表顺序重新排列链表中的顺序，最终形成了环。 总结：当两个线程分别对hashmap进行插入操作，并且都发生了扩容，并且重新扩容后的定位又在同一个桶，使用头插法时就会导致成环。 使用尾插法在扩容时会保持链表元素原本的顺序，就不会出现链表成环的问题了。但依然无法保证线程安全。例如，线程A和B同时进行put操作，刚好这两条不同的数据hash值一样，并且该位置数据为null，假设线程A进入判断后还未进行数据插入时挂起，而线程B正常执行，正常插入数据，然后线程A获取CPU时间片，此时线程A不用再进行hash判断了，问题出现：线程A会把线程B插入的数据给覆盖，发生线程不安全。 为什么hashmap的默认初始化大小是16？ 1234/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 为了服务将key映射到index的算法 index = HashCode(Key) &amp; (Length - 1)，Length - 1的值是所有二进制位全为1，这种情况下，index的结果等同于HashCode后几位的值，只要hashcode分布均匀，hash算法的结果就是分布均匀的，所以默认长度为16，是为了降低hash碰撞的几率。 为什么hashmap线程不安全？ 线程A和B同时进行put操作，刚好这两条不同的数据hash值一样，并且该位置数据为null，假设线程A进入判断后还未进行数据插入时挂起，而线程B正常执行，正常插入数据，然后线程A获取CPU时间片，此时线程A不用再进行hash判断了，问题出现：线程A会把线程B插入的数据给覆盖，发生线程不安全。 当同一个索引位置的节点在增加后达到 9 个，并且此时数组的长度大于等于 64，则会触发链表节点（Node）转红黑树节点（TreeNode），转成红黑树节点后，其实链表的结构还存在，通过 next 属性维持。而如果数组长度小于64，则不会触发链表转红黑树，而是会进行扩容。 当同一个索引位置的节点在移除后达到 6 个时，并且该索引位置的节点为红黑树节点，会触发红黑树节点转链表节点。","link":"/2020/06/18/HashMap/"},{"title":"MySQL","text":"数据库原理SQL语言的分类 DQL 数据查询语言 DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块。 DML 数据操纵语言 DML主要有三种形式：INSERT，UPDATE，DELETE。 DDL 数据定义语言 用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等，如：CREATE TABLE/VIEW/INDEX/SYN/CLUSTER。 DCL 数据控制语言 DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。 ACID Atomicity 原子性 事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用undo log实现：undo log记录事务所执行的修改操作，在回滚时反向执行即可。 Consistency 一致性 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。 Isolation 隔离性 一个事务所做的修改在最终提交之前对其它事务不可见。 Durability 持久性 一旦事务提交，其所做的修改将会永远保存到数据库中，即使系统崩溃，事务的执行结果也不能丢失。 系统崩溃可以用redo log恢复：与undo log记录数据的逻辑修改不同，redo log记录的是数据页的物理修改。 MySQL默认采用AUTOCOMMIT，即如果不显示使用Start Transaction来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。 并发一致性问题并发修改，读脏数据，不可重复读，幻影读 封锁MySQL中提供了两种锁粒度：行级锁，表级锁。 尽量只锁定需要修改的那部分数据，而不是所有资源，锁定的数据量越少，系统的并发程度就越高。但因为获取锁，释放锁，检查锁状态都会增加系统开销，所以封锁粒度越小，系统开销越大。 在选择封锁粒度时，需要在锁开销和并发度之间做一个权衡。 封锁类型读写锁Exclusive 互斥锁，X锁，又称写锁。 Shared 共享锁，S锁，又称读锁。 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁； 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁； 意向锁意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁；一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁； 隔离级别 READ UNCOMMITED 未提交读 事务中的修改，即使没有提交，对其他事务也是可见的。 READ COMMITED 提交读 一个事务只能读取已经提交的事务所做的修改。 REPEATABLE READ 可重复读 保证在同一个事务中多次读取同一数据的结果是一样的。 SERIALIZABLE 可串行化 强制事务串行执行，这样多个事务互不干扰，不会出现并发一致性问题。 隔离级别能解决的并发一致性问题 脏读 不可重复读 幻影读 未提交读 提交读 可解决 可重复读 可解决 可解决 可串行化 可解决 可解决 可解决 1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 多版本并发控制Multi-Version Concurrency Control 是MySQL的InnoDB存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。 写操作更新最新版本快照，而读操作去读旧版本快照，没有互斥关系，类似于CopyOnWrite。在MVCC中，DELETE，INSERT，UPDATE会为数据行新增一个版本快照。为了解决脏读和不可重复读问题，MVCC规定只能读取已经提交的快照。 版本号系统版本号SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自增； 事务版本号TRX_ID：事务开始时的系统版本号； 1 实现原理InnoDB的MVCC可以是通过在每行记录中保存两个隐藏的列来实现的,创建事物id，删除事物id。每开始一个新的事务，系统版本号(可以理解为事务的ID)就会自动递增，事务开始时刻的系统版本号会作为事务的ID。 Innodb的最基本行记录（row）中包含一些额外的存储信息：DATA_TRX_ID，DATA_ROLL_PTR，DB_ROW_ID，DELETE BIT 列名 长度 备注 DATA_TRX_ID 6字节 标记了最新更新这条行记录的transaction id，每处理一个事务，事物值自动+1 DATA_ROLL_PTR 7字节 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针 DB_ROW_ID 6字节 innodb自动产生聚集索引时，聚集索引包括这一列，否则聚集索引中不包括这个值。 DELETE BIT 位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候 2 Select当隔离级别是REPEATABLE READ时select操作，InnoDB查询时必须保证每行数据符合两个条件： InnoDB只查找版本号必须小于等于事务版本的数据行。这确保当前事务读取的行都是事务之前已经存在的，或者是由当前事务创建或修改的行。 行的删除操作的版本一定是未定义的或者大于当前事务的版本号，确定了当前事务开始之前，行没有被删除。 3 实现MVCC有下面几个特点（看起来有点乐观锁的味道）： a、每行数据都存在一个版本。 b、每次数据更新时都更新该版本 修改时Copy出当前版本随意修改，个事务之间无干扰。 c、保存时比较版本号，如果成功（commit），则覆盖原记录；失败则放弃copy（rollback）。 Innodb的MVCC实现方式如下： a、事务以排他锁的形式修改原始数据， b、把修改前的数据存放于undo log，通过回滚指针与主数据关联 c、修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） 索引B+ Tree 原理B Tree Balance tree，平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree是基于B Tree和叶子节点顺序访问指针进行实现，它具有B Tree的平衡性，并且通过顺序访问指针来提高区间查询的性能。在B+ Tree中，一个节点的key从左到右非递减排列。 查找操作：首先在根节点进行二分查找，找到一个key所在的指针，然后递归的在指针所指向的节点进行查找，直到找到叶子节点，然后在叶子节点上进行二分查找，找出key对应的data。 与红黑树对比1）B+ Tree有更低的树高 因为红黑树的出度为2，而B+ Tree的出度一般很大，所以红黑树的树高h比B+ Tree大非常多。 2）磁盘访问效率 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。因为进行寻道的次数与树高成正比，B+ Tree相对于红黑树有更低的树高，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以B+ Tree更适合磁盘数据的读取。 3）磁盘预读特性 为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。 与B Tree对比B Tree一个节点可以存储多个元素，相对于完全平衡二叉树整体树高降低，磁盘IO效率提高，而B+ Tree相比于B Tree，只是把非叶子节点冗余，为了提高范围查找的效率，并且存在指针指向下一个节点的叶子节点。 总结：MySQL选用B+ Tree作为索引，提高查询索引时的磁盘IO效率，和范围查询的效率，并且B+ Tree中的元素是有序的。 MySQL索引索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。 B+ Tree 索引因为B+ Tree的有序性，所以除了用于查找，还可以用于排序和分组。 是大多数MySQL存储引擎的默认索引类型。InnoDB的B+ Tree索引分为主索引和辅助索引。 主索引的叶子节点data域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有聚簇索引。 辅助索引的叶子节点的data域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找主键值，然后再到主索引中进行查找。 Hash 索引能以O(1)时间进行查找，但是失去了有序性，所以 无法用于排序和分组； 只支持精确查找，无法用于部分查找和范围查找； InnoDB 存储引擎有一个特殊的功能，自适应哈希索引，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 适用场景：等值查询场景，就只有（key, value）的情况，例如Redis等NoSQL的中间件。 全文索引用于查找文本中的关键词，而不是直接比较是否相等。全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 查找条件使用MATCH AGAINST，而不是普通的WHERE。 空间数据索引MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 索引的优点大大减少了服务器需要扫描的数据行数； 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）； 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）； 索引的使用条件对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效； 对于中到大型的表，索引就非常有效； 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术； 存储引擎InnoDB是MySQL默认的事务型存储引擎，实现了四个标准的隔离级别，默认是可重复读 REPEATABLE READ，在可重复读隔离级别下，通过MVCC+Next-key Locking 可防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，对查询性能有很大提升。 MyISAM 设计简单，数据以紧密格式存储； 提供了大量特性，包括压缩表，空间数据索引等； 不支持事务； 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 比较： 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句； 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁； 外键：InnoDB 支持外键； 备份：InnoDB 支持在线热备份； 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢； 其它特性：MyISAM 支持压缩表和空间数据索引； MySQL引擎对索引的支持 Storage Engine Permissible Index Types InnoDB BTREE MyISAM BTREE MEMORY/HEAP HASH, BTREE NDB HASH, BTREE(see note in text) 但是innoDB存储引擎支持hash索引是自适应的，innoDB存储引擎会根据表的使用情况自动为表生成hash索引，不能人为干预是否在一张表中生成hash索引。 调优方案排除缓存干扰MySQL 8.0之前存在缓存，在执行SQL时需加上SQL NoCache，得出的才是真实的查询时间。（缓存失效的场景：对表更新时，这个表的所有缓存会被清空） Explain1）为什么rows和总行数不一样？ MySQL中数据的单位都是页，通过采样统计的方法，InnoDB默认选择N个数据页，统计这些页面上不同值的平均值，然后乘这个索引的页面数，就得到了rows。 2）为什么MySQL索引不一定走到最优索引？ 假如走A索引要扫描100行，B索引只要20行，但是它可能走A索引，因为优化器在选择时发现走A索引没有额外的代价，而B索引需要回表。可使用force index。 覆盖索引在建立的索引上就已经有需要的字段了，就可以避免了回表。 联合索引需要注意索引占用的空间。 最左匹配原则可以对现有的索引进行最大化的利用。 索引下推在MySQL5.6引入的索引下推优化，在索引遍历的过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 前缀索引当需要将很长的字段作为索引时，为了节省空间，可以采用倒序或者截取字符串使区分度提高。 flush当MySQL突然卡顿时，可能发生了flush，即redo log更新到磁盘。 更新之前，当内存数据页与磁盘数据页内容不一致时，这个页被称为脏页； 内存数据写入到磁盘后，内容和磁盘上的数据页内容一致，称为干净页； flush的时机： InnoDB的redo log写满了，这时系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写； 系统内存不足，当需要新的内存页，而内存不够用时，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘； MySQL认为系统“空闲”时，只要有机会就刷一点“脏页”； MySQL正常关闭时，MySQL会把内存的脏页都flush到磁盘上，这样下次启动时，就可以直接从磁盘上读数据，提高启动速度。 如何控制flush的时机： innodb_io_capacity会告诉InnoDB你的磁盘能力，这个值建议设置成磁盘的IOPS，磁盘的IOPS可以通过fio这个工具来测试。 刷脏页时，如果旁边也是脏页，会一起刷掉。innodb_flush_neighbors=0这个参数可以不产生连带制。在MySQL 8.0中，innodb_flush_neighbors的默认值已经是0了。 索引失效的情况 对字段做了函数操作就不会走索引了，因为破坏了索引值的有序性，因为优化器就决定放弃走树搜索功能。 隐式类型转换 1select * from t where id=1 如果id是字符型 1是数字型，此时会走全表扫描。因为MySQL底层对比较进行转换，相当于加了CAST(id as signed int)。 隐式字符编码转换 如果两个表的字符集不一样，一个是utf8mb4，一个是utf8，因为utf8mb4是utf8的超集，所以一旦两个字符比较，就会转换为utf8mb4再比较。转换的过程相当于加了CONVERT(id USING utf8mb4)。 where语句中索引列使用了负向查询，可能会导致索引失效 负向查询包括：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等。 like通配符可能会导致索引失效 like查询以%开头时，会导致索引失效。 各种logbin log记录了数据库表结构和表数据变更，比如update，delete，insert，truncate，create，它不会记录select。存储着每条变更的SQL语句（还有事务id等）。 作用：复制和恢复数据。 在一主多从结构中，实现从服务器与主服务器数据保持一致； 恢复数据库数据； 三种模式： Row Level 行模式 记录每一行数据被修改的形式。（批量修改时 记录的是单条语句） 缺点：会产生大量的日志内容。 Statement Level（默认） 记录每一条修改数据的SQL语句。（记录的是批量修改的SQL语句事件） 缺点：使用了某些特定的函数或者功能时，会导致复制存在问题。 Mixed MySQL会根据执行的每一条具体的SQL来区分对待日志的记录形式，在Statement和Row之间选一种。 redo log内存写完后会写一份redo log，记载了在某页上做了什么修改。 作用：当修改时，数据写完内存但还没真正写到磁盘时，数据库挂了，此时可以根据redo log对数据进行恢复，因为redo log是顺序IO，所以写入速度很快，并且redo log记载的是物理变化（XX页做了XX修改），文件体积小，恢复速度快。 bin log和redo log 区别： 存储的内容： bin log记录的是update，delete，insert这样的SQL语句，即逻辑变化； redo log记录的是物理修改的内容（XX页修改了XX），即物理变化； 功能： bin log的作用是复制和恢复； redo log的作用是为了持久化； 产生： bin log无论MySQL引擎，都会存在；在事务提交时才记录； redo log是MySQL的InnoDB引擎产生的；事务开始时 记录每次的变更信息； MySQL通过两阶段提交保证redo log和bin log的数据是一致的： 阶段1：InnoDB redo log写盘，InnoDB事务进入prepare状态； 阶段2：bin log写盘，InnoDB事务进入commit状态； 假如数据库数据被删除，可以用redo log恢复数据么？ 不可以，redo log存储的是物理数据的变更，如果内存数据已经刷到磁盘，那redo log的数据就无效了。所以redo log不会存储着历史所有数据的变更，文件的内容会被覆盖。 undo log作用：回滚和MVCC 数据修改时，不仅记录了redo log，也记录了undo log，undo log存储的也是逻辑日志，例如insert一条数据，undo log会记录一条对应的delete日志；update一条数据，undo log会记录一条对应相反的update记录。 因为undo log存储着修改前的数据，相当于前一个版本，MVCC实现的是读写不堵塞，读的时候需要返回前一个版本的数据。 主从复制主要涉及三个线程：Binlog Dump线程、I/O线程和SQL线程。 Binlog Dump线程跑在主库上，IO线程和SQL线程跑在从库上。 Binlog Dump线程：负责将主服务器上的数据更改写入二进制日志（Binary log）中；I/O 线程：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）；SQL线程：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）； 过程当在从库上启动复制时，首先创建I/O线程连接主库； 主库随后创建Binlog Dump线程读取数据库事件并发送给I/O线程； I/O线程获取到事件数据后更新到从库的Relay Log中继日志中； 从库上的SQL线程读取Relay Log中更新的数据库事件并应用； 常见问题MySQL的存储结构MySQL的基本存储结构是页，各个数据页可以组成一个双向链表，而每个数据页中的记录又可以组成一个单向链表。每个数据页都会为存储在它内部的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。 以非主键列作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 1select * from user where username = 'Tom'; 没有进行任何优化的SQL语句： 1）定位到记录所在的页；（遍历双向链表） 2）从所在页内中查找相应的记录；（由于不是根据主键查询，所以需要遍历所在页的单链表） 索引的最左匹配原则多个列组成索引（a, b, c, d），此时key也由多个列组成，索引只能用于查找key是否存在（相等），遇到范围查询（&lt;，&gt;，between，like左匹配）等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 如有索引 (a, b, c, d) 查询条件 a=1 and b=2 and c&gt;3 and d=4，则会在每个节点依次命中a，b，c，无法命中d。因为c已经是范围查询了。 索引的缺点创建和维护索引需要时间成本； 索引需要占用物理空间； 对表中的数据进行增删改的时候，索引需要动态维护，降低了数据的维护速度； 索引的分类 普通索引 最基本的索引，没有任何限制。 1ALTER TABLE `table_name` ADD INDEX index_name (`column`) 唯一索引 与普通索引类似，索引列的值必须唯一，允许有空值。 1ALTER TABLE `table_name` ADD UNIQUE [indexName] (`column`) 主键索引 是一种特殊的唯一索引，不允许有空值。 1ALTER TABLE `table_name` ADD PRIMARY KEY (`column`) 全文索引 FULLTEXT索引用于全文搜索，只有InnoDB和MyISAM存储引擎支持 FULLTEXT索引，仅适用于CHAR，VARCHAR和TEXT列。 1ALTER TABLE `table_name` ADD FULLTEXT (`column`) Redis跳表和MySQL索引Redis使用跳表不用B+数的原因是：redis是内存数据库，而B+树纯粹是为了MySQL这种IO数据库准备的。B+树的每个节点的数量都是一个MySQL分区页的大小。 语句执行顺序一、mySql的执行顺序mysql执行sql的顺序从 From 开始，以下是执行的顺序流程 1、FROM table1 left join table2 on 将table1和table2中的数据产生笛卡尔积，生成Temp1 2、JOIN table2 所以先是确定表，再确定关联条件 3、ON table1.column = table2.columu 确定表的绑定条件 由Temp1产生中间表Temp2 4、WHERE 对中间表Temp2产生的结果进行过滤 产生中间表Temp3 5、GROUP BY 对中间表Temp3进行分组，产生中间表Temp4 6、HAVING 对分组后的记录进行聚合 产生中间表Temp5 7、SELECT 对中间表Temp5进行列筛选，产生中间表 Temp6 8、DISTINCT 对中间表 Temp6进行去重，产生中间表 Temp7 9、ORDER BY 对Temp7中的数据进行排序，产生中间表Temp8 10、LIMIT 对中间表Temp8进行分页，产生中间表Temp9 MySQL乐观锁和悲观锁乐观锁假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。适用于读多写少的场景，提高吞吐量。 实现方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳（timestamp）。乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 悲观锁悲观锁（Pessimistic Lock），顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 MySQL隐式和显示锁定InnoDB采用的是两阶段锁定协议（two-phase locking protocol）。在事务执行过程中，随时都可以执行锁定，锁只有在执行 COMMIT或者ROLLBACK的时候才会释放，并且所有的锁是在同一时刻被释放。前面描述的锁定都是隐式锁定，InnoDB会根据事务隔离级别在需要的时候自动加锁。 另外，InnoDB也支持通过特定的语句进行显示锁定，这些语句不属于SQL规范： SELECT … LOCK IN SHARE MODE SELECT … FOR UPDATE","link":"/2020/07/07/MySQL/"},{"title":"SomeSynchronizationTool","text":"同步工具 类 作用 Semaphore 限制线程的数量 Exchanger 两个线程交换数据 CountDownLatch 线程等待直到计数器减为0时开始工作 CyclicBarrier 作用跟CountDownLatch类似，但是可以重复使用 Phaser 增强的CyclicBarrier Semaphore源码分析内部有一个继承了AQS的同步器Sync，重写了tryAcquireShared方法。在这个方法里，会去尝试获取资源。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Creates a {@code Semaphore} with the given number of * permits and nonfair fairness setting. * * @param permits the initial number of permits available. * This value may be negative, in which case releases * must occur before any acquires will be granted. */// 默认情况下使用非公平public Semaphore(int permits) { sync = new NonfairSync(permits);}/** * Creates a {@code Semaphore} with the given number of * permits and the given fairness setting. * * @param permits the initial number of permits available. * This value may be negative, in which case releases * must occur before any acquires will be granted. * @param fair {@code true} if this semaphore will guarantee * first-in first-out granting of permits under contention, * else {@code false} */public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits);}final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }}// 如果阻塞队列没有等待的线程，则参与许可的竞争；否则直接插入到阻塞队列尾节点并挂起，等待唤醒protected int tryAcquireShared(int acquires) { for (;;) { if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }} demo12345678910111213141516171819202122232425262728293031public class SemaphoreDemo { static Semaphore semaphore = new Semaphore(5, true); public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(50); for (int i = 0; i &lt; 100; i++) { service.submit(new Task()); } service.shutdown(); } static class Task implements Runnable { @Override public void run() { try { semaphore.acquire(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;拿到了许可证&quot;); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;释放了许可证&quot;); semaphore.release(2); } }} 适用场景可以用来做流量分流，特别是对公共资源有限的场景； Exchanger用于两个线程交换数据，支持泛型。 源码分析使用park/unpark来实现等待状态的切换 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Waits for another thread to arrive at this exchange point (unless * the current thread is {@linkplain Thread#interrupt interrupted}), * and then transfers the given object to it, receiving its object * in return. * * &lt;p&gt;If another thread is already waiting at the exchange point then * it is resumed for thread scheduling purposes and receives the object * passed in by the current thread. The current thread returns immediately, * receiving the object passed to the exchange by that other thread. * * &lt;p&gt;If no other thread is already waiting at the exchange then the * current thread is disabled for thread scheduling purposes and lies * dormant until one of two things happens: * &lt;ul&gt; * &lt;li&gt;Some other thread enters the exchange; or * &lt;li&gt;Some other thread {@linkplain Thread#interrupt interrupts} * the current thread. * &lt;/ul&gt; * &lt;p&gt;If the current thread: * &lt;ul&gt; * &lt;li&gt;has its interrupted status set on entry to this method; or * &lt;li&gt;is {@linkplain Thread#interrupt interrupted} while waiting * for the exchange, * &lt;/ul&gt; * then {@link InterruptedException} is thrown and the current thread's * interrupted status is cleared. * * @param x the object to exchange * @return the object provided by the other thread * @throws InterruptedException if the current thread was * interrupted while waiting */@SuppressWarnings(&quot;unchecked&quot;)public V exchange(V x) throws InterruptedException { Object v; Object item = (x == null) ? NULL_ITEM : x; // translate null args if ((arena != null || (v = slotExchange(item, false, 0L)) == null) &amp;&amp; ((Thread.interrupted() || // disambiguates null return (v = arenaExchange(item, false, 0L)) == null))) throw new InterruptedException(); return (v == NULL_ITEM) ? null : (V)v;}// 如果在指定时间内没有另一个线程调用exchange，会抛出超时异常。@SuppressWarnings(&quot;unchecked&quot;)public V exchange(V x, long timeout, TimeUnit unit) throws InterruptedException, TimeoutException { Object v; Object item = (x == null) ? NULL_ITEM : x; long ns = unit.toNanos(timeout); if ((arena != null || (v = slotExchange(item, true, ns)) == null) &amp;&amp; ((Thread.interrupted() || (v = arenaExchange(item, true, ns)) == null))) throw new InterruptedException(); if (v == TIMED_OUT) throw new TimeoutException(); return (v == NULL_ITEM) ? null : (V)v;} demo1234567891011121314151617181920212223242526public class ExchangerDemo { public static void main(String[] args) throws InterruptedException { Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); new Thread(() -&gt; { try { System.out.println(&quot;这是线程A，得到了另一个线程的数据：&quot; + exchanger.exchange(&quot;这是来自线程A的数据&quot;)); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); System.out.println(&quot;这个时候线程A是阻塞的，在等待线程B的数据&quot;); Thread.sleep(1000); new Thread(() -&gt; { try { System.out.println(&quot;这是线程B，得到了另一个线程的数据：&quot; + exchanger.exchange(&quot;这是来自线程B的数据&quot;)); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); }} 适用场景一般用于两个线程之间更方便地在内存中交换数据，因为其支持泛型，所以可以传输任何的数据，比如IO流或者IO缓存。根据JDK中的注释，可总结如下： 此类提供对外的操作是同步的； 用于成对出现的线程之间交换数据； 可以视作双向的同步队列； 可应用于基因算法、流水线设计等场景； 当三个线程调用同一个实例的exchange方法时，只有前两个线程会交换数据，第三个线程会进入阻塞状态。 需要注意的是，exchange是可以重复使用的。也就是说。两个线程可以使用Exchanger在内存中不断地再交换数据。 CountDownLatchCountDownLatch用一个给定的计数器来初始化，该计数器的操作是原子操作，即同时只能有一个线程去操作该计数器。调用该类await()的线程会一直处于阻塞状态，直到其他线程调用countDown()使当前计数器的值变为零（每次调用countDown计数器的值减1）。当计数器值减至零时，所有因调用await()而处于等待状态的线程就会继续往下执行。这种现象只会出现一次，因为计数器不能被重置，如果业务上需要一个可以重置计数次数的版本，可以考虑使用CycliBarrier。 原理分析内部有一个继承了AQS的同步器Sync demo123456789101112131415161718192021222324252627282930313233343536/** * 描述： 模拟100米跑步，5名选手都准备好了，只等裁判员一声令下，所有人同时开始跑步。当所有人都到终点后，比赛结束。 */public class CountDownLatchDemo1And2 { public static void main(String[] args) throws InterruptedException { CountDownLatch begin = new CountDownLatch(1); CountDownLatch end = new CountDownLatch(5); ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 5; i++) { final int no = i + 1; Runnable runnable = new Runnable() { @Override public void run() { System.out.println(&quot;No.&quot; + no + &quot;准备完毕，等待发令枪&quot;); try { begin.await(); System.out.println(&quot;No.&quot; + no + &quot;开始跑步了&quot;); Thread.sleep((long) (Math.random() * 10000)); System.out.println(&quot;No.&quot; + no + &quot;跑到终点了&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { end.countDown(); } } }; service.submit(runnable); } //裁判员检查发令枪... Thread.sleep(3000); System.out.println(&quot;发令枪响，比赛开始！&quot;); begin.countDown(); end.await(); System.out.println(&quot;所有人到达终点，比赛结束&quot;); }} 适用场景程序执行需要等待某个条件完成后才能继续执行后续的操作；如并行计算，当某个处理的运算量很大时，可以将该运算任务拆分成多个子任务，等待所有的子任务都完成之后，父任务再拿到所有子任务的运算结果进行汇总。 CyclicBarrier类似于CountDownLatch，它也是通过计数器来实现的。当某个线程调用await方法时，该线程进入等待状态，且计数器加1，当计数器的值达到设置的初始值时，所有因调用await进入等待状态的线程被唤醒，继续执行后续操作。因为CycliBarrier在释放等待线程后可以重用，所以称为循环barrier。CycliBarrier支持一个可选的Runnable，在计数器的值到达设定值后（但在释放所有线程之前），该Runnable运行一次，Runnable在每个屏障点只运行一个。 原理分析虽然功能与CountDownLatch类似，但是实现原理却完全不同，CyclicBarrier内部使用的是Lock + Condition实现的等待/通知模式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Main barrier code, covering the various policies. */private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException,TimeoutException { final ReentrantLock lock = this.lock; lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } int index = --count; if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation &amp;&amp; ! g.broken) { breakBarrier(); throw ie; } else { // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // &quot;belong&quot; to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed &amp;&amp; nanos &lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); }} demo123456789101112131415161718192021222324252627282930313233343536public class CyclicBarrierDemo { public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(5, new Runnable() { @Override public void run() { System.out.println(&quot;所有人都到场了， 大家统一出发！&quot;); } }); for (int i = 0; i &lt; 10; i++) { new Thread(new Task(i, cyclicBarrier)).start(); } } static class Task implements Runnable { private int id; private CyclicBarrier cyclicBarrier; public Task(int id, CyclicBarrier cyclicBarrier) { this.id = id; this.cyclicBarrier = cyclicBarrier; } @Override public void run() { System.out.println(&quot;线程&quot; + id + &quot;现在前往集合地点&quot;); try { Thread.sleep((long) (Math.random() * 10000)); System.out.println(&quot;线程&quot; + id + &quot;到了集合地点，开始等待其他人到达&quot;); cyclicBarrier.await(); System.out.println(&quot;线程&quot; + id + &quot;出发了&quot;); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } } }} Barrier被破坏如果线程在等待的过程中，Barrier被破坏，就会抛出BrokenBarrierException。可以用isBroken()方法检测Barrier是否被破坏。 如果有线程已经处于等待状态，调用reset方法会导致已经在等待的线程出现BrokenBarrierException异常。并且由于出现了BrokenBarrierException，将会导致始终无法等待； 如果在等待的过程中，线程被中断，也会抛出BrokenBarrierException异常，并且这个异常会传播到其他所有的线程； 如果在执行屏障操作过程中发生异常，则该异常将传播到当前线程中，其他线程会抛出BrokenBarrierException，屏障被损坏； 如果超出指定的等待时间，当前线程会抛出TimeoutException 异常，其他线程会抛出BrokenBarrierException异常； ReetrantReadWriteLock具备的特性： 公平性 非公平锁（默认） 由于读线程之间没有锁竞争，所以读操作没有公平性和非公平性，写操作时，由于写操作可能立即获取到锁，所以会推迟一个或多个读操作或者写操作。因此非公平锁的吞吐量要高于公平锁； 公平锁 基于AQS的CLH队列，释放当前保持的锁（读锁或者写锁）时，优先为等待时间最长的那个写线程分配写入锁，当前前提是写线程的等待时间要比所有读线程的等待时间要长。 重入性 写线程获取写入锁后可以再次获取读取锁，但是读线程获取读取锁后却不能获取写入锁。 锁降级 写线程获取写入锁后可以获取读取锁，然后释放写入锁，这样就从写入锁变成了读取锁，从而实现锁降级的特性。 锁获取中断 读取锁和写入锁都支持获取锁期间被中断，这个和独占锁一致。 条件变量 写入锁提供了对Condition的支持，这个和独占锁一致，但读取锁不允许获取条件变量，否则将产生异常UnsupportedOperationException。 state在ReentrantReadWrilteLock里面将这个字段一分为二，高16位表示共享锁的数量，低16位表示独占锁的数量（或者重入数量）。 Phaser","link":"/2020/07/17/SomeSynchronizationTool/"},{"title":"SpringCode","text":"Spring 是如何默认开启循环依赖的？如何关闭支持循环依赖？ @Autowired和@Resource区别 @Autowired由AutowiredAnnotationBeanPostProcessor后置处理器解析，@Resource由CommonAnnotationBeanPostProcessor解析 循环依赖的实现 12345678/** Cache of singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256); /** Cache of singleton factories: bean name to ObjectFactory. */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16); /** Cache of early singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16); singletonObjects 单例池 Spring容器 一级缓存 存储的是bean singletonFactories 工厂 二级缓存 存储的是对象（半成品的bean） earlySingletonObjects 三级缓存 三级缓存put一个从二级缓存中生产出来的一个对象 为什么需要三级缓存？因为防止重复创建，提高效率 Spring的生命周期 new init 生命周期的初始方法 @PostConstruct 注入 代理 aop put singletonObjects Spring的refresh() prepareRefresh() refresh前的预处理 12// Prepare this context for refreshing.prepareRefresh(); initPropertySources() 初始化一些属性设置；子类自定义个性化的属性设置方法； getEnvironment().validateRequireProperties() 检验属性的合法等； earlyApplicationEvents = new LinkedHashSet&lt;&gt;(); 保存容器中的一些早期事件； 获取BeanFactory 1234567// Tell the subclass to refresh the internal bean factory.ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { refreshBeanFactory(); return getBeanFactory();} refreshBeanFactory(); 刷新（创建）BeanFactory； 12345678910111213public GenericApplicationContext() { this.beanFactory = new DefaultListableBeanFactory();}@Overrideprotected final void refreshBeanFactory() throws IllegalStateException { if (!this.refreshed.compareAndSet(false, true)) { throw new IllegalStateException( &quot;GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once&quot;); } // 设置id this.beanFactory.setSerializationId(getId());} getBeanFactory(); 返回刚才GenericApplicationContext创建的BeanFactory对象； 将创建的BeanFactory(DefaultListableBeanFactory)对象； prepareBeanFactory(beanFactory); BeanFactory的预准备工作（BeanFactory的一些设置） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { // Tell the internal bean factory to use the context's class loader etc. // 1. 设置BeanFactory的类加载器、支持表达式解析器等 beanFactory.setBeanClassLoader(getClassLoader()); beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // Configure the bean factory with context callbacks. // 2. 添加部分BeanPostProcessor【ApplicationContextAwareProcessor】 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 3. 设置忽略的自动装配的接口 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); // BeanFactory interface not registered as resolvable type in a plain factory. // MessageSource registered (and found for autowiring) as a bean. // 4. 注册可以解析的自动装配（直接在任何组件中自动注入） beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // Register early post-processor for detecting inner beans as ApplicationListeners. // 5. 添加BeanPostProcessor【ApplicationListenerDetector】 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. // 6. 添加编译时的AspectJ if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } // Register default environment beans. // 7. 给BeanFactory注册一些能用的组件 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); } if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); } if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); }} // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475 BeanFactory准备工作完成后进行的后置处理工作； 子类通过重写这个方法在BeanFactory创建并预准备完成以后做进一步的设置；=========以上是BeanFactory的创建及预准备工作=========+ invokeBeanFactoryPostProcessors(beanFactory); 执行BeanFactoryPostProcessor BeanFactoryPostProcessor：BeanFactory的后置处理器，在BeanFactory标准初始化之后执行； 两个接口：BeanFactoryPostProcessor，BeanDefinitionRegistryPostProcessor； 先执行BeanDefinitionRegistryPostProcessor： 1) 获取所有的BeanDefinitionRegistryPostProcessor； 2) 先执行实现了PriorityOrdered优先级接口的BeanDefinitionRegistryPostProcessor postProcessor.postProcessBeanDefinitionRegistry(registry); 3) 再执行实现了Ordered顺序接口的BeanDefinitionRegistryPostProcessor postProcessor.postProcessBeanDefinitionRegistry(registry); 4) 最后执行没有实现任何优先级或顺序接口的BeanDefinitionRegistryPostProcessor postProcessor.postProcessBeanDefinitionRegistry(registry); 再执行beanFactoryPostProcessor的方法： 1) 获取所有的BeanFactoryPostProcessor； 2) 先执行实现了PriorityOrdered优先级接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory); 3) 再执行实现了Ordered顺序接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory); 4) 最后执行没有实现任何优先级或顺序接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory);+ registerBeanPostProcessors(beanFactory);注册BeanPostProcessor【Bean的后置处理器，拦截bean的创建过程】 不同接口类型的BeanPostProcessor在Bean创建前后的执行时机是不一样的： BeanPostProcessor InstantiationAwareBeanPostProcessor DestructionAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor MergedBeanDefinitionPostProcessor【internalPostProcessors】 1) 获取所有的BeanPostProcessor；（后置处理器都可以通过PriorityOrdered、Ordered接口来执行优先级） 2) 先注册PriorityOrdered优先级接口的BeanPostProcessor 把每一个BeanPostProcessor添加到Beanfactory中 ```java /** * Register the given BeanPostProcessor beans. */ private static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanPostProcessor&gt; postProcessors) { for (BeanPostProcessor postProcessor : postProcessors) { beanFactory.addBeanPostProcessor(postProcessor); } } 3) 再注册Ordered接口的； 4) 最后注册没有实现任何优先级接口的； 5) 最终注册MergedBeanDefinitionPostProcessor； 6) 注册一个ApplicationListenerDetector； 在Bean创建完成后检查是否是ApplicationListener，如果是，则 applicationContext.addApplicationListener((ApplicationListener&lt;?&gt;) bean); 123// Re-register post-processor for detecting inner beans as ApplicationListeners,// moving it to the end of the processor chain (for picking up proxies etc).beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); initMessageSource(); 初始化MessageSource组件（做国际化功能，消息绑定，消息解析） 获取BeanFactory； 看容器中是否有id为messageSource，类型为MessageSource的组件； 若有则赋值给messageSource，若没有则new DelegatingMessageSource(); MessageSource：取出国际化配置文件中的某个key值（能按照区域信息获取）； 把创建好的MessageSource注册在容器中，以后获取国际化配置文件的值时，可以自动注入MessageSource； beanFactory.registerSingleton(MESSAGE_SOURCE_BEAN_NAME, this.messageSource); MessageSource.getMessage(String code, @Nullable Object[] args, Locale locale) initApplicationEventMulticaster(); 初始化事件派发器 获取BeanFactory； 从BeanFactory中获取applicationEventMulticaster的ApplicationEventMulticaster； 如果上一步没有配置，则创建 new SimpleApplicationEventMulticaster(beanFactory); 将创建的ApplicationEventMulticaster添加到BeanFactory中，以后其他组件直接自动注入； onRefresh(); 留给子容器（子类） 子类重写这个方法，在容器刷新时可以自定义逻辑； registerListeners(); 将容器中所有项目里面的ApplicationListener注册进来 从容器中拿到所有的ApplicationListener 2） 将每个监听器添加到事件派发器中 1234// Register statically specified listeners first.for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) { getApplicationEventMulticaster().addApplicationListener(listener);} 派发之前步骤产生的事件 finishBeanFactoryInitialization(beanFactory); 初始化所有剩下的单实例bean beanFactory.preInstantiateSingletons(); 获取容器中的所有Bean，依次进行初始化和创建对象 获取Bean的定义信息：RootBeanDefinition Bean不是抽象的，是单实例的，不是懒加载的： ​ 1) 判断是否是FactoryBean：是否是实现FactoryBean接口的Bean； ​ 2) 不是FactoryBean，利用getBean(beanName);创建对象； ​ doGetBean(name, null, null, false); 先获取缓存中保存的单实例Bean，如果能获取到说明这个Bean之前被创建过（所有创建过的单实例Bean都会被缓存起来）； 从private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256); 中获取 缓存中获取不到，开始Bean的创建对象流程； 标记当前bean已经被创建； 获取Bean的定义信息； 获取当前Bean依赖的其他Bean，若有 则按照getBean()把依赖的Bean先创建出来； 启动单实例Bean的创建流程； ​ 1) createBean(beanName, mbd, args); ​ 2) Object bean = resolveBeforeInstantiation(beanName, mbdToUse); 让BeanPostProcessor先拦截返回代理对象 InstantiationAwareBeanPostProcessor 提前执行 先触发postProcessBeforeInstantiation(); 如果有返回值，触发postProcessAfterInitialization(); ​ 3) 如果前面的InstantiationAwareBeanPostProcessor 没有返回代理对象，则执行4)； ​ 4) Object beanInstance = doCreateBean(beanName, mbdToUse, args); 创建Bean ​ 1) 创建Bean实例：createBeanInstance(beanName, mbd, args); ​ 利用工厂方法或者对象的构造器创建出Bean实例； ​ 2) applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); ​ 调用MergedBeanDefinitionPostProcessor 的postProcessMergedBeanDefinition(mbd, beanType, beanName); ​ 3) 【Bean属性赋值】populateBean(beanName, mbd, instanceWrapper); ​ 赋值之前： ​ 1) 拿到InstantiationAwareBeanPostProcessor后置处理器：postProcessAfterInstantiation(); ​ 2) 拿到InstantiationAwareBeanPostProcessor后置处理器：postProcessPropertyValues(); ​ 赋值： ​ 3) 应用Bean属性的值，为属性利用setter方法等进行赋值：applyPropertyValues(beanName, mbd, bw, pvs); ​ 4) 【Bean初始化】exposedObject = initializeBean(beanName, exposedObject, mbd); ​ 1) 【执行Aware接口方法】invokeAwareMethods(beanName, bean); 执行xxxAware的方法；BeanNameAware/BeanClassLoaderAware/BeanFactoryAware ​ 2) 【执行后置处理器初始化之前】applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); ​ BeanPostProcessor.postProcessBeforeInitialization(); ​ 3) 【执行初始化方法】invokeInitMethods(beanName, wrappedBean, mbd); ​ 1) 是否是InitializingBean接口的实现，执行接口规定的初始化； ​ 2) 是否自定义初始化； ​ 4) 【执行后置处理器初始化之后】applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); BeanPostProcessor.postProcessAfterInitialization() ​ 5) 注册Bean的销毁方法registerDisposableBeanIfNecessary(beanName, bean, mbd); ​ 5) 将创建的Bean添加到缓存中SingletonObjects； ​ IOC容器就是这些Map，很多的Map里面保存了Bean和环境信息等； 所有Bean都利用getBean创建完成之后，检查所有的Bean是否是SmartInitializingSingleton接口的，如果是执行afterSingletonsInstantiated()； finishRefresh(); 完成BeanFactory的初始化创建工作，IOC容器就创建完成 123456789101112131415161718// Clear context-level resource caches (such as ASM metadata from scanning).clearResourceCaches();// Initialize lifecycle processor for this context.// 初始化和生命周期有关的后置处理器LifecycleProcessor，默认从容器中找是否有LifecycleProcessor的组件，// 如果没有new DefaultLifecycleProcessor();加入到容器initLifecycleProcessor();// Propagate refresh to lifecycle processor first.// 拿到前面定义的生命周期处理器，回调onRefresh()getLifecycleProcessor().onRefresh();// Publish the final event.// 发布容器刷新完成事件publishEvent(new ContextRefreshedEvent(this));// Participate in LiveBeansView MBean, if active.LiveBeansView.registerApplicationContext(this); 总结： 1）Spring容器在启动时，会先保存所有注册进来的Bean的定义信息 ​ 1）xml注册bean： ​ 2）注解注册bean 2）Spring容器在合适的时机创建这些Bean ​ 1）用到这个Bean的时候：利用getBean创建bean，创建好以后保存在容器中； ​ 2）统一创建剩下所有的bean时，finishBeanFactoryInitialization() 3）后置处理器：BeanPostProcessor 每个bean创建完成时，都会使用各种后置处理器进行处理，来增强bean的功能； AutowiredAnnotationBeanPostProcessor：处理自动注入； AnnotationAwareAspectJAutoProxyCreator：来做AOP功能 … 4）事件驱动模型 ApplicationListener：事件监听 ApplicationEventMulticaster：事件派发 依赖注入 构造器注入（默认无参构造） set注入 @autowired Spring 常用注解 组件类注解： @Component ：标准一个普通的spring Bean类。 @Repository：标注一个DAO组件类。 @Service：标注一个业务逻辑组件类。 @Controller：标注一个控制器组件类。 装配bean时常用的注解： @Autowired：属于Spring 的org.springframework.beans.factory.annotation包下,可用于为类的属性、构造器、方法进行注值； @Resource：不属于spring的注解，而是来自于JSR-250位于java.annotation包下，使用该annotation为目标bean指定协作者Bean。 @PostConstruct 和 @PreDestroy 方法 实现初始化和销毁bean之前进行的操作； SpringMVC常用的注解 @Controller ：表明该类会作为与前端作交互的控制层组件，通过服务接口定义的提供访问应用程序的一种行为，解释用户的输入，将其转换成一个模型然后将试图呈献给用户。 @RequestMapping ： 这个注解用于将url映射到整个处理类或者特定的处理请求的方法。可以只用通配符； @RequestParam ：将请求的参数绑定到方法中的参数上，有required参数，默认情况下，required=true，也就是改参数必须要传。如果改参数可以传可不传，可以配置required=false。 @PathVariable ： 该注解用于方法修饰方法参数，会将修饰的方法参数变为可供使用的uri变量（可用于动态绑定）。 @RequestBody ： @RequestBody是指方法参数应该被绑定到HTTP请求Body上。 @ResponseBody ： @ResponseBody与@RequestBody类似，它的作用是将返回类型直接输入到HTTP response body中。@ResponseBody在输出JSON格式的数据时，会经常用到。 AOP通配符： *匹配任意数量的字符 +匹配指定类及其子类 ..一般用于匹配任意数的子包或参数 原理 织入的时机 编译期（AspectJ） 类加载时（AspectJ 5+） 运行时（Spring AOP） 动态代理 实现：基于接口代理，基于继承代理 JDK动态代理 类：java.lang.reflect.Proxy 接口：InvocationHandler 只能基于接口进行动态代理 cglib动态代理 区别 JDK只能针对有接口的类的接口方法进行动态代理，jdk也不能对private方法进行动态代理 cglib基于继承来实现代理，无法对static，final类进行代理，无法对private，static方法进行代理 JDK动态代理使用Java原生的反射API进行操作，在生成类上比较高效；CGLIB使用ASM框架直接对字节码进行操作，在类的执行过程中比较高效； 在Spring中的实现： 123456789101112131415161718192021 public class DefaultAopProxyFactory implements AopProxyFactory, Serializable { @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } ...} 如果目标对象实现了接口，则默认采用JDK动态代理，否则采用cglib动态代理； 如果目标对象实现了接口，且强制cglib代理，则使用cglib动态代理； 强制使用cglib动态代理：@EnableAspectJAutoProxy(proxyTargetClass = true) 多个AOP如何叠加？ 责任链模式 1234567891011121314151617181920212223242526272829303132333435 public class ReflectiveMethodInvocation implements ProxyMethodInvocation, Cloneable { ... @Override @Nullable public Object proceed() throws Throwable { // We start with an index of -1 and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) { return invokeJoinpoint(); } Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) { // Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass()); if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) { return dm.interceptor.invoke(this); } else { // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. return proceed(); } } else { // It's an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); } } ...} IOC","link":"/2020/06/30/SpringCode/"},{"title":"ThreadPool","text":"线程池为什么使用线程池 创建/销毁线程需要消耗系统资源，线程池可以复用已创建的线程； 并发线程数量过多，可能会导致资源消耗过多，从而造成服务器崩溃，线程池可以统一管理线程，控制并发的数量（主要原因）； 原理 ThreadPoolExecutor 四种构造方法 12345678910111213141516171819202122232425262728293031// 五个参数的构造函数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)// 六个参数的构造函数-1public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory)// 六个参数的构造函数-2public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler)// 七个参数的构造函数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) int corePoolSize：该线程池中核心线程数最大值； 核心线程：线程池中有两类线程，核心线程和非核心线程。核心线程默认情况下会一直存在于线程池中，即使这个核心线程什么都不干。 int maximumPoolSize：该线程池中线程总数最大值； long keepAliveTime：非核心线程闲置超时时长； 非核心线程如果处于闲置状态超过该值，就会被销毁。如果设置allowCoreThreadTimeOut(true)，则会也作用于核心线程。 TimeUnit unit：keepAliveTime的单位。 TimeUnit是一个枚举类型 ，包括以下属性： NANOSECONDS ： 1微毫秒 = 1微秒 / 1000 MICROSECONDS ： 1微秒 = 1毫秒 / 1000 MILLISECONDS ： 1毫秒 = 1秒 /1000 SECONDS ： 秒 MINUTES ： 分 HOURS ： 小时 DAYS ： 天 BlockingQueue workQueue：阻塞队列，维护着等待执行的Runnable任务对象； ThreadFactory threadFactory：创建线程的工厂，用于批量创建线程，统一在创建线程时设置一些参数，如是否守护线程、线程的优先级等。如果不指定，会新建一个默认的线程工厂。 123456789101112131415161718192021222324252627/** * The default thread factory */static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = &quot;pool-&quot; + poolNumber.getAndIncrement() + &quot;-thread-&quot;; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; }} RejectedExecutionHandler handler：拒绝处理策略，线程数量大于最大线程数就会采用拒绝处理策略，四种： ThreadPoolExecutor.AbortPolicy：默认拒绝处理策略，丢弃任务并抛出RejectedExecutionException异常； ThreadPoolExecutor.DiscardPolicy：丢弃新来的任务，但是不抛出异常； ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列头部（最旧的）的任务，然后重新尝试执行程序（如果再次失败，重复此过程）； ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务； 线程池状态线程池本身有一个调度线程，这个线程就是用于管理布控整个线程池里的各种任务和事务，例如创建线程、销毁线程、任务队列管理、线程队列管理等等，故线程池也有自己的状态。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * The main pool control state, ctl, is an atomic integer packing * two conceptual fields * workerCount, indicating the effective number of threads * runState, indicating whether running, shutting down etc * * In order to pack them into one int, we limit workerCount to * (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2 * billion) otherwise representable. If this is ever an issue in * the future, the variable can be changed to be an AtomicLong, * and the shift/mask constants below adjusted. But until the need * arises, this code is a bit faster and simpler using an int. * * The workerCount is the number of workers that have been * permitted to start and not permitted to stop. The value may be * transiently different from the actual number of live threads, * for example when a ThreadFactory fails to create a thread when * asked, and when exiting threads are still performing * bookkeeping before terminating. The user-visible pool size is * reported as the current size of the workers set. * * The runState provides the main lifecycle control, taking on values: * * RUNNING: Accept new tasks and process queued tasks * SHUTDOWN: Don't accept new tasks, but process queued tasks * STOP: Don't accept new tasks, don't process queued tasks, * and interrupt in-progress tasks * TIDYING: All tasks have terminated, workerCount is zero, * the thread transitioning to state TIDYING * will run the terminated() hook method * TERMINATED: terminated() has completed * * The numerical order among these values matters, to allow * ordered comparisons. The runState monotonically increases over * time, but need not hit each state. The transitions are: * * RUNNING -&gt; SHUTDOWN * On invocation of shutdown(), perhaps implicitly in finalize() * (RUNNING or SHUTDOWN) -&gt; STOP * On invocation of shutdownNow() * SHUTDOWN -&gt; TIDYING * When both queue and pool are empty * STOP -&gt; TIDYING * When pool is empty * TIDYING -&gt; TERMINATED * When the terminated() hook method has completed * * Threads waiting in awaitTermination() will return when the * state reaches TERMINATED. * * Detecting the transition from SHUTDOWN to TIDYING is less * straightforward than you'd like because the queue may become * empty after non-empty and vice versa during SHUTDOWN state, but * we can only terminate if, after seeing that it is empty, we see * that workerCount is 0 (which sometimes entails a recheck -- see * below). */private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; // runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 线程池创建后处于RUNNING状态； 调用shutdown()方法后处于SHUTDOWN状态，线程池不能接受新的任务，清除一些空闲worker，会等待阻塞队列的任务完成；（SHUTDOWN：不接受新任务，但处理排队任务） 调用shutdownNow()方法后处于STOP状态，线程池不能接受新的任务，中断所有线程，阻塞队列中没有被执行的任务全部丢弃。此时，poolsize=0,阻塞队列的size也为0；（STOP：不接受新任务，也不处理排队任务，并中断正在执行的任务） 当所有的任务已终止，ctl记录的“任务数量”为0，线程池会变为TIDYING状态。接着会执行terminated()钩子方法； ThreadPoolExecutor中有一个控制状态的属性叫ctl，它是一个AtomicInteger类型的变量。 线程池处在TIDYING状态时，执行完terminated()方法之后，就会由 TIDYING -&gt; TERMINATED， 线程池被设置为TERMINATED状态； 线程池的任务处理流程123456789101112131415161718192021222324252627282930313233343536373839404142// JDK 1.8// 执行命令，其中命令（下面称任务）对象是Runnable的实例public void execute(Runnable command) { // 判断命令（任务）对象非空 if (command == null) throw new NullPointerException(); // 获取ctl的值 int c = ctl.get(); // 判断如果当前工作线程数小于核心线程数，则创建新的核心线程并且执行传入的任务 if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) // 如果创建新的核心线程成功则直接返回 return; // 这里说明创建核心线程失败，需要更新ctl的临时变量c c = ctl.get(); } // 如果不小于corePoolSize，则将任务添加到workQueue队列。 // 判断线程池是否处于运行中状态，同时尝试用非阻塞方法向任务队列放入任务（放入任务失败返回false） if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); // 这里是向任务队列投放任务成功，对线程池的运行中状态做二次检查 // 如果线程池二次检查状态是非运行中状态，则从任务队列移除当前的任务调用拒绝策略处理之（也就是移除前面成功入队的任务实例） if (! isRunning(recheck) &amp;&amp; remove(command)) // 调用拒绝策略处理任务 - 返回 reject(command); // 线程池处于running状态，但是没有线程，则创建线程 // 如果当前工作线程数量为0，则创建一个非核心线程并且传入的任务对象为null - 返回 // 也就是创建的非核心线程不会马上运行，而是等待获取任务队列的任务去执行 // 如果前工作线程数量不为0，原来应该是最后的else分支，但是可以什么也不做，因为任务已经成功入队列，总会有合适的时机分配其他空闲线程去执行它 else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 走到这里说明有以下的前提： // 0、线程池中的工作线程总数已经大于等于corePoolSize（简单来说就是核心线程已经全部懒创建完毕） // 1、线程池可能不是RUNNING状态 // 2、线程池可能是RUNNING状态同时任务队列已经满了 // 如果放入workQueue失败，则创建非核心线程执行任务， // 如果这时创建非核心线程失败(当前线程总数不小于maximumPoolSize时)，就会执行拒绝策略。 else if (!addWorker(command, false)) // 调用拒绝策略处理任务 - 返回 reject(command);} 为什么要二次检查线程池的状态? 在多线程的环境下，线程池的状态是时刻发生变化的。很有可能刚获取线程池状态后线程池状态就改变了。判断是否将command加入workqueue是线程池之前的状态。倘若没有二次检查，万一线程池处于非RUNNING状态（在多线程环境下很有可能发生），那么command永远不会执行。 整体流程： 线程总数量小于corePoolSize，无论线程是否空闲，都会直接创建核心线程执行任务（注意，这一步需要获得全局锁）； 线程总数量大于等于corePoolSize，判断线程池是否处于运行中状态，同时尝试用非阻塞方法向任务队列放入任务，这里会二次检查线程池运行状态，如果当前工作线程数量为0，则创建一个非核心线程并且传入的任务对象为null； 如果缓存队列满了，则会尝试创建非核心线程传入任务实例执行（注意这一步需要获得全局锁）； 如果创建非核心线程失败，此时需要拒绝执行任务，调用拒绝策略处理任务； ThreadPoolExecutor如何做到线程复用的？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * Checks if a new worker can be added with respect to current * pool state and the given bound (either core or maximum). If so, * the worker count is adjusted accordingly, and, if possible, a * new worker is created and started, running firstTask as its * first task. This method returns false if the pool is stopped or * eligible to shut down. It also returns false if the thread * factory fails to create a thread when asked. If the thread * creation fails, either due to the thread factory returning * null, or due to an exception (typically OutOfMemoryError in * Thread.start()), we roll back cleanly. * * @param firstTask the task the new thread should run first (or * null if none). Workers are created with an initial first task * (in method execute()) to bypass queuing when there are fewer * than corePoolSize threads (in which case we always start one), * or when the queue is full (in which case we must bypass queue). * Initially idle threads are usually created via * prestartCoreThread or to replace other dying workers. * * @param core if true use corePoolSize as bound, else * maximumPoolSize. (A boolean indicator is used here rather than a * value to ensure reads of fresh values after checking other pool * state). * @return true if successful */private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { // 创建一个worker对象 w = new Worker(firstTask); // 实例化一个Thread对象 final Thread t = w.thread; if (t != null) { // 线程池全局锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { // 启动这个线程 t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted;} worker类部分源码： 12345678910111213141516// Worker类部分源码private final class Worker extends AbstractQueuedSynchronizer implements Runnable{ final Thread thread; Runnable firstTask; Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } public void run() { runWorker(this); } //其余代码略...} Worker类实现了Runnable接口，所以Worker也是一个线程任务。在构造方法中，创建了一个线程，线程的任务就是自己。故addWorker方法调用addWorker方法源码下半部分中的第4步t.start，会触发Worker类的run方法被JVM调用。 常见的四种线程池 1）newCachedThreadPool：它是一个可以无限扩大的线程池 12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} 它比较适合处理执行时间比较短的任务； corePoolSize为0，maxPoolSize为无限大，意味着线程数量可以无限大，keepAliveTime为60S，意味着线程空闲时间超过60S就会被回收； 采用SynchronousQueue装等待的任务，这个阻塞队列没有存储空间，这意味着只要有请求到来，就必须要找到一条工作线程处理他，如果当前没有空闲的线程，那么就会再创建一条新的线程； 存在的问题：无界线程池，具有自动回收多余线程的功能。弊端在于第二个参数maxPoolSize被设置为Integer.MAX_VALUE，这可能会创建数量非常多的线程，甚至导致OOM。 2）newFixedThreadPool：它是一种固定大小的线程池 12345public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());} corePoolSize和maxPoolSize都为用户设定的线程数量nThreads； keepAliveTime为0，意味着一旦有多余的空闲线程，就会被立即停止掉；但这里keepAliveTime无效； 阻塞队列采用了LinkedBlockingQueue，它是一个无界队列，因此永远不可能拒绝任务；故如果核心线程空闲，则交给核心线程处理；如果核心线程不空闲，则入列等待，直到核心线程空闲。 存在的问题：由于传入的LinkedBlockingQueue是没有容量上限的，所以当请求数越来越多，并且无法及时处理完毕的时候，即请求堆积的时候，会容易造成占用大量的内存，可能会导致OOM。 与CachedThreadPool的区别： 因为 corePoolSize == maximumPoolSize ，所以FixedThreadPool只会创建核心线程。 而CachedThreadPool因为corePoolSize=0，所以只会创建非核心线程。 在 getTask() 方法，如果队列里没有任务可取，线程会一直阻塞在 LinkedBlockingQueue.take() ，线程不会被回收。 CachedThreadPool会在60s后收回。 由于线程不会被回收，会一直卡在阻塞，所以没有任务的情况下， FixedThreadPool占用资源更多。 都几乎不会触发拒绝策略，但是原理不同。FixedThreadPool是因为阻塞队列可以很大（最大为Integer最大值），故几乎不会触发拒绝策略；CachedThreadPool是因为线程池很大（最大为Integer最大值），几乎不会导致线程数量大于最大线程数，故几乎不会触发拒绝策略。 3）newSingleThreadExecutor：有且仅有一个核心线程 123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));} 有且仅有一个核心线程（corePoolSize = maximumPoolSize = 1），使用了LinkedBlockingQueue（容量很大），所以不会创建非核心线程。所有任务按照先来先执行的顺序执行。 存在的问题：由于传入的LinkedBlockingQueue是没有容量上限的，所以如果这个唯一的线程不空闲，那么新来的任务会堆积在任务队列里等待执行，会容易造成占用大量的内存，可能会导致OOM。 4）newScheduledThreadPool：可调度的线程池 12345678910public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize);}//ScheduledThreadPoolExecutor():public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue());} 创建一个定长线程池，支持定时及周期性任务执行。 线程池里的线程数量设定为多少比较合适？ CPU密集型（加密，计算hash等）：最佳线程数为CPU核心数的1-2倍左右； 耗时IO型（读写数据库、文件、网络读写等）：最佳线程数一般会大于CPU核心数很多倍，以JVM线程监控显示繁忙情况为依据，保证线程空闲可以衔接上。 参考Brain Goetz推荐的计算方法； 线程数 = CPU核心数 * （1 + 平均等待时间 / 平均工作时间） 或者进行压测，根据压测结果确定线程数； 阻塞队列场景BlockingQueue一般用于生产者-消费者模式，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。BlockingQueue就是存放元素的容器。 操作方法 方法\\处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() - - 抛出异常：如果试图的操作无法立即执行，抛异常。当阻塞队列满时候，再往队列里插入元素，会抛出IllegalStateException(“Queue full”)异常。当队列为空时，从队列里获取元素时会抛出NoSuchElementException异常 。 返回特殊值：如果试图的操作无法立即执行，返回一个特殊值，通常是true / false。 一直阻塞：如果试图的操作无法立即执行，则一直阻塞或者响应中断。 超时退出：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功，通常是 true / false。 注意之处 不能往阻塞队列中插入null,会抛出空指针异常。 可以访问阻塞队列中的任意元素，调用remove(o)可以将队列之中的特定对象移除，但并不高效，尽量避免使用。 实现类 1）ArrayBlockingQueue 有界阻塞队列，内部结构是数组，故具有数组的特性。 123public ArrayBlockingQueue(int capacity, boolean fair){ //..省略代码} 可以初始化队列大小， 且一旦初始化不能改变。构造方法中的fair表示控制对象的内部锁是否采用公平锁，默认是非公平锁。 2）LinkedBlockingQueue 有界阻塞队列，内部结构是链表，具有链表的特性。默认队列的大小是Integer.MAX_VALUE，也可以指定大小。此队列按照先进先出的原则对元素进行排序。 3）DelayQueue DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 注入其中的元素必须实现 java.util.concurrent.Delayed 接口。该队列中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。 4）PriorityBlockingQueue 基于优先级的无界阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），内部控制线程同步的锁采用的是公平锁。 PriorityBlockingQueue不会阻塞数据生产者（因为队列是无界的），而只会在没有可消费的数据时，阻塞数据的消费者。因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。对于使用默认大小的LinkedBlockingQueue也是一样的。 5）SynchronousQueue 这个队列比较特殊，没有任何内部容量，甚至连一个队列的容量都没有。并且每个put必须等待一个take，反之亦然。 需要区别容量为1的ArrayBlockingQueue、LinkedBlockingQueue。 以下方法的返回值，可以帮助理解这个队列： iterator() 永远返回空，因为里面没有东西； peek() 永远返回null； put() 往queue放进去一个element以后就一直wait直到有其他thread进来把这个element取走； offer() 往queue里放一个element后立即返回，如果碰巧这个element被另一个thread取走了，offer方法返回true，认为offer成功；否则返回false； take() 取出并且remove掉queue里的element，取不到东西他会一直等； poll() 取出并且remove掉queue里的element，只有到碰巧另外一个线程正在往queue里offer数据或者put数据的时候，该方法才会取到东西。否则立即返回null； isEmpty() 永远返回true； remove()&amp;removeAll() 永远返回false； 源码分析 构造器 对同一个锁（lock）初始化了两个监视器，分别是notEmpty和notFull。这两个监视器的作用目前可以简单理解为标记分组，当该线程是put操作时，给他加上监视器notFull,标记这个线程是一个生产者；当线程是take操作时，给他加上监视器notEmpty，标记这个线程是消费者。 123456789101112131415161718192021//数据元素数组final Object[] items;//下一个待取出元素索引int takeIndex;//下一个待添加元素索引int putIndex;//元素个数int count;//内部锁final ReentrantLock lock;//消费者监视器private final Condition notEmpty;//生产者监视器private final Condition notFull; public ArrayBlockingQueue(int capacity, boolean fair) { //..省略其他代码 lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition();} put源码 12345678910111213141516171819202122232425262728public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; // 1.自旋拿锁 lock.lockInterruptibly(); try { // 2.判断队列是否满了 while (count == items.length) // 2.1如果满了，阻塞该线程，并标记为notFull线程， // 等待notFull的唤醒，唤醒之后继续执行while循环。 notFull.await(); // 3.如果没有满，则进入队列 enqueue(e); } finally { lock.unlock(); }}private void enqueue(E x) { // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; // 4 唤醒一个等待的线程 notEmpty.signal();} 所有执行put操作的线程竞争lock锁，拿到了lock锁的线程进入下一步，没有拿到lock锁的线程自旋竞争锁。 判断阻塞队列是否满了，如果满了，则调用await方法阻塞这个线程，并标记为notFull（生产者）线程，同时释放lock锁,等待被消费者线程唤醒。 如果没有满，则调用enqueue方法将元素put进阻塞队列。注意这一步的线程还有一种情况是第二步中阻塞的线程被唤醒且又拿到了lock锁的线程。 唤醒一个标记为notEmpty（消费者）的线程。 take源码 1234567891011121314151617181920212223242526public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); }}private E dequeue() { // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(&quot;unchecked&quot;) E x = (E) items[takeIndex]; items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); notFull.signal(); return x;} take操作和put操作的流程是类似的，总结一下take操作的流程： 所有执行take操作的线程竞争lock锁，拿到了lock锁的线程进入下一步，没有拿到lock锁的线程自旋竞争锁。 判断阻塞队列是否为空，如果是空，则调用await方法阻塞这个线程，并标记为notEmpty（消费者）线程，同时释放lock锁,等待被生产者线程唤醒。 如果没有空，则调用dequeue方法。注意这一步的线程还有一种情况是第二步中阻塞的线程被唤醒且又拿到了lock锁的线程。 唤醒一个标记为notFull（生产者）的线程。 应用场景 生产者-消费者模式 线程池中 常见问题 submit和execute区别 参数不一样 1234void execute(Runnable command);Future&lt;T&gt; submit(Callable&lt;T&gt; task);Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); execute()在Executor接口中定义的，在ThreadPoolExecutor中实现的； execute会在运行期直接抛出异常，submit在调用Future.get时才会抛出异常； 线程池如何保证当前线程获取池内的worker时不产生争用 worker实现了AQS，通过volatile修饰的state 12345678// Lock methods//// The value 0 represents the unlocked state.// The value 1 represents the locked state.protected boolean isHeldExclusively() { return getState() != 0;}","link":"/2020/08/05/ThreadPool/"},{"title":"basicconcurrency","text":"进程和线程的区别 进程是一个独立的运行环境，而线程是在进程中执行的一个任务。他们两个本质的区别是是否单独占有内存地址空间及其它系统资源（比如I/O）： 进程单独占有一定的内存地址空间，所以进程间存在内存隔离，数据是分开的，数据共享复杂但是同步简单，各个进程之间互不干扰；而线程共享所属进程占有的内存地址空间和资源，数据共享简单，但是同步复杂。 进程单独占有一定的内存地址空间，一个进程出现问题不会影响其他进程，不影响主程序的稳定性，可靠性高；一个线程崩溃可能影响整个程序的稳定性，可靠性较低。 进程单独占有一定的内存地址空间，进程的创建和销毁不仅需要保存寄存器和栈信息，还需要资源的分配回收以及页调度，开销较大；线程只需要保存寄存器和栈信息，开销较小。 另外一个重要区别是，进程是操作系统进行资源分配的基本单位，而线程是操作系统进行调度的基本单位，即CPU分配时间的单位 。 上下文切换 CPU为每个进程分配一个时间段，称作它的时间片。如果在时间片结束时进程还在运行，则暂停这个进程的运行，并且CPU分配给另一个进程（这个过程叫做上下文切换）。如果进程在时间片结束前阻塞或结束，则CPU立即进行切换，不用等待时间片用完。 上下文切换（有时也称做进程切换或任务切换）是指 CPU 从一个进程（或线程）切换到另一个进程（或线程）。上下文是指某一时间点 CPU 寄存器和程序计数器的内容。 CPU通过为每个线程分配CPU时间片来实现多线程机制。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。 但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。 寄存器是cpu内部的少量的速度很快的闪存，通常存储和访问计算过程的中间值提高计算机程序的运行速度。 程序计数器是一个专用的寄存器，用于表明指令序列中 CPU 正在执行的位置，存的值为正在执行的指令的位置或者下一个将要被执行的指令的位置，具体实现依赖于特定的系统。 举例说明 线程A - B 1.先挂起线程A，将其在cpu中的状态保存在内存中。 2.在内存中检索下一个线程B的上下文并将其在 CPU 的寄存器中恢复,执行B线程。 3.当B执行完，根据程序计数器中指向的位置恢复线程A。 实现多线程 我们在程序里面调用了start()方法后，虚拟机会先为我们创建一个线程，然后等到这个线程第一次得到时间片时再调用run()方法。 1.1 继承Thread类或者实现Runnable接口这两种方式，它们之间有什么优劣呢？ 由于Java“单继承，多实现”的特性，Runnable接口使用起来比Thread更灵活；Runnable接口出现更符合面向对象，将线程单独进行对象的封装；Runnable接口出现，降低了线程对象和线程任务的耦合性；如果使用线程时不需要使用Thread类的诸多方法，显然使用Runnable接口更为轻量；所以，我们通常优先使用“实现Runnable接口”这种方式来自定义线程类。 1.2 同时使用两种方法： 12345678public static void main(String[] args) { new Thread(() -&gt; System.out.println(&quot;我来自Runnable&quot;)) { @Override public void run() { System.out.println(&quot;我来自Thread&quot;); } }.start(); } 输出：我来自Thread 线程的状态 RUNNABLE： 1234567/** * Thread state for a runnable thread. A thread in the runnable * state is executing in the Java virtual machine but it may * be waiting for other resources from the operating system * such as processor. */RUNNABLE, 处于RUNNABLE状态的线程在Java虚拟机中运行，也有可能在等待其他系统资源（比如I/O）。 Java线程的RUNNABLE状态其实是包括了传统操作系统线程的ready和running两个状态的。 线程的生命周期 1234567891011121314151617181920212223242526272829303132public synchronized void start() { /** * This method is not invoked for the main method thread or &quot;system&quot; * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state &quot;NEW&quot;. */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group's list of threads * and the group's unstarted count can be decremented. */ group.add(this); boolean started = false; try { start0(); started = true; } finally { try { // 若线程启动失败，从线程组里移除该线程 if (!started) { group.threadStartFailed(this); } } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ } }} 如何正确停止线程 线程中断机制是一种协作机制。通过中断操作并不能直接终止一个线程，而是通知需要被中断的线程自行处理。 Thread.interrupt()：中断线程。这里的中断线程并不会立即停止线程，而是设置线程的中断状态为true（默认是flase）； Thread.interrupted()：测试当前线程是否被中断。线程的中断状态受这个方法的影响，意思是调用一次使线程中断状态设置为true，连续调用两次会使得这个线程的中断状态重新转为false； Thread.isInterrupted()：测试当前线程是否被中断。与上面方法不同的是调用这个方法并不会影响线程的中断状态； 原理介绍：使用interrupt来通知，而不是强制 解读：想要停止线程，其实是如何正确的用interrupt通知那个线程，以及被停止的线程如何配合。 使用interrupt 普通情况下停止线程 123456789101112131415161718192021public class RightWayStopThreadWithoutSleep implements Runnable { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(new RightWayStopThreadWithoutSleep()); thread.start(); Thread.sleep(1000); thread.interrupt(); } @Override public void run() { int num = 0; while (!Thread.currentThread().isInterrupted() &amp;&amp; num &lt;= Integer.MAX_VALUE / 2) { if (num % 10000 == 0) { System.out.println(num + &quot;是10000的倍数&quot;); } num++; } System.out.println(&quot;任务运行结束了&quot;); }} 线程可能被阻塞的情况下被停止 12345678910111213141516171819202122232425262728293031public class RightWayStopThreadWithSleep { public static void main(String[] args) throws InterruptedException { Runnable runnable = () -&gt; { int num = 0; try { while (num &lt; 300 &amp;&amp; !Thread.currentThread().isInterrupted()) { if (num % 100 == 0) { System.out.println(num + &quot;是100的倍数&quot;); } num++; } Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } }; Thread thread = new Thread(runnable); thread.start(); Thread.sleep(500); thread.interrupt(); }}/**0是100的倍数100是100的倍数200是100的倍数java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at threadcoreknowledge.stopthreads.RightWayStopThreadWithSleep.lambda$main$0(RightWayStopThreadWithSleep.java:21) at java.lang.Thread.run(Thread.java:748)*/ 如果线程在每次迭代后被阻塞 如果在执行过程中，每次循环都会调用sleep或wait等方法，那么不需要每次迭代都检查是否已中断。 1234567891011121314151617181920212223public class RightWayStopThreadWithSleepEveryLoop { public static void main(String[] args) throws InterruptedException { Runnable runnable = () -&gt; { int num = 0; try { // 不需要做是否中断的检测 while (num &lt; 10000) { if (num % 100 == 0) { System.out.println(num + &quot;是100的倍数&quot;); } num++; Thread.sleep(10); } } catch (InterruptedException e) { e.printStackTrace(); } }; Thread thread = new Thread(runnable); thread.start(); Thread.sleep(5000); thread.interrupt(); }} 如果while里面放try/catch，会导致中断失效 因为sleep会清除中断信号，将中断标记位设置成 false（源码分析待补全） 12345678910111213141516171819202122public class CantInterrupt { public static void main(String[] args) throws InterruptedException { Runnable runnable = () -&gt; { int num = 0; while (num &lt; 10000 &amp;&amp; !Thread.currentThread().isInterrupted()) { if (num % 100 == 0) { System.out.println(num + &quot;是100的倍数&quot;); } num++; try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } } }; Thread thread = new Thread(runnable); thread.start(); Thread.sleep(5000); thread.interrupt(); }} 实际开发中的两种最佳实践 不应屏蔽中断 优先选择：传递中断 catch了InterruptException之后的优先选择：在方法签名中抛出异常，那么在run()就会强制try/catch 1234567891011121314151617181920212223242526public class RightWaySropThreadInProd1 implements Runnable { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(new RightWaySropThreadInProd1()); thread.start(); Thread.sleep(1000); thread.interrupt(); } @Override public void run() { while (true) { System.out.println(&quot;go&quot;); try { throwInMethod(); } catch (InterruptedException e) { // 保存日志，停止程序 System.out.println(&quot;保存日志&quot;); e.printStackTrace(); } } } private void throwInMethod() throws InterruptedException { Thread.sleep(1000); }} 不想或无法传递：恢复中断 在catch子语句中调用Thread.currentThread().interrupt()来恢复设置中断状态，以便在后续的执行中，依然能够检查到刚才发生了中断。（使得其他代码有办法知道它发生了中断） 12345678910111213141516171819202122232425262728public class RightWaySropThreadInProd2 implements Runnable { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(new RightWaySropThreadInProd2()); thread.start(); Thread.sleep(1000); thread.interrupt(); } @Override public void run() { while (true) { if (Thread.currentThread().isInterrupted()) { System.out.println(&quot;程序运行结束&quot;); break; } reInterrupt(); } } private void reInterrupt() { try { Thread.sleep(1000); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); } }} 响应中断的方法总结列表 Object.wait()/wait(long)/wait(long, int) Thread.sleep(long)/sleep(long, int) Thread.join()/join(long)/join(long, int) java.util.cocurrent.BlockingQueue.take()/put(E) java.util.cocurrent.locks.Lock.lockInterruptibly() java.util.cocurrent.CountDownLatch.await() java.util.cocurrent.CyclicBarrier.await() java.util.cocurrent.Exchanger.exchange(V) java.nio.channels.InterruptibleChannel相关方法 java.nio.channels.Selector相关方法 错误的停止方法 被弃用的stop，suspend，resume方法（待补全） stop() 弃用原因： 原因1：即刻停止run()方法中剩余的全部工作，包括在catch或finally语句中，并抛出ThreadDeath异常(通常情况下此异常不需要显示的捕获)，因此可能会导致一些清理性的工作的得不到完成，如文件，数据库等的关闭； 原因2：会立即释放该线程所持有的所有的锁，导致数据得不到同步的处理，出现数据不一致的问题； suspend() resume()弃用原因 suspend()和resume()必须要成对出现，否则非常容易发生死锁； 不推荐使用suspend()去挂起线程的原因，是因为suspend()在导致线程暂停的同时，并不会去释放任何锁资源。其他线程都无法访问被它占用的锁。直到对应的线程执行resume()方法后，被挂起的线程才能继续，从而其它被阻塞在这个锁的线程才可以继续执行。 如果一个线程在resume目标线程之前尝试持有这个重要的系统资源锁再去resume目标线程，这两条线程就相互死锁了，也就冻结线程。 用volatile设置boolean标记位 原因：如果线程发生阻塞了，它将无法执行判断标识位的代码，阻塞无法收到停止线程的通知。线程依然不能停止。 停止线程相关重要函数解析（待补全） interrupt方法 1234567891011121314public void interrupt() { if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) { Interruptible b = blocker; if (b != null) { interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; } } interrupt0();} 判断是否已被中断相关方法 static boolean interrupted() boolean isInterrupted() Thread.interrupted()的目的对象 线程的各个属性 线程的异常 代码举例： 123456789101112131415public class MyUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler { private String name; public MyUncaughtExceptionHandler(String name) { this.name = name; } @Override public void uncaughtException(Thread t, Throwable e) { Logger logger = Logger.getAnonymousLogger(); logger.log(Level.WARNING, &quot;线程异常，终止啦&quot; + t.getName()); System.out.println(name + &quot;捕获了异常&quot; + t.getName() + &quot;异常&quot;); }} 123456789101112131415161718public class UseOwnUncaughtExceptionHandler implements Runnable { public static void main(String[] args) throws InterruptedException { Thread.setDefaultUncaughtExceptionHandler(new MyUncaughtExceptionHandler(&quot;捕获器1&quot;)); new Thread(new UseOwnUncaughtExceptionHandler(), &quot;MyThread-1&quot;).start(); Thread.sleep(300); new Thread(new UseOwnUncaughtExceptionHandler(), &quot;MyThread-2&quot;).start(); Thread.sleep(300); new Thread(new UseOwnUncaughtExceptionHandler(), &quot;MyThread-3&quot;).start(); Thread.sleep(300); new Thread(new UseOwnUncaughtExceptionHandler(), &quot;MyThread-4&quot;).start(); } @Override public void run() { throw new RuntimeException(); }} 输出： 1234567891011121314七月 11, 2020 4:02:09 下午 threadcoreknowledge.uncaughtexception.MyUncaughtExceptionHandler uncaughtException警告: 线程异常，终止啦MyThread-1捕获器1捕获了异常MyThread-1异常七月 11, 2020 4:02:09 下午 threadcoreknowledge.uncaughtexception.MyUncaughtExceptionHandler uncaughtException警告: 线程异常，终止啦MyThread-2捕获器1捕获了异常MyThread-2异常七月 11, 2020 4:02:09 下午 threadcoreknowledge.uncaughtexception.MyUncaughtExceptionHandler uncaughtException警告: 线程异常，终止啦MyThread-3捕获器1捕获了异常MyThread-3异常七月 11, 2020 4:02:10 下午 threadcoreknowledge.uncaughtexception.MyUncaughtExceptionHandler uncaughtException警告: 线程异常，终止啦MyThread-4捕获器1捕获了异常MyThread-4异常Process finished with exit code 0 Thread及Object中线程相关的重要方法 类 方法名 简介 Thread sleep相关 相关，指重写的方法 join 等待其他线程执行完毕 yield相关 放弃以获取到的CPU资源 currentThread 获取当前执行线程的引用 start，run相关 启动线程相关 interrupt相关 中断线程 stop()，suspend()，resume()相关 已废弃 Object wait/notify/notifyAll相关 让线程暂时休眠或唤醒 currentThread()：静态方法，返回对当前正在执行的线程对象的引用；start()：开始执行线程的方法，java虚拟机会调用线程内的run()方法；yield()：yield在英语里有放弃的意思，同样，这里的yield()指的是当前线程愿意让出对当前处理器的占用。这里需要注意的是，就算当前线程调用了yield()方法，程序在调度的时候，也还有可能继续运行这个线程的；sleep()：静态方法，使当前线程睡眠一段时间；join()：使当前线程等待另一个线程执行完毕之后再继续执行，内部调用的是Object类的wait方法实现的；","link":"/2020/07/11/basicconcurrency/"},{"title":"spring_ioc","text":"IOC准备： 1234567891011121314@Testpublic void doTest() { // 传入配置类 AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfig.class); // 根据name获取bean Person person = (Person) annotationConfigApplicationContext.getBean(&quot;person&quot;); // 根据class获取bean String[] beanNamesForType = annotationConfigApplicationContext.getBeanNamesForType(Person.class); // 可获得容器中所有组件的名称 String[] beanDefinitionNames = annotationConfigApplicationContext.getBeanDefinitionNames(); for (String name : beanDefinitionNames) { System.out.println(name); }} @Bean 默认情况下bean的名称和方法名称相同，但也可以使用name属性来指定。 1234567891011121314151617181920@Configuration@ComponentScan(value = &quot;com.example.springlearn&quot; )public class MainConfig { @Bean(&quot;person1&quot;) public Person person() { return new Person(&quot;jony&quot;, 19); }}@Testpublic void doTest() { AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfig.class); String[] beanDefinitionNames = annotationConfigApplicationContext.getBeanDefinitionNames(); for (String name : beanDefinitionNames) { System.out.println(name); }}// 输出:// ... person1 @ComponentScan value:指定要扫描的包 excludeFilters = Filter[] 指定扫描的时候按照什么规则排除哪些组件 includeFilters = Filter[] 指定扫描的时候只需要包含哪些组件 FilterType.ANNOTATION 按照注解 FilterType.ASSIGNABLE_TYPE 按照给定的类型 FilterType.ASPECTJ 使用ASPECTJ表达式 FilterType.REGEX 使用正则指定 FilterType.CUSTOM 使用自定义规则 useDefaultFilters 默认为true，加载所有组件，即使用默认的 Filter 进行包扫描，默认的 Filter 对标有 @Service,@Controller和@Repository 的注解的类进行扫描 示例：自定义过滤规则需要实现TypeFilter接口 12345678910111213141516171819202122232425262728293031323334353637@Configuration@ComponentScans(value = {@ComponentScan(value = &quot;com.example.springlearn&quot;, includeFilters = {@ComponentScan.Filter(type = FilterType.CUSTOM, classes = {MyTypeFilter.class})}, useDefaultFilters = false)})public class MainConfig { @Bean(&quot;person1&quot;) public Person person() { return new Person(&quot;jony&quot;, 19); }}public class MyTypeFilter implements TypeFilter { /** * @param metadataReader 读取到当前正在扫描的类信息 * @param metadataReaderFactory 读取到其他的类信息 * @return boolean */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException { // 获取当前类注解信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); // 获取当前正在扫描到类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); // 获取当前类资源信息 (类的路径) Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(&quot;===&gt; &quot; + className); if (className.contains(&quot;service&quot;)) { return true; } return false; }}// 输出:// ... personService// person1 @Scope singleton：默认，容器启动时就会创建对象，放到IoC容器中； prototype：容器启动并不会去调用方法创建对象，而是在每次获取时 才会创建对象。 123456@Scope(&quot;prototype&quot;)@Beanpublic Person person() { System.out.println(&quot;向容器中添加person...&quot;); return new Person(&quot;Tom&quot;, 19);} @Lazy 针对单实例Bean，容器启动时不创建对象，第一次使用（获取）Bean创建对象，并初始化。 @Conditional 按照一定的条件进行判断，满足条件给容器中注册Bean，可用在方法或类上 示例： 1234567891011@Conditional({WindowsCon.class})@Bean(&quot;Windows&quot;)public Person personCon() { return new Person(&quot;windows&quot;, 30);}@Conditional({LinuxCon.class})@Bean(&quot;Linux&quot;)public Person personCon2() { return new Person(&quot;linux&quot;, 35);} 123456789101112131415161718192021222324252627public class LinuxCon implements Condition { /** * @param conditionContext 判断条件能使用的上下文(环境) * @param annotatedTypeMetadata 注释信息 * @return boolean */ @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { // 获取IOC使用的BeanFactory ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory(); // 获取类加载器 ClassLoader classLoader = conditionContext.getClassLoader(); // 获取当前环境信息 Environment environment = conditionContext.getEnvironment(); // 获取bean定义的注册类 BeanDefinitionRegistry registry = conditionContext.getRegistry(); // 可以判断容器中bean注册情况, 也可以给容器中注册bean boolean definitionRes = registry.containsBeanDefinition(&quot;person&quot;); String property = environment.getProperty(&quot;os.name&quot;); if (property.contains(&quot;Windows&quot;)) { return false; } return true; }} 123456789public class WindowsCon implements Condition { @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { // 获取当前环境信息 Environment environment = conditionContext.getEnvironment(); String property = environment.getProperty(&quot;os.name&quot;); return property.contains(&quot;Windows&quot;); }} @Import @Import(要导入到容器中到组件)，容器中就会自动注册这个组件，id默认是全类名 示例：新建一个Cat类 123456789101112131415161718@Configuration@Import(Cat.class)public class MainConfig2 { ...}public class MainApplication2 { public static void main(String[] args) { AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); String[] beanDefinitionNames = annotationConfigApplicationContext.getBeanDefinitionNames(); for (String name : beanDefinitionNames) { System.out.println(name); } }}// 输出:// ... com.example.springlearn.bean.Cat ImportSelector：返回需要导入的组件的全数组 示例：创建一个Dog类 12345678910public class MyImportSelector implements ImportSelector { /** * @param annotationMetadata 当前标注@Import注解的类的所有注解信息 * @return java.lang.String[] */ @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { return new String[]{&quot;com.example.springlearn.bean.Dog&quot;}; }} ImportBeanDefinitionRegistrar：手动注册Bean 示例：创建一个Rabbit类 123456789101112131415161718public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { /** * @param importingClassMetadata * @param registry BeanDefinition注册类, 把所有需要添加到容器中的bean 调用 registry.registerBeanDefinition 手动注册进来 * @param importBeanNameGenerator * @Description // 当前类的注解信息 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry, BeanNameGenerator importBeanNameGenerator) { boolean rabbit = registry.containsBeanDefinition(&quot;rabbit&quot;); if (!rabbit) { // 指定bean定义信息 (bean的类型 作用域等...) RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Rabbit.class); // 注册一个bean 指定bean名 registry.registerBeanDefinition(&quot;rrrabbit&quot;, rootBeanDefinition); } }} 123456789@Configuration@Import({Cat.class, MyImportSelector.class, MyImportBeanDefinitionRegistrar.class})public class MainConfig2 { ...}// 输出:// ... com.example.springlearn.bean.Cat// com.example.springlearn.bean.Dog// rrrabbit FactoryBean 示例： 12345678910111213141516171819202122public class CatFactoryBean implements FactoryBean&lt;Cat&gt; { /** * @Description // 返回一个Cat对象, 这个对象会添加到容器中 * @return com.example.springlearn.bean.Cat */ @Override public Cat getObject() throws Exception { System.out.println(&quot;Cat getObject()&quot;); return new Cat(); } @Override public Class&lt;?&gt; getObjectType() { return Cat.class; } @Override public boolean isSingleton() { // true 单例, false 多实例 return true; }} 123456789@Configuration@Import({Cat.class, MyImportSelector.class, MyImportBeanDefinitionRegistrar.class})public class MainConfig2 { @Bean public CatFactoryBean catFactoryBean() { return new CatFactoryBean(); } ...} 123456789101112131415@Testpublic void doTest() { AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); // 工厂bean获取的是调用getObject()创建的对象 Object catFactoryBean = annotationConfigApplicationContext.getBean(&quot;catFactoryBean&quot;); System.out.println(&quot;bean的类型: &quot; + catFactoryBean.getClass()); Object factoryBean = annotationConfigApplicationContext.getBean(&quot;&amp;catFactoryBean&quot;); System.out.println(&quot;bean的类型: &quot; + factoryBean.getClass());}// 输出:// ... Cat getObject()// bean的类型: class com.example.springlearn.bean.Cat// bean的类型: class com.example.springlearn.bean.CatFactoryBean 默认获取到的是工厂bean调用getObject()创建的对象； 要获取工厂Bean本身，需要在id前加&amp;； 123456789101112/** * 容器中注册组件的方式: * 1) 包扫描+注解(@Controller, @Service, @Component, @Repository) * 2) @Bean(导入的第三方包) * 3) @Import(快速给容器中导入组件) * @Import id默认是全类名 * ImportSelector 返回需要导入的组件的全类名数组 * ImportBeanDefinitionRegistrar 手动注册bean到容器中 * 4) 使用Spring提供的FactoryBean * 默认获取到的是工厂bean调用getObject创建的对象 * 要获取工厂bean本身 需要在id前加&amp; */ 生命周期1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * @ClassName MainConfigOfLifeCycle * @Description * bean的生命周期: bean创建 -&gt; 初始化 -&gt; 销毁的过程 * 容器管理bean的生命周期: * 可以自定义初始化和销毁方法: 容器在bean进行到当前生命周期的时候 来调用自定义的初始化和销毁方法 * 构造 (对象创建) * 单实例: 在容器启动的时候创建对象 * 多实例: 在每次获取的时候创建对象 * * BeanPostProcessor.postProcessBeforeInitialization * 初始化： * 对象创建完成, 并赋值好, 调用初始化方法... * BeanPostProcessor.postProcessAfterInitialization * 销毁： * 单实例: 容器关闭的时候 * 多实例: 容器不会管理这个bean,容器不会调用销毁方法; 需要手动调用 * * 源码debug: * 遍历得到容器中所有的BeanPostProcessor: 逐个执行 beforeInitialization * 一旦返回null, 跳出for循环, 不会执行后面的 BeanPostProcessor.PostProcessorsBeforeInitialization * * BeanPostProcessor原理: * populateBean(beanName, mbd, instanceWrapper); 给bean进行属性赋值 * initializeBean(beanName, exposedObject, mbd) { * applyBeanPostProcessorsBeforeInitialization(bean, beanName); * invokeInitMethods(beanName, wrappedBean, mbd); 执行自定义初始化 * applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); * } * * 自定义的方法: * 1) 指定初始化和销毁方法: 指定init-method 和 destroy-method * 2) 通过让bean实现 InitializingBean, DisposableBean * 3) 可以使用JSR250： * @PostConstruct 在bean创建完成并且属性赋值完成: 来执行初始化方法 * @PreDestroy 在容器销毁bean之前通知我们进行清理工作 * 4) BeanPostProcessor【interface】, bean的后置处理器: * 在bean初始化前后进行一些处理工作: * postProcessBeforeInitialization: 在初始化之前工作 * postProcessAfterInitialization: 在初始化之后工作 * @Author yjy * @Date 2020/10/1 16:35 * @Vertion 1.0 **/ 指定初始化和销毁方法 12345678910111213public class Car { public Car() { System.out.println(&quot;car constructor...&quot;); } public void initCar() { System.out.println(&quot;car init...&quot;); } public void destroyCar() { System.out.println(&quot;car destroy...&quot;); }} 123456789@Configuration@ComponentScan(&quot;com.example.springlearn.bean&quot;)public class MainConfigOfLifeCycle { @Bean(initMethod = &quot;initCar&quot;, destroyMethod = &quot;destroyCar&quot;) public Car car() { return new Car(); }} 1234567891011@SpringBootTestpublic class TestLifeCycle { @Test public void doTest() { AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfigOfLifeCycle.class); System.out.println(&quot;ioc container finished&quot;); // 关闭容器 annotationConfigApplicationContext.close(); }} singleton: 容器关闭后执行destroyMethod方法； prototype: 获取的时候才会初始化，容器关闭后不会执行destroyMethod方法，需手动调用； 实现接口 InitializingBean和DisposableBean 123456789101112@Componentpublic class Bus implements InitializingBean, DisposableBean { @Override public void destroy() throws Exception { System.out.println(&quot;Bus destroy...&quot;); } @Override public void afterPropertiesSet() throws Exception { System.out.println(&quot;Bus afterPropertiesSet...&quot;); }} @PostConstruct和@PreDestroy 123456789101112131415161718@Componentpublic class Bicycle { public Bicycle() { System.out.println(&quot;Bicycle constructor...&quot;); } // 对象创建并赋值后调用 @PostConstruct public void init() { System.out.println(&quot;Bicycle PostConstruct...&quot;); } // 容器移除对象前调用 @PreDestroy public void destroy() { System.out.println(&quot;Bicycle PreDestroy...&quot;); }} 自定义BeanPostProcessor 1234567891011121314@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;postProcessBeforeInitialization...&quot; + bean + &quot; &quot; + beanName); return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;postProcessAfterInitialization...&quot; + bean + &quot; &quot; + beanName); return bean; }} BeanPostProcessor【interface】, bean的后置处理器, 在bean初始化前后进行一些处理工作: postProcessBeforeInitialization: 在初始化之前工作 postProcessAfterInitialization: 在初始化之后工作 BeanPostProcessor原理 遍历得到容器中所有的BeanPostProcessor: 逐个执行 beforeInitialization，一旦返回null，跳出for循环，不会执行后面的 BeanPostProcessor.PostProcessorsBeforeInitialization； BeanPostProcessor原理： populateBean(beanName, mbd, instanceWrapper); 给bean进行属性赋值 initializeBean(beanName, exposedObject, mbd) { ​ applyBeanPostProcessorsBeforeInitialization(bean, beanName); ​ invokeInitMethods(beanName, wrappedBean, mbd); 执行自定义初始化 ​ applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } 属性赋值 @Value @PropertySource 123456789// 使用@PropertySource读取外部配置文件中的 k/v 保存到运行的环境变量中; 加载完外部的配置文件以后使用${}取出配置文件中的值@Configuration@PropertySource(&quot;classpath:/person.properties&quot;)public class MainConfigOfPropertyValues { @Bean public Person person() { return new Person(); }} 123456789101112public class Person { private String name; /** * 使用@Value赋值: * 1) 基本数值 * 2) 可以写SpEL, #{} * 3) 可以写${} 取出配置文件【properties】中的值 (在运行环境变量里的值) */ @Value(&quot;${p.age}&quot;) private Integer age; ...} 因为是运行时变量，所以可以用applicationContext.getEnvironment 1234567891011@SpringBootTestpublic class TestPropertyValues { AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(MainConfigOfPropertyValues.class); @Test public void doTest() { ConfigurableEnvironment environment = annotationConfigApplicationContext.getEnvironment(); String property = environment.getProperty(&quot;p.age&quot;); System.out.println(property); }} 自动装配","link":"/2020/09/01/spring-ioc/"},{"title":"springframework","text":"项目导入IDEA 环境：Windows，JDK8，Gradle 6.5.1，IDEA 2020.1.3 进入 https://github.com/spring-projects/spring-framework，选择版本5.2.X，下载zip或clone到本地； 编辑项目目录下的build.gradle，全局搜索allprojects，编辑其repositories属性，配置阿里云镜像，如下： 123456repositories { maven { url 'http://maven.aliyun.com/nexus/content/groups/public/' } maven{ url 'http://maven.aliyun.com/nexus/content/repositories/jcenter'} mavenCentral() maven { url &quot;https://repo.spring.io/libs-spring-framework-build&quot; }} 进入项目根目录下，预编译spring-oxm，在命令行中执行： 1gradlew :spring-oxm:compileTestJava (需耐心等待3-5分钟左右…) 成功后打印BUILD SUCCESSFUL 导入项目至idea； 移除spring-aspects模块； 阅读reference文档 GA：General Availability，官方正式发布的稳定版本（RELEASE，Stable，Final） RC：Release Candidate，发行候选版本，基本不再加入新的功能 Alpha：内部测试版本，bug较多，功能不全 Beta：公开测试版本，比Alpha晚些，还会加功能，修bug M：Milestone，开发期发行版本，边开发边发行 单一职责原则 门面模式 facade pattern 子系统的外部与其内部的通信必须通过统一的对象进行 提供一个高层次的接口，使得子系统更易于使用 适配器模式 泛型 让数据类型变得参数化 定义泛型时，对应的数据类型是不确定的； 泛型方法被调用时，会指定具体类型； 核心目标：解决容器类型在编译时安全检查的问题 泛型类 泛型的参数不支持基本类型 泛型相关的信息不会进入到运行阶段 能否在泛型里面使用具备继承关系的类？不可以 解决办法： 使用通配符 ?，但是会使得泛型的类型检查失去意义； 给泛型加入上边界 ? extends E； 给泛型加入下边界 ? super E； 泛型接口 泛型方法 123456789101112131415161718192021222324@Datapublic class GenericClassExample&lt;T&gt; { /** * member这个成员变量的类型为T,T的类型由外部指定 */ private T member; public GenericClassExample(T member) { this.member = member; } public static &lt;E&gt; void printArray(E[] inputArray) { for (E element : inputArray) { System.out.printf(&quot;%s&quot;, element); System.out.printf(&quot; &quot;); } System.out.println(); } public T handleSomething(T target) { return target; }} 泛型方法中的泛型标识符可独立于泛型类存在的，而泛型类中其他方法受制于泛型标识符的 泛型字母的含义 E - Element 在集合中使用，因为集合中存放的是元素； T - Type Java类； K - Key 键； V - Value 值； N - Number 数值类型； Servlet原理总结 减少Servlet的数量 参照SpringMVC，仅通过DispatcherServlet进行请求派发； 拦截所有请求 解析请求 派发给对应的Controller里面的方法进行处理 简单工厂模式 定义一个工厂类，根据传入的参数值不同返回不同的实例 特点：被创建的实例具有共同的父类或接口 适用场景： 需要创建的对象较少； 客户端不关心对象的创建过程； 优点：可以对创建的对象进行“加工”，对客户端隐藏相关细节； 缺点：因创建逻辑复杂或创建对象过多而造成代码臃肿；新增、删除子类均会违反开闭原则； 开闭原则（待补全） 工厂方法模式 定义一个用于创建对象的接口，让子类决定实例化哪一个类 对类的实例化延迟到子类 优点： 遵循开闭原则 对客户端隐藏对象的创建细节 遵循单一职责 缺点： 增加子类的时候“拖家带口” 只支持同一类产品的创建 抽象工厂 提供一个创建一系列相关或相互依赖对象的接口 抽象工厂模式侧重的是同一产品族 工厂方法模式更加侧重于同一产品等级 解决了工厂模式只支持生产一种产品的弊端 新增一个产品族，只需要增加一个新的具体工厂，不需要修改代码； IOCSpring IOC容器使用了工厂模式+反射机制 反射：允许程序在运行时来进行自我检查并且对内部的成员进行操作 1）作用： 在运行时判断任意一个对象所属的类； 在运行时获取类的对象； 在运行时访问Java对象的属性、方法、构造方法等； 2）java.lang.reflect 类库里主要的类 Field：表示类中的成员变量 Method：表示类中的方法 Constructor：表示类的构造方法 Array：该类提供了动态创建数组和访问数组元素的静态方法 3）反射依赖的Class JVM中只有唯一一个和类相对应的Class对象来描述其类型信息。 4）获取Class对象的三种方式： Object -&gt; getClass() 任何数据类型（包括基本数据类型）都有一个“静态”的class属性 通过Class类的静态方法：forName(String className) （常用） 123456789101112131415public class ReflectTarget { public static void main(String[] args) throws ClassNotFoundException { ReflectTarget reflectTarget = new ReflectTarget(); // 第一种方式获取class对象 Class reflectTargetClass1 = reflectTarget.getClass(); // 第二种 Class reflectTargetClass2 = ReflectTarget.class; System.out.println(reflectTargetClass1 == reflectTargetClass2); // 第三种 Class reflectTargetClass3 = Class.forName(&quot;demo.reflect.ReflectTarget&quot;); System.out.println(reflectTargetClass2 == reflectTargetClass3); }} 5）获取并操作构造函数 1234567891011121314151617181920212223242526272829303132333435// 获取所有&quot;公有的&quot;的构造方法@CallerSensitivepublic Constructor&lt;?&gt;[] getConstructors() throws SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); return copyConstructors(privateGetDeclaredConstructors(true));}// 获取所有的构造方法(包括private public default protected)@CallerSensitivepublic Constructor&lt;?&gt;[] getDeclaredConstructors() throws SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return copyConstructors(privateGetDeclaredConstructors(false));}// 获取单个的&quot;公有的&quot;构造方法@CallerSensitivepublic Constructor&lt;T&gt; getConstructor(Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException, SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); return getConstructor0(parameterTypes, Member.PUBLIC);}// 获取某个构造方法(包括private public default protected)@CallerSensitivepublic Constructor&lt;T&gt; getDeclaredConstructor(Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException, SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return getConstructor0(parameterTypes, Member.DECLARED);}// 调用构造方法Constructor -&gt; newInstance(Object... initargs) // 暴力访问(忽略掉访问修饰符)Constructor -&gt; setAccessible(true) 6）获取并操作成员变量 12345678910111213141516171819202122232425262728293031323334353637383940// 获取所有的&quot;公有字段&quot; 包含继承字段@CallerSensitivepublic Field[] getFields() throws SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); return copyFields(privateGetPublicFields(null));}// 获取所有字段(包括private修饰的) 不包含继承的字段@CallerSensitivepublic Field[] getDeclaredFields() throws SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return copyFields(privateGetDeclaredFields(false));}// 获取某个&quot;公有的&quot;字段 包含继承字段@CallerSensitivepublic Field getField(String name) throws NoSuchFieldException, SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); Field field = getField0(name); if (field == null) { throw new NoSuchFieldException(name); } return field;}// 获取某个字段(可以是私有的) 不包括继承字段@CallerSensitivepublic Field getDeclaredField(String name) throws NoSuchFieldException, SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); Field field = searchFields(privateGetDeclaredFields(false), name); if (field == null) { throw new NoSuchFieldException(name); } return field;}// 设置字段的值 obj:要设置的字段所在的对象 value:要为字段设置的值Field -&gt; public void set(Object obj, Object value); 7）获取并操作成员方法 1234567891011121314151617181920212223242526272829303132333435363738// 获取所有的&quot;公有方法&quot; 包含了父类的方法和Object类@CallerSensitivepublic Method[] getMethods() throws SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); return copyMethods(privateGetPublicMethods());}// 获取所有的成员方法，包括私有的，不包含继承的@CallerSensitivepublic Method[] getDeclaredMethods() throws SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return copyMethods(privateGetDeclaredMethods(false));}// name:方法名 Class...:形参的Class类型对象public Method getMethod(String name, Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException, SecurityException { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), true); Method method = getMethod0(name, parameterTypes, true); if (method == null) { throw new NoSuchMethodException(getName() + &quot;.&quot; + name + argumentTypesToString(parameterTypes)); } return method;}@CallerSensitivepublic Method getDeclaredMethod(String name, Class&lt;?&gt;... parameterTypes) throws NoSuchMethodException, SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); Method method = searchMethods(privateGetDeclaredMethods(false), name, parameterTypes); if (method == null) { throw new NoSuchMethodException(getName() + &quot;.&quot; + name + argumentTypesToString(parameterTypes)); } return method;}// 调用方法 obj:要调用方法的对象 args:调用方法时传递的实参Method -&gt; public Object invoke(Object obj, Object... args); 8）反射的获取源 用XML来保存类相关的信息以供反射调用； 用注解来保存类相关的信息以供反射调用； 注解：提供一种为程序元素设置元数据的方法 元数据是添加到程序元素如方法、字段、类和包上的额外信息。 1）功能 作为特定的标记，用于告诉编译器一些信息； 编译时动态处理，如动态生成代码； 运行时动态处理，作为额外信息的载体，如获取注解信息； 2）注解分类 标准注解：Override、Deprecated、SuppressWarnings 元注解：@Retention、@Target、@Inherited、@Documented 自定义注解 3）元注解 ​ @Target：注解的作用目标 ​ @Retention：注解的生命周期 ​ @Documented：注解是否应当被包含在JavaDoc文档中 ​ @Inherited：是否允许子类继承该注解 单例模式（待总结） 使用反射时恶汉和懒汉模式都不能保证单例，所以采取枚举解决（枚举原理待补全） 需要实现的点 创建注解 提取标记对象 extractPackageClass 指定范围，获取范围内的所有类； 遍历所有类，获取被注解标记的类并加载进容器里； 实现容器 保存Class对象及其实例的载体 容器的加载（配置的管理与获取，获取指定范围内的Class对象，依据配置提取Class对象，连同实例一并存入容器） 容器的操作方式（涉及到容器的增删改查：增加、删除操作，根据Class获取对应实例，获取所有的Class和实例，通过注解来获取被注解标注的Class，通过超类获取对应的子类Class，获取容器载体保存Class的数量） 依赖注入 定义相关的注解标签 实现创建被注解标记的成员变量实例，并将其注入到成员变量里 依赖注入的使用 Spring框架有多种作用域 singleton prototype request session globalsession IOC源码学习 配置文件： 1）根据配置，生成用来描述bean的BeanDefinition，常用属性： 作用范围scope（@Scope） 懒加载lazy-init（@Lazy）：决定Bean实例是否延迟加载 首选primary（@Primary）：设置为true的bean会是优先的实现类 factory-bean和factory-method（@Configuration和@Bean） GenericBeanDefinition 2）术语补充 组件扫描：自动发现应用容器中需要创建的Bean 自动装配：自动满足Bean之间的依赖 3）ApplicationContext常用容器 传统的基于XML配置的经典容器 FileSystemXmlApplicationContext：从文件系统加载配置 ClassPathXmlApplicationContext：从classpath加载配置 XmlWebApplicationContext：用于Web应用程序的容器 4）容器的初始化：refresh() 在已启动的情况下调用refresh()可以清除缓存，重新装载配置信息。 主要功能： 容器初始化、配置解析 BeanFactoryPostProcessor和BeanPostProcessor的注册和激活 国际化配置 web内置容器的构造 模板方法模式：围绕抽象类，实现通用逻辑，定义模板结构，部分逻辑由子类实现 作用：复用代码，反向控制 模式涉及的方法种类：模板方法，具体方法，钩子方法，抽象方法 重要类：AbstractApplicationContext 5）Resource ResourceLoader 容器之间的关系 根据资源地址自动选择正确的Resource 自动识别classpath，file等资源地址前缀； 支持自动解析Ant风格带通配符的资源地址； Ant：路径匹配表达式，用来对URI进行匹配 ? 匹配任何单字符； * 匹配0或者任意数量的字符； ** 匹配0或者更多的目录； ResourceLoader：实现不同的Resource加载策略，按需返回特定类型的Resource。 6）BeanDefinitionReader 读取BeanDefinition BeanDefinitionRegistry 总结 1）容器初始化主要做的事情 场景问题 BeanFactory和factoryBean有什么联系和区别 后置处理器PostProcessor 本身也是一种需要注册到容器里的bean 其里面的方法会在特定的时机被容器调用； 实现不改变容器或者bean核心逻辑的情况下对bean进行扩展； 对bean进行包装，影响其行为、修改bean的内容等； PostProcessor种类 大类分为容器级别的后置处理器以及Bean级别的后置处理器 BeanDefinitionRegistryPostProcessor 容器级别 BeanFactoryPostProcessor 容器级别 BeanPostProcessor bean级别 defaultResourceLoader","link":"/2020/07/11/springframework/"},{"title":"JavaBasicProblem","text":"一些容易忘记的Java基础知识点：） Java基本数据类型 类型名称 关键字 占用内存 取值范围 字节型 byte 1字节 -128~127 短整型 short 2字节 -32768~32767 整型 int 4字节 -2147483648~2147483647 长整型 long 8字节 -9223372036854775808L~9223372036854775807L 单精度浮点型 float 4字节 +/-3.4E+38F（6~7 个有效位） 双精度浮点型 double 8字节 +/-1.8E+308 (15 个有效位） 字符型 char 2字节 ISO 单一字符集 布尔型 boolean 1字节 true 或 false 注：当需要将数值范围较大的数值类型赋给数值范围较小的数值类型变量时，此时可能会丢失精度，即强制类型转换。 123456public static void main(String[] args) { int a = 233; byte b = (byte) a; System.out.println(b);}// 输出：-23 原因：233的二进制为：24位0 + 11101001，而byte只有8位，于是从高位开始舍弃，最后剩下：11101001，由于二进制最高位1表示负数，0表示正数，故其十进制输出为-23。 浏览器输入URL发生了什么 浏览器查找域名的IP地址（DNS解析，查找过程：浏览器缓存，路由器缓存，DNS缓存）； 建立TCP连接，浏览器向web服务器发送一个HTTP请求； 服务器处理请求并返回HTTP报文； 浏览器解析渲染页面； 连接结束； 不论文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么IO操作还要分为字节流操作和字符流操作呢字符流是由JVM将字节转换得到的，这个过程比较耗时，并且编码类型不确定会出现乱码问题。所以IO流提供了一个直接操作字符的接口，方便对字符进行流操作。如果是音频文件、图片等媒体文件用字节流比较好，涉及到字符的用字符流比较好。 深拷贝和浅拷贝的区别浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝；深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝； Spring AOP的实现当目标对象实现接口，Spring使用JDK动态代理；没有实现接口时，使用CGLIB实现。 12345678910111213141516171819202122public class DefaultAopProxyFactory implements AopProxyFactory, Serializable { public DefaultAopProxyFactory() { } public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (!config.isOptimize() &amp;&amp; !config.isProxyTargetClass() &amp;&amp; !this.hasNoUserSuppliedProxyInterfaces(config)) { return new JdkDynamicAopProxy(config); } else { Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(&quot;TargetSource cannot determine target class: Either an interface or a target is required for proxy creation.&quot;); } else { return (AopProxy)(!targetClass.isInterface() &amp;&amp; !Proxy.isProxyClass(targetClass) ? new ObjenesisCglibAopProxy(config) : new JdkDynamicAopProxy(config)); } } } private boolean hasNoUserSuppliedProxyInterfaces(AdvisedSupport config) { Class&lt;?&gt;[] ifcs = config.getProxiedInterfaces(); return ifcs.length == 0 || ifcs.length == 1 &amp;&amp; SpringProxy.class.isAssignableFrom(ifcs[0]); }} https://segmentfault.com/a/1190000015797402 JDK动态代理：利用实现了InvocationHandler接口的拦截器加上反射机制生成一个实现代理接口的匿名类，然后通过invoke进行调用；生成类的速度很快，但运行时因为是基于反射，调用后续的类操作会很慢； CGLIB动态代理：在内存中动态生成子类对原对象进行代理（直接继承原始类），无法代理final类以及方法；底层基于ASM第三方框架，将代理对象类的class文件加载进来，通过修改其字节码生成子类来处理，所以生成类的速度慢，后续执行类的操作快； 强制使用CGLIB实现AOP： 12&lt;!-- 在spring配置文件中加入 --&gt;&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt; ArrayList和LinkedList LinkedList 实现了List接口和Deque接口的双端链表。LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有队列的特性; LinkedList不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法： 1List list=Collections.synchronizedList(new LinkedList(...)); LinkedList类中的内部私有类Node： 1234567891011private static class Node&lt;E&gt; { E item;//节点值 Node&lt;E&gt; next;//后继节点 Node&lt;E&gt; prev;//前驱节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; }} 这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。 ArrayList 实现了RandomAccess接口，可以随机访问； 实现了Cloneable接口，可以克隆； 实现了Serializable接口，可以序列化、反序列化； 实现了List接口，是List的实现类之一； 实现了Collection接口，是Java Collections Framework成员之一； 实现了Iterable接口，可以使用for-each迭代； ArrayList底层使用数组实现存储，查询效率高，增删效率低，线程不安全。如果涉及频繁的增删，可以使用LinkedList，线程安全可以使用Vector。 通过无参构造方法的方式ArrayList()初始化，则赋值底层数Object[] elementData为一个默认空数组Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}，所以数组容量为0，只有真正对数据进行添加add时，才分配默认DEFAULT_CAPACITY = 10的初始容量。 扩容 1234567891011private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);} 原数组的数据，原封不动的复制到新数组中，再把指向原数组的地址换到新数组。 12345678910public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); }} 注意，ArrayList（int initialCapacity）不会初始化数组大小，因为它基于elementData数组而不是大小。 ArrayList的遍历性能比LinkedList快很多，因为ArrayList是内存连续的，CPU的内部缓存结构会缓存连续的内存片段，可以大幅降低读取内存的性能开销，所以ArrayList遍历性能好。 LinkedHashMap和TreeMapLinkedHashMap内部维护了一个双向链表，保存了记录的插入顺序，因此可以按照插入的顺序从头部或者尾部迭代； 因为TreeMap实现了SortMap接口，将保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器。当用Iterator遍历TreeMap时，得到的记录是排过序的。TreeMap基于红黑树实现，非线程安全。 TreeMap()：构建一个空的映像树； TreeMap(Map m)：构建一个映像树，并且添加映像m中所有元素； TreeMap(Comparator c)：构建一个映像树，并且使用特定的比较器对关键字进行排序； TreeMap(SortedMap s)：构建一个映像树，添加映像树s中所有映射，并且使用与有序映像s相同的比较器排序； hashcode()和equals()==：比较的是变量(栈)内存中存放的对象的(堆)内存地址，用来判断两个对象的地址是否相同，即是否是指相同一个对象。比较的是真正意义上的指针操作。 equals()：是对两个对象的地址值进行的比较（即比较引用是否相同）。但是String 、Math、Integer、Double等这些封装类在使用equals()方法时，已经覆盖了object类的equals()方法，从而进行的是内容的比较。当然，基本类型是进行值的比较。 它的性质有： 自反性（reflexive）。对于任意不为null的引用值x，x.equals(x)一定是true。 对称性（symmetric）。对于任意不为null的引用值x和y，当且仅当x.equals(y)是true时，y.equals(x)也是true。 传递性（transitive）。对于任意不为null的引用值x、y和z，如果x.equals(y)是true，同时y.equals(z)是true，那么x.equals(z)一定是true。 一致性（consistent）。对于任意不为null的引用值x和y，如果用于equals比较的对象信息没有被修改的话，多次调用时x.equals(y)要么一致地返回true要么一致地返回false。 对于任意不为null的引用值x，x.equals(null)返回false。 对于Object类来说，equals()方法在对象上实现的是差别可能性最大的等价关系，即，对于任意非null的引用值x和y，当且仅当x和y引用的是同一个对象，该方法才会返回true。 需要注意的是当equals()方法被override时，hashCode()也要被override。按照一般hashCode()方法的实现来说，相等的对象，它们的hash code一定相等。 hashcode()：hashCode()方法给对象返回一个hash code值。 Java对于eqauls方法和hashCode方法是这样规定的： 1.如果两个对象相同，那么它们的hashCode值一定要相同； 2.如果两个对象的hashCode相同，它们并不一定相同（这里说的对象相同指的是用eqauls方法比较）。 如不按要求去做了，会发现相同的对象可以出现在Set集合中，同时，增加新元素的效率会大大下降。 3.equals()相等的两个对象，hashcode()一定相等；equals()不相等的两个对象，却并不能证明他们的hashcode()不相等。 ​ 换句话说，equals()方法不相等的两个对象，hashcode()有可能相等（我的理解是由于哈希码在生成的时候产生冲突造成的）。反过来，hashcode()不等，一定能推出equals()也不等；hashcode()相等，equals()可能相等，也可能不等。 equals()被覆盖过，则hashcode()也必须被覆盖； ConcurrentHashMap如何计算size使用Redis实现延迟消息队列zset 为什么String是final的 final关键字的好处： 提高了性能，JVM和Java应用都会缓存final变量； final变量可以安全的在多线程环境下进行共享，不需要额外的同步开销； 使用final关键字，JVM会对方法，变量及类进行优化； 只有当字符串是不可变的，字符串池才有可能实现，字符串池在运行时节约很多堆空间； 线程安全； 因为字符串不可变，创建时hashcode被缓存，不需要重新计算，使得字符串很适合作为Map的key； count(*) 和 count(1)和count(列名)区别count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL； count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL； count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计； 执行效率上： 列名为主键，count(列名)会比count(1)快 ；列名不为主键，count(1)会比count(列名)快；如果表多个列并且没有主键，则 count（1） 的执行效率优于 count(*)； 如果有主键，则 select count（主键）的执行效率是最优的 ；如果表只有一个字段，则 select count（*）最优； Springboot启动流程@SpringBootApplication包括三个注解，功能如下： @EnableAutoConfiguration：SpringBoot根据应用所声明的依赖来对Spring框架进行自动配置； @SpringBootConfiguration(内部为@Configuration)：被标注的类等于在spring的XML配置文件中(applicationContext.xml)，装配所有bean事务，提供了一个spring的上下文环境； @ComponentScan：组件扫描，可自动发现和装配Bean，默认扫描SpringApplication的run方法里的Booter.class所在的包路径下文件，所以最好将该启动类放到根包路径下； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty();//1.通过SpringFactoriesLoader查找并加载所有的SpringApplicationRunListeners，通过调用//starting()方法通知所有的SpringApplicationRunListeners：应用开始启动了 SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try {//2.创建并配置当前应用将要使用的Environment ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment);//3.打印banner Banner printedBanner = printBanner(environment);//4.根据是否是web项目，来创建不同的ApplicationContext容器 context = createApplicationContext();//5.创建一系列FailureAnalyzer exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context);//6.初始化ApplicationContext prepareContext(context, environment, listeners, applicationArguments, printedBanner);//7.调用ApplicationContext的refresh()方法,刷新容器 refreshContext(context);//8.查找当前context中是否注册有CommandLineRunner和ApplicationRunner，如果有则遍历执行它们。 afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } listeners.started(context); callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, listeners, exceptionReporters, ex); throw new IllegalStateException(ex); } listeners.running(context); return context; } 1.通过SpringFactoriesLoader查找并加载所有的SpringApplicationRunListeners，通过调用starting()方法通知所有的SpringApplicationRunListeners：应用开始启动了。（SpringApplicationRunListeners其本质上就是一个事件发布者，它在SpringBoot应用启动的不同时间点发布不同应用事件类型(ApplicationEvent)，如果有哪些事件监听者(ApplicationListener)对这些事件感兴趣，则可以接收并且处理） 看下SpringApplicationRunListeners源码： 123456789101112131415161718public interface SpringApplicationRunListener { // 运行run方法时立即调用此方法，可以用户非常早期的初始化工作 void starting(); // Environment准备好后，并且ApplicationContext创建之前调用 void environmentPrepared(ConfigurableEnvironment environment); // ApplicationContext创建好后立即调用 void contextPrepared(ConfigurableApplicationContext context); // ApplicationContext加载完成，在refresh之前调用 void contextLoaded(ConfigurableApplicationContext context); // 当run方法结束之前调用 void finished(ConfigurableApplicationContext context, Throwable exception);} SpringApplicationRunListener只有一个实现类：EventPublishingRunListener。①处的代码只会获取到一个EventPublishingRunListener的实例，我们来看看starting()方法的内容： 1234public void starting() { // 发布一个ApplicationStartedEvent this.initialMulticaster.multicastEvent(new ApplicationStartedEvent(this.application, this.args));} 2.创建并配置当前应用将要使用的Environment，Environment用于描述应用程序当前的运行环境，其抽象了两个方面的内容：配置文件(profile)和属性(properties)，不同的环境(eg：生产环境、预发布环境)可以使用不同的配置文件，而属性则可以从配置文件、环境变量、命令行参数等来源获取。因此，当Environment准备好后，在整个应用的任何时候，都可以从Environment中获取资源。 判断Environment是否存在，不存在就创建（如果是web项目就创建StandardServletEnvironment，否则创建StandardEnvironment） 配置Environment：配置profile以及properties 调用SpringApplicationRunListener的environmentPrepared()方法，通知事件监听者：应用的Environment已经准备好 3.打印banner（可以自定义）4.根据是否是web项目，来创建不同的ApplicationContext容器5.创建一系列FailureAnalyzer，创建流程依然是通过SpringFactoriesLoader获取到所有实现FailureAnalyzer接口的class，然后在创建对应的实例。FailureAnalyzer用于分析故障并提供相关诊断信息。 6.初始化ApplicationContext 将准备好的Environment设置给ApplicationContext 遍历调用所有的ApplicationContextInitializer的initialize()方法来对已经创建好的ApplicationContext进行进一步的处理 调用SpringApplicationRunListener的contextPrepared()方法，通知所有的监听者：ApplicationContext已经准备完毕 将所有的bean加载到容器中 调用SpringApplicationRunListener的contextLoaded()方法，通知所有的监听者：ApplicationContext已经装载完毕 7.调用ApplicationContext的refresh()方法,刷新容器 这里的刷新和spring中刷新原理类似，这里重点关注invokeBeanFactoryPostProcessors(beanFactory);方法，主要完成获取到所有的BeanFactoryPostProcessor来对容器做一些额外的操作，通过源可以进入到PostProcessorRegistrationDelegate类 的invokeBeanFactoryPostProcessors()方法，会获取类型为BeanDefinitionRegistryPostProcessor的beanorg.springframework.context.annotation.internalConfigurationAnnotationProcessor，对应的Class为ConfigurationClassPostProcessor。ConfigurationClassPostProcessor用于解析处理各种注解，包括：@Configuration、@ComponentScan、@Import、@PropertySource、@ImportResource、@Bean。当处理@import注解的时候，就会调用&lt;自动配置&gt;这一小节中的EnableAutoConfigurationImportSelector.selectImports()来完成自动配置功能 8.查找当前context中是否注册有CommandLineRunner和ApplicationRunner，如果有则遍历执行它们。 抽象类和接口接口： 因为java不支持多重继承，所以有了接口，一个类只能继承一个父类，但可以实现多个接口，接口本身也可以继承多个接口。 接口里面的成员变量默认都是public static final类型的，必须被显示的初始化；接口里面的方法默认都是public abstract类型的，隐式声明； 接口没有构造方法，不能被实例化； 类如果实现了一个接口，那么必须实现接口里面的所有抽象方法，否则类要被定义为抽象类； 抽象类： 如果将一个类声明为abstract，此类不能生成对象，只能被继承使用； 抽象方法必须存在于抽象类中，抽象类中可以有一般的变量和一般的方法； 子类继承抽象类必须实现其中抽象方法，除非子类为抽象类； private void print(){}；此语句表示方法的空实现。 abstract void print()； 此语句表示方法的抽象，无实现。 区别： 接口只能包含抽象方法，抽象类可以包含普通方法； 接口只能定义静态常量属性，抽象类既可以定义普通属性，也可以定义静态常量属性； 接口不包含构造方法，抽象类里可以包含构造方法； 抽象类中可以包含静态方法，接口中不能包含静态方法； CAP一个分布式系统不可能同时满足一致性（ C:Consistency )，可用性（ A: Availability ）和分区容错性（ P：Partition tolerance ）这三个基本需求，最多只能同时满足其中的 2 个。 如下： 选项 描述 C(Consistence) 一致性，指数据在多个副本之间能够保持一致的特性（严格的一致性）。 A(Availability) 可用性，指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据。 P(Network partitioning) 分区容错性，分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障。 分区：一个分布式系统，网络不通讯，导致连接不通，系统被分割成几个数据区域。 因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类：CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。CP - 满足一致性+分区 的系统，通常性能不是特别高。AP - 满足可用性+分区 的系统，通常可能对一致性要求低一些。 notify()和notifyAll()区别notify()只唤醒一个等待（对象的）线程并使该线程开始执行。所以如果有多个线程等待一个对象，这个方法只会唤醒其中一个线程，选择哪个线程取决于操作系统对多线程管理的实现。 notifyAll 会唤醒所有等待(对象的)线程，哪一个线程将会第一个处理取决于操作系统的实现。 如果当前情况下有多个线程需要被唤醒，推荐使用notifyAll 方法。比如在生产者-消费者里面的使用，每次都需要唤醒所有的消费者或是生产者，以判断程序是否可以继续往下执行。 sleep()和wait()区别 sleep() 是 Thread 类的静态本地方法；wait() 是Object类的成员本地方法 sleep() 方法可以在任何地方使用；wait() 方法则只能在同步方法或同步代码块中使用，否则抛出异常Exception in thread “Thread-0” java.lang.IllegalMonitorStateException sleep() 会休眠当前线程指定时间，释放 CPU 资源，不释放对象锁，休眠时间到自动苏醒继续执行；wait() 方法放弃持有的对象锁，进入等待队列，当该对象被调用 notify() / notifyAll() 方法后才有机会竞争获取对象锁，进入运行状态 JDK1.8 sleep() wait() 均需要捕获 InterruptedException 异常 Thread类中interrupt（）、interrupted（）和isInterrupted（）方法详解interrupt()：其作用是中断此线程（此线程不一定是当前线程，而是指调用该方法的Thread实例所代表的线程），但实际上只是给线程设置一个中断标志，线程仍会继续运行。 interrupted()：作用是测试当前线程是否被中断（检查中断标志），返回一个boolean并清除中断状态，第二次再调用时中断状态已经被清除，将返回一个false。 isInterrupted()：作用是只测试此线程是否被中断 ，不清除中断状态。 @Transactional实现原理OOP的理解OOP是面向对象编程，特征是封装、继承、多态、抽象。封装：是指将对象信息状态通过访问权限修饰符隐藏在对象内部，不允许外部程序直接访问，如果外部程序要访问对象内部，可以调用内部提供的get或set方法。简单来说，封装就是要找出某一类事务的公性然后提取出来。继承：子类继承了父类所有的成员方法和属性，并且可以拥有自己特性。继承解决了代码的重用问题；多态：多态存在的三个条件1.继承2.重写3.父类引用指向子类对象多态的实现方式：接口实现，继承父类方法重写，同一个类中进行重载；重载：多个同名函数同时存在，具有不同的参数个数/类型，返回值类型可以相同可以不同，调用方法时通过传递给它们的不同参数个数和参数类型来决定具体使用哪个方法, 这就是多态性，存在于父类和子类、同类中；重写： ​ 1.参数列表必须完全与被重写的方法相同；​ 2.返回的类型必须一直与被重写的方法的返回类型相同；​ 3.访问修饰符的限制一定要大于被重写方法的访问修饰符；​ 4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常；​ 5.存在于父类和子类之间，方法被定义为final不能被重写；抽象：如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。因为抽象类中含有无具体实现的方法，所以不能用抽象类创建对象。抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public。 集合的基类子类继承父类的初始化顺序 父类的静态成员变量 父类的静态代码块 子类的静态成员变量 子类的静态代码块 父类的成员变量 父类的代码块 父类的构造函数 子类的成员变量 子类的代码块 子类的构造函数 HTTP和TCP的区别和联系1、TCP连接 手机能够使用联网功能是因为手机底层实现了TCP/IP协议，可以使手机终端通过无线网络建立TCP连接。TCP协议可以对上层网络提供接口，使上层网络数据的传输建立在“无差别”的网络之上。 建立起一个TCP连接需要经过“三次握手”： 第一次握手：客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认； 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连 接之前，TCP 连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手”（过程就不细写 了，就是服务器和客户端交互，最终确定断开）2、HTTP连接 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。 1）在HTTP 1.0中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 2）在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。通常的 做法是即时不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道客 户端“在线”。若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 二者的区别和联系： TCP是底层通讯协议，定义的是数据传输和连接方式的规范； HTTP是应用层协议，定义的是传输数据的内容的规范； TCP是传输层，而http是应用层； HTTP协议中的数据是利用TCP协议传输的，所以支持HTTP也就一定支持TCP； 二叉树 平衡二叉树 红黑树的区别二叉查找/搜索/排序树 BST (binary search/sort tree)（1）若它的左子树不空，则左子树上所有结点的值均小于它的根节点的值；（2）若它的右子树上所有结点的值均大于它的根节点的值；（3）它的左、右子树也分别为二叉排序树。 注意：对二叉查找树进行中序遍历，得到有序集合。 平衡二叉树（Self-balancing binary search tree） 自平衡二叉查找树，又被称为AVL树：它是一 棵空树或它的左右两个子树的高度差(平衡因子)的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树，同时，平衡二叉树必定是二叉搜索树，反之则不一定。 红黑树 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种平衡二叉树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性:（1）每个节点或者是黑色，或者是红色。（2）根节点是黑色。（3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！]（4）如果一个节点是红色的，则它的子节点必须是黑色的。（5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 对象相等与指向他们的引用相等 的区别对象的相等，比的是内存中存放的内容是否相等； 引用相等，比较的是他们指向的内存地址是否相等； 子类构造方法之前会先调用父类无参构造方法在子类构造器的第一行会隐式的调用 super()，即调用父类的构造器，如果父类中没有定义空参的构造器，则必须在子类的构造器的第一行显示的调用super(参数)，以调用父类中构造器； 帮助子类做初始化工作 死锁四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用； 请求与保持条件 不剥夺条件 循环等待条件 Servlet生命周期Servlet的生命周期一般可以用三个方法来表示： init()：仅执行一次，负责在装载Servlet时初始化Servlet对象 service() ：核心方法，一般HttpServlet中会有get,post两种处理方式。在调用doGet和doPost方法时会构造servletRequest和servletResponse请求和响应对象作为参数。 destory()：在停止并且卸载Servlet时执行，负责释放资源 初始化阶段：Servlet启动，会读取配置文件中的信息，构造指定的Servlet对象，创建ServletConfig对象，将ServletConfig作为参数来调用init()方法。 接口与抽象类区别A.抽象类可以有非抽象的方法，而接口中的方法都是抽象方法 B.java中类只能单继承，接口可以‘继承’多个接口 C.抽象类必须有构造方法，接口一定没有构造方法 D.实例化一般指new一个对象，所以抽象类不能实例化 关于抽象类 JDK 1.8以前，抽象类的方法默认访问权限为protected JDK 1.8时，抽象类的方法默认访问权限变为default 关于接口 JDK 1.8以前，接口中的方法必须是public的 JDK 1.8时，接口中的方法可以是public的，也可以是default的 JDK 1.9时，接口中的方法可以是private的 CountDownLatch 和 CyclicBarrierCountDownLatch 是等待一组线程执行完，才执行后面的代码。此时这组线程已经执行完。CyclicBarrier 是等待一组线程至某个状态后再同时全部继续执行线程。此时这组线程还未执行完。 CountDownLatch 允许一个线程或多个线程等待特定情况，同步完成线程中其他任务。举例：百米赛跑，就绪运动员等待发令枪发动才能起步。CyclicBarrier 和CountDownLatch一样都可以协同多个线程，让指定数量的线程等待期他所有的线程都满足某些条件之后才继续执行。举例：排队上摩天轮时，每到齐四个人，就可以上同一个车厢。 如果finally块中有return语句的话，它将覆盖掉函数中其他return语句。 interface中的方法默认为public abstract 的 ，变量默认为public static final 成员变量有初始值，而局部变量没有初始值得。变量没有初始值就使用了，编译通不过； 实际上这道题考查的是两同两小一大原则： 方法名相同，参数类型相同 子类返回类型小于等于父类方法返回类型，子类抛出异常小于等于父类方法抛出异常，子类访问权限大于等于父类方法访问权限。 Socket套接字 就是源Ip地址，目标IP地址，源端口号和目标端口号的组合 服务器端：ServerSocket提供的实例 ServerSocket server= new ServerSocket(端口号) 客户端：Socket提供的实例 Socket soc=new Socket(ip地址，端口号) 异常异常是指程序运行时（非编译）所发生的非正常情况或错误，当程序违反了语音规则，jvm就会将出现的错误表示一个异常抛出。 异常也是java 的对象，定义了基类 java。lang。throwable作为异常父类。 这些异常类又包括error和exception。两大类 error类异常主要是运行时逻辑错误导致，一个正确程序中是不应该出现error的。当出现error一般jvm会终止。 exception表示可恢复异常，包括检查异常和运行时异常。 检查异常是最常见异常比如 io异常sql异常，都发生在编译阶段。这类通过try、catch捕捉 而运行时异常，编译器没有强制对其进行捕捉和处理。一般都会把异常向上抛出，直到遇到处理代码位置，若没有处理块就会抛到最上层，多线程用thread。run（）抛出，单线程用main（）抛出。常见的运行异常包括 空指针异常 类型转换异常 数组月结异常 数组存储异常 缓冲区溢出异常 算术异常等。 粉红色的是受检查的异常(checked exceptions),其必须被 try{}catch语句块所捕获,或者在方法签名里通过throws子句声明.受检查的异常必须在编译时被捕捉处理,命名为 Checked Exception 是因为Java编译器要进行检查,Java虚拟机也要进行检查,以确保这个规则得到遵守. 绿色的异常是运行时异常(runtime exceptions),需要程序员自己分析代码决定是否捕获和处理,比如 空指针,被0除… 而声明为Error的，则属于严重错误，如系统崩溃、虚拟机错误、动态链接失败等，这些错误无法恢复或者不可能捕捉，将导致应用程序中断，Error不需要捕捉。 Java中的位运算符： &gt;&gt;表示右移，如果该数为正，则高位补0，若为负数，则高位补1； &gt;&gt;&gt;表示无符号右移，也叫逻辑右移，即若该数为正，则高位补0，而若该数为负数，则右移后高位同样补0。 内部类(1)把类定义在另一个类的内部，该类就被称为内部类，举例：把类B定义在类A中，类B就被称为内部类。(2)内部类的访问规则可以直接访问外部类的成员，包括私有；外部类要想访问内部类成员，必须创建对象；(3)内部类的分类成员内部类；局部内部类；匿名内部类 (4)成员内部类访问规则成员内部类不是静态的：外部类名.内部类名 对象名 = new 外部类名().new 内部类名();成员内部类是静态的： 外部类名.内部类名 对象名 = new 外部类名.内部类名(); (5)局部内部类局部内部类访问局部变量必须加final修饰，因为局部变量使用完毕就消失，而堆内存的数据并不会立即消失，所以，堆内存还是用该变量，而改变量已经没有了，为了让该值还存在，就加final修饰。通过反编译工具我们看到了，加入final后，堆内存直接存储的是值，而不是变量名。(6)匿名内部类(掌握)是局部内部类的简化形式前提：存在一个类或者接口格式:new 类名或者接口名() {重写方法;}本质：其实是继承该类或者实现接口的子类匿名对象 对于外部类来说，只有两种修饰，public和默认（default），因为外部类放在包中，只有两种可能，包可见和包不可见。 对于内部类来说，可以有所有的修饰，因为内部类放在外部类中，与成员变量的地位一致，所以有四种可能。 Math类Math类中提供了三个与取整有关的方法：ceil,floor,round,这些方法的作用于它们的英文名称的含义相对应，例如： ceil的英文意义是天花板，该方法就表示向上取整，Math.ceil（11.3）的结果为12，Math.ceil(-11.6)的结果为-11； floor的英文是地板，该方法就表示向下取整，Math.floor(11.6)的结果是11，Math.floor(-11.4)的结果-12； 最难掌握的是round方法，他表示“四舍五入”，算法为Math.floor(x+0.5),即将原来的数字加上0.5后再向下取整，所以，Math.round(11.5)的结果是12，Math.round(-11.5)的结果为-11. 枚举类枚举类 所有的枚举值都是类静态常量，在初始化时会对所有的枚举值对象进行第一次初始化。 重载就是一句话：同名不同参，返回值无关。 覆盖/重写：同名同参 hibernate1、什么是延迟加载？定义：延迟加载(lazy load)是Hibernate3 关联关系对象默认的加载方式，延迟加载机制是为了避免一些无谓的性能开销而提出来的。就是只有当真正需要数据时，才真正的执行数据加载操作。延迟加载是 hibernate 中用于提高查询效率的一种措施，它的对立面是 立即加载。2、如何实现延迟加载？Hibernate 2 实现延迟加载有 2 种方式：实体对象集合Hibernate 3 又引入了一种新的加载方式：属性的延迟加载一般使用load()的方法来实现延迟加载：当调用load方法加载对象时，返回***对象，等到真正用到对象的内容时才发出sql语句3、Hibernate 其他介绍Hibernate 使用 Java 反射机制，而不是字节码增强程序来实现透明性Hibernate 的性能非常好，因为它是个轻量级框架。映射的灵活性很出色。它支持各种关系数据库，从一对一到多对多的各种复杂关系。4、优化 Hibernate 所鼓励的 7 大措施 1.尽量使用多对一，避免使用单项一对多 2.灵活使用单向一对多 3.不用一对一，使用多对一代替一对一 4.配置对象缓存，不使用集合缓存 5.一对多使用Bag ，多对一使用Set 6.继承使用显示多态 HQL:from object polymorphism=”exlicit” 避免查处所有对象 7.消除大表，使用二级缓存","link":"/2020/04/24/JavaBasicProblem/"},{"title":"Redis","text":"Redis为什么这么快 完全基于内存，绝大部分请求是纯粹的内存操作； 数据结构简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件； 使用多路I/O复用模型，非阻塞IO； Redis构建了自己的VM机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； Redis和Memcached的区别Redis支持复杂的数据结构，并且原生支持集群模式； MC处理请求时使用多线程异步IO的方式，合理利用CPU多核的优势，缺点是，key不能超过250个字节，value不能超过1M字节，key的最大失效时间是30天，只支持K-V结构，不提供持久化和主从同步功能； Redis的线程模型Redis内部使用文件事件处理器 file event handler，它采用IO多路复用机制同时监听多个Socket，根据Socket上的事件来选择对应的事件处理器进行处理。 文件事件处理器由四部分组成： 多个Socket； IO多路复用程序； 文件事件分派器； 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）； 多个Socket并发产生不同的操作，每个操作对应不同的文件事件，IO多路复用程序监听多个Socket，并将其产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，将其交给对应的事件处理器进行处理。 基础五种基本的数据结构字符串String底层结构底层类似Java中的ArrayList，从源码的 sds.h/sdshdr 文件中可以看到 Redis 底层对于字符串的定义 SDS，即 Simple Dynamic String 结构。在源码中同样一组结构Redis使用泛型定义了好多次，为什么不直接使用int类型？ 因为当字符串比较短时，len和alloc可以使用byte和short来表示，故为了优化内存，不同长度的字符串使用不同的结构体来表示。 SDS与C语言中字符串的区别SDS： 123456789101112131415161718192021222324252627282930/* Note: sdshdr5 is never used, we just access the flags byte directly. * However is here to document the layout of type 5 SDS strings. */struct __attribute__ ((__packed__)) sdshdr5 { unsignedchar flags; /* 3 lsb of type, and 5 msb of string length */ char buf[];};struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsignedchar flags; /* 3 lsb of type, 5 unused bits */ char buf[];};struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsignedchar flags; /* 3 lsb of type, 5 unused bits */ char buf[];};struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsignedchar flags; /* 3 lsb of type, 5 unused bits */ char buf[];};struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsignedchar flags; /* 3 lsb of type, 5 unused bits */ char buf[];}; C语言中使用长度为N+1的字符数组表示长度为N的字符串，且字符数组最后一个元素总是’\\0’，会造成如下问题： 获取字符串长度的时间复杂度总是O(N)，因为C语言的实现中没有保存数组的长度，每次都要遍历整个数组； 在拼接或截取字符串时，若操作不当，容易发生缓冲区溢出/内存泄漏的问题，原因同上； 只能保存文本数据，因为C语言中的字符串必须符合某种编码（如ASCII） 追加字符串，Redis源码如下：(Redis规定字符串不得超过512MB) 12345678910111213141516/* Append the specified binary-safe string pointed by 't' of 'len' bytes to the * end of the specified sds string 's'. * * After the call, the passed sds string is no longer valid and all the * references must be substituted with the new pointer returned by the call. */sds sdscatlen(sds s, const void *t, size_t len) { // 获取原字符串的长度 size_t curlen = sdslen(s); // 按需调整空间，如果容量不够容纳追加的内容，就会重新分配字节数组并复制原字符串的内容到新数组中 s = sdsMakeRoomFor(s,len); if (s == NULL) returnNULL; // 内存不足 memcpy(s+curlen, t, len); // 追加目标字符串到字节数组中 sdssetlen(s, curlen+len); // 设置追加后的长度 s[curlen+len] = '\\0'; // 让字符串以 \\0 结尾，便于调试打印 return s;} 基本操作/命令设置和获取键值对（key存在时，SET命令会直接覆盖旧的值） 批量设置键值对 过期和SET命令扩展 计数：如果 value 是一个整数，还可以对它使用 INCR 命令进行 原子性 的自增操作，这意味着及时多个客户端对同一个 key 进行操作，也决不会导致竞争的情况。 12345&gt; SET counter 100&gt; INCR count(interger) 101&gt; INCRBY counter 50(integer) 151 返回原值的GETSET命令 应用 缓存功能：String字符串是最常用的数据类型，不仅仅是Redis，各个语言都是最基本类型，因此，利用Redis作为缓存，配合其它数据库作为存储层，利用Redis支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。 计数器：许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。 共享用户Session：用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。 列表list底层类似于Java中的LinkedList，故删除/插入极快O(1)，索引定位慢O(n)。 基本操作 LPUSH和RPUSH分别可以向 list 的左边（头部）和右边（尾部）添加一个新元素； LRANGE命令可以从 list 中取出一定范围的元素； LINDEX命令可以从 list 中取出指定下表的元素，相当于 Java 链表操作中的get(int index)操作； list实现队列12345678910&gt; RPUSH books python java golang(integer) 3&gt; LPOP books&quot;python&quot;&gt; LPOP books&quot;java&quot;&gt; LPOP books&quot;golang&quot;&gt; LPOP books(nil) list实现栈123456789&gt; RPUSH books python java golang&gt; RPOP books&quot;golang&quot;&gt; RPOP books&quot;java&quot;&gt; RPOP books&quot;python&quot;&gt; RPOP books(nil) 应用 消息队列：Redis的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 文章列表或者数据分页展示的应用。比如，常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用Redis的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。（lrange命令） 字典hash底层结构类似于Java中的HashMap，内部实现也类似，通过”数组 + 链表”的链地址法来解决部分哈希冲突，同时这样的结构也吸收了两种不同数据结构的优点。 12345678910111213141516171819typedefstruct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsignedlong size; // 哈希表大小掩码，用于计算索引值，总是等于 size - 1 unsignedlong sizemask; // 该哈希表已有节点的数量 unsignedlong used;} dictht;typedefstruct dict { dictType *type; void *privdata; // 内部有两个 dictht 结构 dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsignedlong iterators; /* number of iterators currently running */} dict; table 属性是一个数组，数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针，而每个 dictEntry 结构保存着一个键值对： 12345678910111213typedefstruct dictEntry { // 键 void *key; // 值 union { void *val; uint64_t u64; int64_t s64; double d; } v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;} dictEntry; 从源码中可以看到dict中包含两个dicht，通常情况下只有一个dicht是有值的，但在字典扩容缩容时需要分配新的dicht，进行渐进式搬迁（会在rehash的同时保留新旧两个hash结构，在后续的定时任务以及hash操作指令中，逐渐把旧字典的内容迁移到新字典中，当搬迁完成，使用新的hash结构取而代之。） 扩容条件条件：hash表中元素的个数等于第一维数组的长度时。 容量：扩容的新数组是原数组大小的2倍。 特殊情况：Redis正在执行bgsave时（持久化命令），Redis尽量不扩容，当hash表达到第一维数组长度5倍时，会执行强制扩容。 缩容条件条件：元素个数低于数组长度的10% 缩容不会考虑Redis是否在做bgsave。 字典的基本操作123456789101112131415&gt; HSET books java &quot;think in java&quot; # 命令行的字符串如果包含空格则需要使用引号包裹(integer) 1&gt; HSET books python &quot;python cookbook&quot;(integer) 1&gt; HGETALL books # key 和 value 间隔出现1) &quot;java&quot;2) &quot;think in java&quot;3) &quot;python&quot;4) &quot;python cookbook&quot;&gt; HGET books java&quot;think in java&quot;&gt; HSET books java &quot;head first java&quot; (integer) 0 # 因为是更新操作，所以返回 0&gt; HMSET books java &quot;effetive java&quot; python &quot;learning python&quot; # 批量操作OK 应用这个是类似Map的一种结构，一般可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在Redis里，然后每次读写缓存的时候，可以就操作Hash里的某个字段。 集合set类似于Java中的HashSet，内部的键值对是无序的，唯一的。内部实现相当于一个特殊的字典，字典中所有的value都是NULL。 基本操作123456789101112131415&gt; HSET books java &quot;think in java&quot; # 命令行的字符串如果包含空格则需要使用引号包裹(integer) 1&gt; HSET books python &quot;python cookbook&quot;(integer) 1&gt; HGETALL books # key 和 value 间隔出现1) &quot;java&quot;2) &quot;think in java&quot;3) &quot;python&quot;4) &quot;python cookbook&quot;&gt; HGET books java&quot;think in java&quot;&gt; HSET books java &quot;head first java&quot; (integer) 0 # 因为是更新操作，所以返回 0&gt; HMSET books java &quot;effetive java&quot; python &quot;learning python&quot; # 批量操作OK 有序列表zset底层实现基于跳跃表 基本操作1234567891011121314151617181920212223242526272829303132333435363738&gt; ZADD books 9.0 &quot;think in java&quot;&gt; ZADD books 8.9 &quot;java concurrency&quot;&gt; ZADD books 8.6 &quot;java cookbook&quot;&gt; ZRANGE books 0 -1 # 按 score 排序列出，参数区间为排名范围1) &quot;java cookbook&quot;2) &quot;java concurrency&quot;3) &quot;think in java&quot;&gt; ZREVRANGE books 0 -1 # 按 score 逆序列出，参数区间为排名范围1) &quot;think in java&quot;2) &quot;java concurrency&quot;3) &quot;java cookbook&quot;&gt; ZCARD books # 相当于 count()(integer) 3&gt; ZSCORE books &quot;java concurrency&quot; # 获取指定 value 的 score&quot;8.9000000000000004&quot; # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题&gt; ZRANK books &quot;java concurrency&quot; # 排名(integer) 1&gt; ZRANGEBYSCORE books 0 8.91 # 根据分值区间遍历 zset1) &quot;java cookbook&quot;2) &quot;java concurrency&quot;&gt; ZRANGEBYSCORE books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。1) &quot;java cookbook&quot;2) &quot;8.5999999999999996&quot;3) &quot;java concurrency&quot;4) &quot;8.9000000000000004&quot;&gt; ZREM books &quot;java concurrency&quot; # 删除 value(integer) 1&gt; ZRANGE books 0 -11) &quot;java cookbook&quot;2) &quot;think in java&quot; 应用有序集合的使用场景与集合类似，但是set集合不是自动有序的，而Sorted set可以利用分数进行成员间的排序，而且是插入时就排序好。所以当需要一个有序且不重复的集合列表时，就可以选择Sorted set数据结构作为选择方案。 排行榜：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。例如微博热搜榜。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 不基本的数据结构BitMap通过一个bit位来表示某个元素对应的值或者状态，key就是对应元素本身。可以将其想象成以位为单位的数组，数组的每个单元只能存储0和1，数组的下标叫做偏移量。 优势：极大的节省储存空间。 基本操作1&gt; SETBIT key offset value 返回值：指定偏移量上原来存储的bit 1&gt; GETBIT key offset 说明：获取指定偏移量上的bit，当offset比字符串值的长度大，或者key不存在时，返回0。 1&gt; BITCOUNT key start end 说明：计算给定字符串中，指定的偏移量上，被设置为1的位的数量。 应用用户签到 统计活跃用户 HyperLogLog供不精确的去重技术功能，适合做大规模数据的去重统计。 Redis ModuleBloomFilter底层基于BitMap，由一个位数组和K个哈希函数组成。 当一个元素加入布隆过滤器时，会经历如下步骤： 1）使用K个哈希函数对元素进行K次计算，得到K个哈希值； 2）在位数组中根据得到的哈希值，把对应的下标的值置为1； 优点：空间效率高，占用空间小，查询效率高； 存在的问题：当插入的元素越来越多，即位数组中被置为1的位置也越多，当一个不在布隆过滤器中的元素，经过哈希计算之后，得到的值在位数组中查询，有可能这些位置也都被置为1，即导致误判。 主要命令： bf.add添加元素到布隆过滤器中； bf.exists判断某个元素是否在过滤器中； 在redis中有两个值决定布隆过滤器的准确率： error_rate：允许布隆过滤器的错误率，这个值越低过滤器的位数组的大小越大，占用空间也就越大； initial_size：布隆过滤器可以储存的元素个数，当实际存储的元素个数超过这个值之后，过滤器的准确率会下降； redis 中有一个命令可以来设置这两个值： 1bf.reserve urls 0.01 100 第一个参数是过滤器的名字； 第二个参数是error_rate的值； 第三个参数是initial_size的值； Redis Module，如BloomFilter，RedisSearch，RedisML Redis的持久化因为Redis数据全部保存在内存中，如果突然宕机，数据就会全部丢失，所以Redis持久化机制应运而生，即将内存中的数据保存到磁盘中。 持久化的过程客户端向数据库发出写命令，数据库接收到客户端的写请求后，调用系统API将数据写入磁盘，操作系统将写缓冲区传输到磁盘控制器，然后由磁盘控制器将数据写入实际的物理媒介中。 数据流向： 客户端的内存 -&gt; 服务器的内存 -&gt; 内核缓冲区 -&gt; 磁盘缓存 -&gt; 磁盘 方式一：快照 RDB（Redis Database） 触发时机： save的规则满足的情况下； 执行flushall命令； 退出Redis； 如何恢复rdb文件： 只需要将rdb文件放到redis启动目录即可，redis启动时会自动检查dump.rdb恢复其中的数据。 原理：系统多进程Copy On Write机制 + fork函数 快照将生成一个包含整个数据集的.rdb文件。 fork创建子进程过程： 分配新的内存块和内核数据结构给子进程； 将父进程部分数据结构内容拷贝给子进程； 添加子进程到系统进程列表中； fork返回，开始调度器调度； Redis在持久化时会调用glibc的函数fork产生一个子进程（共享代码块和数据段），将快照持久化交给子进程处理，父进程则继续处理客户端请求。 父进程对内存数据结构进行修改，会对子进程造成影响么？ 不会，因为此时使用操作系统的COW机制进行数据段页面的分离，数据段是由很多操作系统的页面组成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，对这个复制的页面进行修改。这时子进程 相应的页面并没有变化，还是进程产生时的数据。 缺点：如果快照保存完成前宕机，这段时间写入Redis的最新数据将会丢失；fork进程会占用一定的内存空间；在生成数据快照时，如果文件很大，客户端可能会暂停几毫秒甚至几秒。 方式二：AOFAppend Only File，每次执行修改内存中数据集的写操作时，都会记录该操作。有灵活的同步策略，no，always，every seconds。 原理：AOF日志是以文件的形式存在的，当程序对AOF日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核会通过glibc提供的fsync(int fd)函数，异步将指定文件内容强制从内核缓存刷到磁盘。 如果aof文件有错位，这时Redis无法connect，可以使用redis-check-aof –fix修复。 缺点：相同规模的数据集，AOF大于RDB；运行效率低于RDB。 rewrite重写规则 优化Redis提供了bgrewriteaof指令用于对AOF日志进行瘦身。 原理：开辟一个子进程对内存进行遍历转换成一系列Redis的操作指令，序列化到一个新的AOF日志文件中。序列化完成后再将操作期间发生的增量AOF日志追加到这个新的AOF日志文件中，追加完毕后就立即替代旧的AOF日志文件了，瘦身工作就完成了。 方式三：混合持久化Redis4.0开始提供。将rdb文件的内容和增量的AOF日志文件存在一起。这里的 AOF日志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的增量AOF日志，通常这部分AOF日志很小。于是重启Redis时，可以先加载rdb的内容，然后再重放增量AOF日志，重启效率大幅提升。 常见场景缓存雪崩原因：缓存服务器重启或key同时失效，请求全部落到数据库。 方案： 1）在Redis中批量存数据时，将key的失效时间设为随机值，让缓存失效时间尽量均匀； 2）设置热点数据永不过期，有更新操作就手动更新缓存； 3）使用快速失败的熔断策略，减少DB瞬间压力； 缓存穿透原因：缓存和数据库中都没有的数据，用户不断发起请求。（比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。） 方案： 1）如果一个查询返回的数据为空，将这个空结果进行缓存，但过期时间需设置很短； 2）把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，会先判断用户发来的请求的值是否存在布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，从而避免了对数据库的查询压力，存在的话才会走下面的流程。 3）增加参数校验； 缓存击穿原因：缓存中没有数据但数据库中有数据（缓存时间到期） 方案： 1）设置热点数据永远不过期； 2）加上互斥锁； 1234567891011121314151617181920212223public static String getData(String Key) throws InterruptedException { // 从Redis查询数据 String result = getDataByKV(Key); // 校验 if (StringUtils.isBlank(result)) { // 获取锁 if (reenLock.trylock()) { // 去数据库查询 result = getDataByDB(Key); // 校验 if (StringUtils.isBlank(result)) { // 放入缓存 setDataToKV(Key, result); } // 释放锁 reenLock.unLock(); } else { Thread.sleep(100L); result = getData(key); } } return result;} 六种Key的淘汰策略Redis中通过maxmemory参数来设定内存的使用上限，当Redis使用内存达到设定的最大值时，会根据配置文件中的策略删除key，从而给新的键值留出空间。 目前Redis提供了6种的淘汰策略（默认的是noeviction）： volatile-lru：在设置了过期时间的键空间中，移除最近最少使用的key； allkeys-lru：移除最近最少使用的key； volatile-random，在设置了过期时间的键空间中，随机移除一个key； allkeys-random，随机移除一个key； volatile-ttl，在设置了过期时间的键空间中，优先回收存活时间（TTL）较短的键，即移除将要过期的key； noeviction：当内存使用达到阀值的时候，所有引起申请内存的命令会报错； 三种删除过期key策略 定时删除 在设置某个key的过期时间同时，创建一个定时器，让定时器在该过期时间到来时，立即执行删除操作。 缺点：在过期键比较多时，删除过期键会占用一部分CPU时间，对服务器的响应时间和吞吐量造成影响。 惰性删除 放任键过期不管，每次获取键时，都检查取得的键是否过期，若过期，删除即可。 缺点：如果多个键都已经过期，而这些键又恰好没有被访问，那么这部分的内存就都不会被释放。 定期删除 Redis会周期性的随机检查设置了过期时间的key，删除里面过期的key。 缺点：难以确定操作执行的频率。 应用Redis集群主从之间的数据同步master负责写，将数据同步给slave，slave负责读，分发掉大量请求，并且可以实现水平扩容。复制只能是单向的。 启动一台slave时，发送psync命令给master，如果这个slave是第一次连接到master，则会触发一个全量复制，即master启动一个线程生成RDB快照，并将新的写请求缓存在内存中，RDB文件生成后，master将其发送给slave，slave拿到后写进本地磁盘，然后加载进内存，之后master会把内存中缓存的新命令发给slave。 从节点可以执行写命令么？ 可以，修改redis.conf中slave-read-only 设置为no，即可执行写命令，但从节点写命令的数据，其他从节点或主节点是不能获取的。 至少需要三台，一主二从。 作用： 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式； 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复（实际上是一种服务的冗余）； 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，从节点提供读服务，分担服务器负载；（尤其在写少读多场景下，可以大大提高Redis服务器的并发量） 高可用（集群）基石：是哨兵和集群能够实施的基础； 1info replication # 查看 默认情况下 每台Redis服务器都是master，只需配置slave。使用命令SLAVEOF是暂时的，修改配置文件是永久的。 master断开，slave依旧连接到master；当master启动后，slave可以直接获取到master写的信息 复制原理 Slave启动成功连接到master后会发送一个sync同步命令，Master接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕后，master将传送整个数据文件到slave，并完成一次完全同步。 全量复制：slave服务在接收到数据库文件数据后，将其存盘并加载到内存中； 增量复制：master继续将新的所有收集到的修改命令一次传给slave，完成同步； 宕机后手动配置主机 1SLAVEOF no one 哨兵模式基于主从模式的优化：当主节点挂掉后，从节点能够自动变成主节点。 工作方式后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，因此 可以使用多个哨兵进行监控，各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 配置哨兵配置文件 sentinel.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# Example sentinel.conf # 哨兵sentinel实例运行的端口 默认26379port 26379 # 哨兵sentinel的工作目录dir /tmp # 哨兵sentinel监控的redis主节点的 ip port # master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。# quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; sentinel monitor mymaster 127.0.0.1 6379 2 # 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster MySUPER--secret-0123passw0rd # 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;sentinel down-after-milliseconds mymaster 30000 # 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;sentinel parallel-syncs mymaster 1 # 故障转移的超时时间 failover-timeout 可以用在以下这些方面： #1. 同一个sentinel对同一个master两次failover之间的间隔时间。#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。#3.当想要取消一个正在进行的failover所需要的时间。 #4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了# 默认三分钟# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;sentinel failover-timeout mymaster 180000 # SCRIPTS EXECUTION #配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。#对于脚本的运行结果有以下规则：#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。 # 通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。# 通知脚本# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; sentinel notification-script mymaster /var/redis/notify.sh # 客户端重新配置主节点参数脚本# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。# 以下参数将会在调用脚本时传给脚本:# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;# 目前&lt;state&gt;总是“failover”,# &lt;role&gt;是“leader”或者“observer”中的一个。 # 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt; sentinel client-reconfig-script mymaster /var/redis/reconfig.sh 如果master恢复，只能归并到新的master下当做slave。 三个定时任务： 1）每10秒，每个sentinel对master和slave执行info命令：用来发现slave节点；确定主从关系。 2）每2秒，每个sentinel通过master节点的channel（名称为sentinel:hello）交换信息（pub/sub）：用来交互对节点的看法以及自身信息。 3）每1秒，每个sentinel对其他sentinel和redis执行ping命令，用于心跳检测，作为节点存活的判断依据。 如果一个实例（instance）距离最后一次有效回复PING命令的时间超过down-after-milliseconds选项所指定的值， 则这个实例会被Sentinel进程标记为主观下线SDOWN； 当有足够数量的Sentinel进程（大于等于配置文件指定的值）在指定时间范围内确认Master进入了主观下线状态SDOWN， 则Master会被标记为客观下线ODOWN；（此时开启故障转移机制） 当Master被Sentinel进程标记为ODOWN后，Sentinel进程向下线的Master的所有Slave发送INFO命令的频率会从10秒一次改为1秒一次； 哨兵组件的主要功能： 集群监控：负责监控Redis master和slave进程是否正常工作； 消息通知：如果某个Redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员； 故障转移：如果 master node 挂掉了，会自动转移到slave node上； 例子： 当master出现故障，此时3个Sentinel节点共同选举了Sentinel3作为领导，负载处理主节点的故障转移： 将slave-1脱离原从节点，升级为master； 将从节点slave-2指向新的主节点； 通知客户端master已更换； 将原主节点（oldMaster）变成slave，指向新的master； 配置中心：如果故障转移发生了，通知client客户端新的master地址； 缺点：较难支持在线扩容，在集群容量达到上限时在线扩容会很复杂；每台redis存储相同数据，浪费内存； Redis-ClusterRedis3.0，实现了redis的分布式存储，即每台redis上存储不同的内容。 Redis-Cluster采用无中心结构，特点如下： 所有的节点彼此互联（PING-PONG机制），内部使用二进制协议优化传输速度和带宽； 节点的fail是通过集群中超过半数的节点检测失效时才生效； 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可； 工作方式每一个节点上，都有两个部分：插槽（slot），它的的取值范围是：0-16383；cluster，可以理解为是一个集群管理的插件。当存取的key到达时，redis会根据crc16的算法得出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，通过这个值，找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 从redis中没有slot，不会参与集群投票，仅仅作为主机的备份。 为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。 什么情况下集群不可用？ 任意主节点和它的从节点都宕机了； 集群中超过半数的master挂掉，无论是否有slave，集群都进入fail状态； KV，DB读写模式Cache Aside Pattern 读的时候，先取缓存，如果缓存中没有，就读数据库，取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后删除缓存。 为什么删除缓存，而不是更新缓存？ 可能对应的缓存数据需要综合其他数据进行计算； 假如数据在 1 分钟内修改了 20 次，或 100 次，那么缓存就更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，存在大量的冷数据； Lazy加载思想； Redis分布式锁一致性hash算法将整个哈希值空间按顺时针方向组织成一个虚拟的圆环； 根据hash函数计算出的hash值，将对象key映射到环形空间； 使用相同的hash算法将cache也映射到这个环形空间； 每个key顺时针找到的第一个cache节点就是存储位置； 优势：假如有一台服务器不可用，受影响的仅仅是从此服务器逆时针方向的前一台服务器之间的数据，其他的不会受影响； 数据倾斜：当服务节点较少时，容易因为节点分布不均匀而造成数据倾斜现象，此时可以运用一致性哈希的虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。 Redis实现消息队列Redis提供了两种方式来做消息队列，生产消费模式，和发布订阅模式。 生产消费模式 Redis数据结构的列表List提供了push和pup命令，遵循着先入先出FIFO的原则。使用push/pop方式的优点在于消息可以持久化，缺点是一条消息只能被一个消费者接收，是一种比较简陋的消息队列。 如果队列空了，消费者会陷入pop死循环，即使没有数据也不会停止，影响Redis性能，所以可以使用brpop和blpop实现阻塞读取，阻塞读在队列没有数据时会立即进入休眠状态，一旦数据到来则立即被唤醒，消息的延迟几乎为零。需要注意的是如果线程一直阻塞在那里，连接就会被服务器主动断开来减少资源占用，这时blpop/brpop会抛出异常，所以编写消费段时需要注意异常的处理。可以设置超时时间，如果列表中没有消息则一直阻塞直到超时，减小Redis的压力。 发布订阅模式 原理：通过subscribe命令订阅某频道后，redis-server里维护了一个字典，字典的键就是一个个channel，字典的值则是一个链表，链表中保存了所有订阅这个channel的客户端。subscribe命令的关键，就是将客户端添加到给定channel的订阅链表中。 Redis自带pub/sub机制即发布订阅模式，此模式中生产者producer和消费者consumer之间的关系是一对多的，也就是一条消息会被多个消费者所消费，当只有一个消费者时可视为一对一的消息队列。 发布订阅模式常见命令： psubscribe 订阅一个或多个符合给定模式的频道 publish 将消息发布到指定的频道 pubsub查看订阅与发布系统状态 pubsub channels pattern 列出当前的活跃频道 pubsub numsub channel-1 channel-n 获取给定频道的订阅者数量 pubsub numpat 获取订阅模式的数量 punsubscribe 指示客户端退订所有给定模式 subscribe 订阅给定的一个或多个频道的消息 unsubscribe 指示客户端退订给定的频道 场景：实时消息系统；实时聊天，将消息回显给所有人即可；订阅/关注系统； Redis实现延时消息队列使用zset，消息作为value，时间作为score Hot Key和Big KeyRedis脑裂注意：较新版本的redis.conf文件中的参数变成了 12min-replicas-to-write 3min-replicas-max-lag 10 按照上面的配置，要求至少3个slave节点，且数据复制和同步的延迟不能超过10秒，否则的话master就会拒绝写请求，配置了这两个参数之后，如果发生集群脑裂，原先的master节点接收到客户端的写入请求会拒绝，就可以减少数据同步之后的数据丢失。 redis中的异步复制情况下的数据丢失问题也能使用这两个参数","link":"/2020/07/15/Redis/"},{"title":"spring_refresh","text":"对Spring框架中refresh()的源码分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. // spring开始实例化单例的类 finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } }} prepareRefresh()refresh前的预处理 12345678910111213141516171819202122232425262728293031323334353637383940/** * Prepare this context for refreshing, setting its startup date and * active flag as well as performing any initialization of property sources. */protected void prepareRefresh() { // Switch to active. this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isDebugEnabled()) { if (logger.isTraceEnabled()) { logger.trace(&quot;Refreshing &quot; + this); } else { logger.debug(&quot;Refreshing &quot; + getDisplayName()); } } // Initialize any placeholder property sources in the context environment. initPropertySources(); // Validate that all properties marked as required are resolvable: // see ConfigurablePropertyResolver#setRequiredProperties getEnvironment().validateRequiredProperties(); // Store pre-refresh ApplicationListeners... if (this.earlyApplicationListeners == null) { this.earlyApplicationListeners = new LinkedHashSet&lt;&gt;(this.applicationListeners); } else { // Reset local application listeners to pre-refresh state. this.applicationListeners.clear(); this.applicationListeners.addAll(this.earlyApplicationListeners); } // Allow for the collection of early ApplicationEvents, // to be published once the multicaster is available... this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();} initPropertySources() 初始化一些属性设置；子类自定义个性化的属性设置方法； getEnvironment().validateRequireProperties() 检验属性的合法等； earlyApplicationEvents = new LinkedHashSet&lt;&gt;(); 保存容器中的一些早期事件； obtainFreshBeanFactory()1234567// Tell the subclass to refresh the internal bean factory.ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { refreshBeanFactory(); return getBeanFactory();} refreshBeanFactory()刷新(创建)BeanFactory 123456789101112131415161718public GenericApplicationContext() { this.beanFactory = new DefaultListableBeanFactory();}/** * Do nothing: We hold a single internal BeanFactory and rely on callers * to register beans through our public methods (or the BeanFactory's). * @see #registerBeanDefinition */@Overrideprotected final void refreshBeanFactory() throws IllegalStateException { if (!this.refreshed.compareAndSet(false, true)) { throw new IllegalStateException( &quot;GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once&quot;); } // 设置id this.beanFactory.setSerializationId(getId());} getBeanFactory()返回刚才GenericApplicationContext创建的BeanFactory对象； 12345678/** * Return the single internal BeanFactory held by this context * (as ConfigurableListableBeanFactory). */@Overridepublic final ConfigurableListableBeanFactory getBeanFactory() { return this.beanFactory;} prepareBeanFactory(beanFactory)BeanFactory的预准备工作（BeanFactory的一些设置） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Configure the factory's standard context characteristics, * such as the context's ClassLoader and post-processors. * @param beanFactory the BeanFactory to configure */protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { // Tell the internal bean factory to use the context's class loader etc. // 1. 设置BeanFactory的类加载器、支持表达式解析器等 beanFactory.setBeanClassLoader(getClassLoader()); beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // Configure the bean factory with context callbacks. // 2. 添加部分BeanPostProcessor【ApplicationContextAwareProcessor】 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 3. 设置忽略的自动装配的接口 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); // BeanFactory interface not registered as resolvable type in a plain factory. // MessageSource registered (and found for autowiring) as a bean. // 4. 注册可以解析的自动装配（直接在任何组件中自动注入） beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // Register early post-processor for detecting inner beans as ApplicationListeners. // 5. 添加BeanPostProcessor【ApplicationListenerDetector】 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. // 6. 添加编译时的AspectJ if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } // Register default environment beans. // 7. 给BeanFactory注册一些能用的组件 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); } if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); } if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); }} postProcessBeanFactory(beanFactory)BeanFactory准备工作完成后进行的后置处理工作； 子类通过重写这个方法在BeanFactory创建并预准备完成以后做进一步的设置； 以上是BeanFactory的创建及预准备工作 invokeBeanFactoryPostProcessors(beanFactory)执行BeanFactoryPostProcessor BeanFactoryPostProcessor：BeanFactory的后置处理器，在BeanFactory标准初始化之后执行； 12345678910111213141516/** * Instantiate and invoke all registered BeanFactoryPostProcessor beans, * respecting explicit order if given. * &lt;p&gt;Must be called before singleton instantiation. */protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) { // 分析 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor) if (beanFactory.getTempClassLoader() == null &amp;&amp; beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); }} PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors两个接口：BeanFactoryPostProcessor，BeanDefinitionRegistryPostProcessor； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) { // Invoke BeanDefinitionRegistryPostProcessors first, if any. Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); // BeanDefinitionRegistryPostProcessor 先对 BeanDefinitionRegistry 执行 if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) { if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) { BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); } else { regularPostProcessors.add(postProcessor); } } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. boolean reiterate = true; while (reiterate) { reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); } // Now, invoke the postProcessBeanFactory callback of all processors handled so far. invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); } ......} 先执行BeanDefinitionRegistryPostProcessor： 获取所有的BeanDefinitionRegistryPostProcessor； 先执行实现了PriorityOrdered优先级接口的BeanDefinitionRegistryPostProcessor; postProcessor.postProcessBeanDefinitionRegistry(registry); 再执行实现了Ordered顺序接口的BeanDefinitionRegistryPostProcessor; postProcessor.postProcessBeanDefinitionRegistry(registry); 最后执行没有实现任何优先级或顺序接口的BeanDefinitionRegistryPostProcessor postProcessor.postProcessBeanDefinitionRegistry(registry); 再执行beanFactoryPostProcessor的方法： 获取所有的BeanFactoryPostProcessor； 先执行实现了PriorityOrdered优先级接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory); 再执行实现了Ordered顺序接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory); 最后执行没有实现任何优先级或顺序接口的BeanFactoryPostProcessor postProcessor.postProcessBeanFactory(beanFactory); registerBeanPostProcessors(beanFactory)注册BeanPostProcessor【Bean的后置处理器，拦截bean的创建过程】 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // Register BeanPostProcessorChecker that logs an info message when // a bean is created during BeanPostProcessor instantiation, i.e. when // a bean is not eligible for getting processed by all BeanPostProcessors. int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // Separate between BeanPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;BeanPostProcessor&gt; internalPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); // 1.获取所有的BeanPostProcessor (后置处理器都可以通过PriorityOrdered、Ordered接口来执行优先级) for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } // First, register the BeanPostProcessors that implement PriorityOrdered. // 2.先注册PriorityOrdered优先级接口的BeanPostProcessor, 把每一个BeanPostProcessor添加到Beanfactory中 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // Next, register the BeanPostProcessors that implement Ordered. // 3.再注册实现Ordered接口的 List&lt;BeanPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // Now, register all regular BeanPostProcessors. // 4.最后注册没有实现任何优先级接口的 List&lt;BeanPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); // 对MergedBeanDefinitionPostProcessor进行判断 if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // Finally, re-register all internal BeanPostProcessors. // 5.最终注册MergedBeanDefinitionPostProcessor sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, // moving it to the end of the processor chain (for picking up proxies etc). // 6.注册一个ApplicationListenerDetector，在Bean创建完成后检查是否是ApplicationListener，如果是，则applicationContext.addApplicationListener((ApplicationListener&lt;?&gt;) bean); beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext));}/** * Register the given BeanPostProcessor beans. */private static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanPostProcessor&gt; postProcessors) { for (BeanPostProcessor postProcessor : postProcessors) { beanFactory.addBeanPostProcessor(postProcessor); }} 不同接口类型的BeanPostProcessor在Bean创建前后的执行时机是不一样的： BeanPostProcessor InstantiationAwareBeanPostProcessor DestructionAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor MergedBeanDefinitionPostProcessor【internalPostProcessors】 initMessageSource()初始化MessageSource组件（做国际化功能，消息绑定，消息解析） 123456789101112131415161718192021222324252627282930313233343536/** * Initialize the MessageSource. * Use parent's if none defined in this context. */protected void initMessageSource() { // 1.获取BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); // 2.判断容器中是否有id为messageSource，类型为MessageSource的组件, 若有则赋值给messageSource, 若没有则new DelegatingMessageSource(); // MessageSource：取出国际化配置文件中的某个key值 (能按照区域信息获取) if (beanFactory.containsLocalBean(MESSAGE_SOURCE_BEAN_NAME)) { this.messageSource = beanFactory.getBean(MESSAGE_SOURCE_BEAN_NAME, MessageSource.class); // Make MessageSource aware of parent MessageSource. if (this.parent != null &amp;&amp; this.messageSource instanceof HierarchicalMessageSource) { HierarchicalMessageSource hms = (HierarchicalMessageSource) this.messageSource; if (hms.getParentMessageSource() == null) { // Only set parent context as parent MessageSource if no parent MessageSource // registered already. hms.setParentMessageSource(getInternalParentMessageSource()); } } if (logger.isTraceEnabled()) { logger.trace(&quot;Using MessageSource [&quot; + this.messageSource + &quot;]&quot;); } } else { // Use empty MessageSource to be able to accept getMessage calls. DelegatingMessageSource dms = new DelegatingMessageSource(); dms.setParentMessageSource(getInternalParentMessageSource()); this.messageSource = dms; // 3.把创建好的MessageSource注册在容器中, 以后获取国际化配置文件的值时, 可以自动注入MessageSource, 例如 MessageSource.getMessage(String code, @Nullable Object[] args, Locale locale) beanFactory.registerSingleton(MESSAGE_SOURCE_BEAN_NAME, this.messageSource); if (logger.isTraceEnabled()) { logger.trace(&quot;No '&quot; + MESSAGE_SOURCE_BEAN_NAME + &quot;' bean, using [&quot; + this.messageSource + &quot;]&quot;); } }} initApplicationEventMulticaster()初始化事件派发器 123456789101112131415161718192021222324252627/** * Initialize the ApplicationEventMulticaster. * Uses SimpleApplicationEventMulticaster if none defined in the context. * @see org.springframework.context.event.SimpleApplicationEventMulticaster */protected void initApplicationEventMulticaster() { // 1.获取BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); // 2.从BeanFactory中获取applicationEventMulticaster的ApplicationEventMulticaster if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) { this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); if (logger.isTraceEnabled()) { logger.trace(&quot;Using ApplicationEventMulticaster [&quot; + this.applicationEventMulticaster + &quot;]&quot;); } } // 3.如果第二步中不存在, 则创建 new SimpleApplicationEventMulticaster(beanFactory); else { this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); // 4.将创建的ApplicationEventMulticaster添加到BeanFactory中，以后其他组件直接自动注入 beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); if (logger.isTraceEnabled()) { logger.trace(&quot;No '&quot; + APPLICATION_EVENT_MULTICASTER_BEAN_NAME + &quot;' bean, using &quot; + &quot;[&quot; + this.applicationEventMulticaster.getClass().getSimpleName() + &quot;]&quot;); } }} onRefresh()留给子容器（子类）实现，子类重写这个方法，在容器刷新时可以自定义逻辑； registerListeners()将容器中所有项目里面的ApplicationListener注册进来 1234567891011121314151617181920212223242526272829/** * Add beans that implement ApplicationListener as listeners. * Doesn't affect other listeners, which can be added without being beans. */protected void registerListeners() { // Register statically specified listeners first. // 1.从容器中拿到所有的ApplicationListener for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) { // 2.将每个监听器添加到事件派发器中 getApplicationEventMulticaster().addApplicationListener(listener); } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let post-processors apply to them! String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) { getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); } // Publish early application events now that we finally have a multicaster... // 3.派发之前步骤产生的事件 Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents; this.earlyApplicationEvents = null; if (!CollectionUtils.isEmpty(earlyEventsToProcess)) { for (ApplicationEvent earlyEvent : earlyEventsToProcess) { getApplicationEventMulticaster().multicastEvent(earlyEvent); } }} finishBeanFactoryInitialization(beanFactory)初始化所有剩下的单实例bean 12345678910111213141516171819202122232425262728293031323334353637/** * Finish the initialization of this context's bean factory, * initializing all remaining singleton beans. */protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { // Initialize conversion service for this context. if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) { beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); } // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) { beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal)); } // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early. // 2.获取Bean的定义信息：RootBeanDefinition String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) { getBean(weaverAwareName); } // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // Allow for caching all bean definition metadata, not expecting further changes. beanFactory.freezeConfiguration(); // Instantiate all remaining (non-lazy-init) singletons. // 实例化所有的单例，非lazy // 分析 beanFactory.preInstantiateSingletons();} beanFactory.preInstantiateSingletons()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public void preInstantiateSingletons() throws BeansException { if (logger.isTraceEnabled()) { logger.trace(&quot;Pre-instantiating singletons in &quot; + this); } // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... // 1.获取容器中的所有Bean，依次进行初始化和创建对象 // 根据名字遍历所有的BeanDefinition，继而验证BeanDefinition for (String beanName : beanNames) { // 2.获取Bean的定义信息：RootBeanDefinition RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 3.Bean不是抽象的, 是单实例的, 不是懒加载的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) { // 3.1 判断是否是FactoryBean：是否是实现FactoryBean接口的Bean； if (isFactoryBean(beanName)) { Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) { FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged( (PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); } if (isEagerInit) { getBean(beanName); } } } // 3.2 不是FactoryBean, 利用getBean(beanName); 创建对象 else { // 开始实例普通的bean getBean(beanName); } } } // Trigger post-initialization callback for all applicable beans... // 4.所有Bean都利用getBean创建完成之后，检查所有的Bean是否是SmartInitializingSingleton接口的，如果是执行afterSingletonsInstantiated() for (String beanName : beanNames) { Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) { SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; { smartSingleton.afterSingletonsInstantiated(); return null; }, getAccessControlContext()); } else { smartSingleton.afterSingletonsInstantiated(); } } }}@Overridepublic Object getBean(String name) throws BeansException { return doGetBean(name, null, null, false);} doGetBean()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191protected &lt;T&gt; T doGetBean( String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException { // 验证bean的名字是否非法 String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. // 1.先获取缓存中保存的单实例Bean, 如果能获取到说明这个Bean之前被创建过 // (所有创建过的单实例Bean都会被缓存起来) Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) { if (logger.isTraceEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.trace(&quot;Returning eagerly cached instance of singleton bean '&quot; + beanName + &quot;' that is not fully initialized yet - a consequence of a circular reference&quot;); } else { logger.trace(&quot;Returning cached instance of singleton bean '&quot; + beanName + &quot;'&quot;); } } bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } // 2.缓存中获取不到，开始Bean的创建对象流程 else { // Fail if we're already creating this bean instance: // We're assumably within a circular reference. if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // Check if bean definition exists in this factory. BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) { // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); } else if (args != null) { // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); } else if (requiredType != null) { // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } else { return (T) parentBeanFactory.getBean(nameToLookup); } } // 3.标记当前bean已经被创建 if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { // 4.获取Bean的定义信息 RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. // 5.获取当前Bean依赖的其他Bean, 若有, 则按照getBean()把依赖的Bean先创建出来 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { if (isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between '&quot; + beanName + &quot;' and '&quot; + dep + &quot;'&quot;); } registerDependentBean(dep, beanName); try { getBean(dep); } catch (NoSuchBeanDefinitionException ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;'&quot; + beanName + &quot;' depends on missing bean '&quot; + dep + &quot;'&quot;, ex); } } } // Create bean instance. // 6.启动单实例Bean的创建流程 if (mbd.isSingleton()) { sharedInstance = getSingleton(beanName, () -&gt; { try { // 6.1 return createBean(beanName, mbd, args); // 6.2 将创建的Bean添加到缓存中 SingletonObjects // IOC容器就是这些Map, 很多的Map里面保存了Bean和环境信息等 } catch (BeansException ex) { // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { // It's a prototype -&gt; create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { String scopeName = mbd.getScope(); if (!StringUtils.hasLength(scopeName)) { throw new IllegalStateException(&quot;No scope name defined for bean ´&quot; + beanName + &quot;'&quot;); } Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(&quot;No Scope registered for scope name '&quot; + scopeName + &quot;'&quot;); } try { Object scopedInstance = scope.get(beanName, () -&gt; { beforePrototypeCreation(beanName); try { return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, &quot;Scope '&quot; + scopeName + &quot;' is not active for the current thread; consider &quot; + &quot;defining a scoped proxy for this bean if you intend to refer to it from a singleton&quot;, ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // Check if required type matches the type of the actual bean instance. if (requiredType != null &amp;&amp; !requiredType.isInstance(bean)) { try { T convertedBean = getTypeConverter().convertIfNecessary(bean, requiredType); if (convertedBean == null) { throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } return convertedBean; } catch (TypeMismatchException ex) { if (logger.isTraceEnabled()) { logger.trace(&quot;Failed to convert bean '&quot; + name + &quot;' to required type '&quot; + ClassUtils.getQualifiedName(requiredType) + &quot;'&quot;, ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean;}protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) { synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) { ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); // 放到三级缓存 this.earlySingletonObjects.put(beanName, singletonObject); // 从二级缓存清除 this.singletonFactories.remove(beanName); } } } } return singletonObject;} createBean()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { if (logger.isTraceEnabled()) { logger.trace(&quot;Creating instance of bean '&quot; + beanName + &quot;'&quot;); } RootBeanDefinition mbdToUse = mbd; // Make sure bean class is actually resolved at this point, and // clone the bean definition in case of a dynamically resolved Class // which cannot be stored in the shared merged bean definition. Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } // Prepare method overrides. try { mbdToUse.prepareMethodOverrides(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, &quot;Validation of method overrides failed&quot;, ex); } // 1.解析resolveBeforeInstantiation(): 让BeanPostProcessor先拦截返回代理对象, InstantiationAwareBeanPostProcessor 提前执行, // 先触发postProcessBeforeInstantiation(), 如果有返回值，则触发postProcessAfterInitialization(); try { // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } catch (Throwable ex) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); } // 2.如果前面的InstantiationAwareBeanPostProcessor 没有返回代理对象, 则执行 // 分析 try { Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isTraceEnabled()) { logger.trace(&quot;Finished creating instance of bean '&quot; + beanName + &quot;'&quot;); } return beanInstance; } catch (BeanCreationException | ImplicitlyAppearedSingletonException ex) { // A previously detected exception with proper bean creation context already, // or illegal singleton state to be communicated up to DefaultSingletonBeanRegistry. throw ex; } catch (Throwable ex) { throw new BeanCreationException( mbdToUse.getResourceDescription(), beanName, &quot;Unexpected exception during bean creation&quot;, ex); }} doCreateBean()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { // 1.实例化对象, 利用工厂方法或者对象的构造器创建出Bean实例, 里面第二次调用后置处理器 instanceWrapper = createBeanInstance(beanName, mbd, args); } Object bean = instanceWrapper.getWrappedInstance(); Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) { mbd.resolvedTargetType = beanType; } // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { // 2.调用MergedBeanDefinitionPostProcessor 的postProcessMergedBeanDefinition(mbd, beanType, beanName) applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Post-processing of merged bean definition failed&quot;, ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 判断是否允许循环依赖 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isTraceEnabled()) { logger.trace(&quot;Eagerly caching bean '&quot; + beanName + &quot;' to allow for resolving potential circular references&quot;); } // 第四次调用后置处理器，判断是否需要AOP addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); } // Initialize the bean instance. Object exposedObject = bean; try { // 3.Bean属性赋值(填充属性), 即常说的自动注入 // 里面会完成第五次和第六次后置处理器的调用 // 赋值之前: // 1) 拿到InstantiationAwareBeanPostProcessor后置处理器 postProcessAfterInstantiation(); // 2) 拿到InstantiationAwareBeanPostProcessor后置处理器 postProcessPropertyValues(); // 赋值: 应用Bean属性的值, 为属性利用setter方法等进行赋值: applyPropertyValues(beanName, mbd, bw, pvs); populateBean(beanName, mbd, instanceWrapper); // 4.Bean初始化 // 里面会进行第七次和第八次后置处理器的调用 // 分析 exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); } } if (earlySingletonExposure) { Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name '&quot; + beanName + &quot;' has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;'getBeanNamesForType' with the 'allowEagerInit' flag turned off, for example.&quot;); } } } } // Register bean as disposable. // 5.注册Bean的销毁方法 try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); } return exposedObject;} initializeBean()12345678910111213141516171819202122232425262728293031323334353637protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) { // 1.执行Aware接口方法 // invokeAwareMethods(beanName, bean): 执行xxxAware的方法, BeanNameAware/BeanClassLoaderAware/BeanFactoryAware if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; { invokeAwareMethods(beanName, bean); return null; }, getAccessControlContext()); } else { invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // 2.执行后置处理器初始化之前 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { // 3.执行初始化方法 // 3.1 是否是InitializingBean接口的实现，执行接口规定的初始化 // 3.2 是否自定义初始化 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); } if (mbd == null || !mbd.isSynthetic()) { // 4.执行后置处理器初始化之后 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean;} finishRefresh()完成BeanFactory的初始化创建工作，IOC容器就创建完成 123456789101112131415161718192021protected void finishRefresh() { // Clear context-level resource caches (such as ASM metadata from scanning). clearResourceCaches(); // Initialize lifecycle processor for this context. // 初始化和生命周期有关的后置处理器LifecycleProcessor // 默认从容器中找是否有LifecycleProcessor的组件， // 如果没有, 则new DefaultLifecycleProcessor(); 并加入到容器 initLifecycleProcessor(); // Propagate refresh to lifecycle processor first. // 拿到前面定义的生命周期处理器，回调onRefresh() getLifecycleProcessor().onRefresh(); // Publish the final event. // 发布容器刷新完成事件 publishEvent(new ContextRefreshedEvent(this)); // Participate in LiveBeansView MBean, if active. LiveBeansView.registerApplicationContext(this);} 总结： 1）Spring容器在启动时，会先保存所有注册进来的Bean的定义信息 xml注册bean：bean标签 注解注册bean 2）Spring容器在合适的时机创建这些Bean 用到这个Bean的时候：利用getBean创建bean，创建好以后保存在容器中； 统一创建剩下所有的bean时，finishBeanFactoryInitialization()； 3）后置处理器：BeanPostProcessor 每个bean创建完成时，都会使用各种后置处理器进行处理，来增强bean的功能； AutowiredAnnotationBeanPostProcessor：处理自动注入； AnnotationAwareAspectJAutoProxyCreator：来做AOP功能； … 4）事件驱动模型 ApplicationListener：事件监听 ApplicationEventMulticaster：事件派发","link":"/2020/09/27/spring-refresh/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"concurrent","slug":"concurrent","link":"/tags/concurrent/"},{"name":"Basis","slug":"Basis","link":"/tags/Basis/"},{"name":"Dubbo","slug":"Dubbo","link":"/tags/Dubbo/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"Book","slug":"Book","link":"/tags/Book/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"lc","slug":"lc","link":"/tags/lc/"},{"name":"lcof","slug":"lcof","link":"/tags/lcof/"},{"name":"dp","slug":"dp","link":"/tags/dp/"},{"name":"MQ","slug":"MQ","link":"/tags/MQ/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Project","slug":"Project","link":"/tags/Project/"},{"name":"Framework","slug":"Framework","link":"/tags/Framework/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"Log","slug":"Log","link":"/tags/Log/"},{"name":"Distribution","slug":"Distribution","link":"/tags/Distribution/"}],"categories":[{"name":"并发","slug":"并发","link":"/categories/%E5%B9%B6%E5%8F%91/"},{"name":"剑指offer","slug":"剑指offer","link":"/categories/%E5%89%91%E6%8C%87offer/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"项目","slug":"项目","link":"/categories/%E9%A1%B9%E7%9B%AE/"}]}